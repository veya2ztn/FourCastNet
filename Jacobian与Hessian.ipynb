{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca86263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b603a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import functorch\n",
    "from functorch import jacrev\n",
    "from functorch import make_functional, vmap, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be03745b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.nn.Linear(in_chan, out_chan,bias=False)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9202bc26",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from functorch import make_functional, vmap, grad\n",
    "model= MyModel(3, 4)\n",
    "x    = torch.randn(7, 3)\n",
    "func_model, params = make_functional(model)\n",
    "w    = params[0]\n",
    "batch_J= vmap(jacrev(func_model, argnums=1), (None, 0))(params, x)\n",
    "batch_H= jacrev(vmap(jacrev(func_model, argnums=1), (None, 0)), argnums=0)(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a593fc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_J.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5611e08",
   "metadata": {},
   "source": [
    "这里设定\n",
    "$$\n",
    "Y=(W^TX)^2 则 Y_{j}=(\\sum W_{jk}X_k)^2\n",
    "$$\n",
    "\n",
    "那么 \n",
    "$$\n",
    "\\frac{dY_{j}}{dX_{\\alpha}}=2 (\\sum W_{jk}X_k) (\\sum W_{jk}\\delta_{\\alpha k})=2 (\\sum W_{jk}X_k) W_{j\\alpha}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a744a",
   "metadata": {},
   "source": [
    "首先来看 Jacobian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07e2772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y1 = torch.einsum('ij,bj->bi',w,x)\n",
    "yx = 2*torch.einsum('bi,ik->bik',y1,w)\n",
    "print(torch.allclose(yx,batch_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819284f",
   "metadata": {},
   "source": [
    "再来看 Hessian\n",
    "\n",
    "那么 \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d^2Y_{j}}{dW_{\\beta\\gamma}dX_{\\alpha}}\n",
    "&=\\frac{d}{dW_{\\beta\\gamma}}\\frac{dY_{j}}{dX_{\\alpha}}=2 \\frac{d}{dW_{\\beta\\gamma}}(\\sum_k W_{jk}X_k) W_{j\\alpha}\\\\\n",
    "&=2 (\\sum_k \\delta_{jk}^{\\beta\\gamma} X_k)  W_{j\\alpha} + 2 (\\sum_k W_{jk}X_k)\\delta_{j\\alpha}^{\\beta\\gamma}\\\\\n",
    "&=2 (\\delta_{j}^{\\beta} X_{\\gamma})W_{j\\alpha} +2 (\\sum_k W_{jk}X_k)\\delta_{j}^{\\beta}\\delta_{\\alpha}^{\\gamma}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f794b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dta1 = torch.eye(w.shape[0],w.shape[0])\n",
    "dta2 = torch.eye(w.shape[1],x.shape[1])\n",
    "\n",
    "h=2*torch.einsum('bj,Bg,ja   ->Bjabg',dta1,x,w) \\\n",
    " +2*torch.einsum('jk,Bk,bj,ga->Bjabg',w,x,dta1,dta2)\n",
    "print(torch.allclose(h,batch_H[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aee316",
   "metadata": {},
   "source": [
    "#### 我们也可以直接来计算正则项的导数\n",
    "$$\n",
    "P^{\\gamma}=< y^\\gamma,t^\\gamma >+\\lambda_1 (\\sum_\\alpha J_\\alpha^{\\gamma}-1)^2+\\lambda_2 [\\sum_\\alpha (J_\\alpha^{\\gamma})^2-1]^2\n",
    "$$\n",
    "\n",
    "如果我们的函数是 $Y = (WX)**2$ 即 $y_j = (\\sum_k W_{jk}X_k)^2$ 那么\n",
    "\n",
    "$$\n",
    "J_\\alpha^\\gamma = \\frac{dY_{\\gamma}}{dX_{\\alpha}}=2 (\\sum_k W_{\\gamma k}X_k) (\\sum_k W_{\\gamma k}\\delta_{\\alpha k})=2 (\\sum_k W_{\\gamma k}X_k) W_{\\gamma\\alpha}\n",
    "$$\n",
    "\n",
    "那么我们可以显式的得到 \n",
    "$$\n",
    "\\sum_\\alpha J_\\alpha^{\\gamma} = \\sum_\\alpha 2 (\\sum_k W_{\\gamma k}X_k) W_{j\\alpha} = 2 (\\sum_k W_{\\gamma k}X_k) \\sum_\\alpha W_{\\gamma\\alpha}\n",
    "$$\n",
    "\n",
    "我们的 L1 约束是对每一个上标 $(\\sum_\\alpha J_\\alpha^{\\gamma}-1)^2$ 都极小, 在这个 toy 模型里面, $\\gamma$ 只是 $W$ 的某一行, 所以我们现在去掉上标 $W_{\\gamma k}\\rightarrow w_k$, 得到\n",
    "$$\n",
    "P(w) = <w|x><w|\\mathbf{1}>\n",
    "$$\n",
    "再给定 |x> 的情况下, 我们要观察 $P(w) =1$ 这个解是否存在, 以及他是否是 $(P(w)-1)^2$ 的极小值解.\n",
    "\n",
    "注意 |x> 是一个 batch tensor, 意思是我们会要求所有的 x 都要满足 $P(w;x) =1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff7eea",
   "metadata": {},
   "source": [
    "我们不妨假设 $\\sum_\\alpha w_{\\alpha} = 1/\\Lambda$ 为一个常数. 那么就得到方程组(n个未知数,一个方程,必有解, 如果有B个 Batch 那么总的方程数是B, 所以最后存在解的条件就是参数量要大于数据量)\n",
    "$$\n",
    "w_1+w_2+\\dots+w_n = 1/\\Lambda\\\\\n",
    "w_1x_1+w_2x_2+\\dots+w_nx_n = \\Lambda\n",
    "$$\n",
    "有柯西不等式和基本不等式\n",
    "$$\n",
    "\\Lambda = \\sum_i (w_ix_i) \\leq \\sqrt{\\sum_i w_i^2 \\sum_i x_i^2} \\leq \\sqrt{(\\sum_i w_i)^2 \\sum_i x_i^2} = |\\frac{1}{\\Lambda}| \\sqrt{ \\sum_i x_i^2}\n",
    "$$\n",
    "也就是\n",
    "$$\n",
    "\\Lambda^2 \\leq \\sqrt{ \\sum_i x_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad282637",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "B=20\n",
    "I=10\n",
    "O=30\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.nn.Linear(in_chan, out_chan,bias=False)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)**2\n",
    "model= MyModel(I, O)\n",
    "x    = torch.randn(B, I)\n",
    "cotangents = torch.ones(B,I)\n",
    "func_model, params = make_functional(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445f2de",
   "metadata": {},
   "source": [
    "###### 验证 estimate 的效果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173cf05",
   "metadata": {},
   "source": [
    "- $||A||_F^2=Tr(AA^T) = \\frac{1}{m}\\sum_i^m v_i AA^T v_i^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b90705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4937.9277)\n",
      "tensor(5025.4863)\n"
     ]
    }
   ],
   "source": [
    "O = 100\n",
    "I = 50 \n",
    "sample_times = 100\n",
    "A = torch.randn(O,I)\n",
    "v = torch.randint(2,(100, O))*2-1\n",
    "v = v.float()\n",
    "ground_truth = (A**2).sum()\n",
    "estima_value = torch.einsum('bi,ij,lj,bl->b',v,A,A,v).mean()\n",
    "print(ground_truth)\n",
    "print(estima_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8528f",
   "metadata": {},
   "source": [
    "$$\n",
    "2(||A||_F^2 - \\sum_{i}^n A_{ii}^2)=Var(v A v^T) \\quad where \\quad v\\sim \n",
    "\\{\\begin{align}\n",
    "    &+1 ，50\\%\\\\\n",
    "    &-1，50\\%\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "sample_times 没必要太大就有比较好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "89fd6dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01167607307434082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed84c549f3af4200a090868ac800e5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f20895fbe50>]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glist=[]\n",
    "elist=[]\n",
    "for _ in tqdm(range(100)):\n",
    "    O = 100\n",
    "    sample_times = 1000\n",
    "    A = torch.randn(O,O) \n",
    "    A = A+A.transpose(1,0)\n",
    "    v = torch.randint(2,(sample_times, O))*2-1\n",
    "    v = v.float()\n",
    "    ground_truth = 2*((A**2).sum() - (A**2).trace())\n",
    "    estima_value = torch.einsum('bi,il,bl->b',v,A,v).var()\n",
    "    glist.append(ground_truth)\n",
    "    elist.append(estima_value)\n",
    "plt.plot(glist,'r')\n",
    "plt.plot(elist,'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0967d",
   "metadata": {},
   "source": [
    "- $\\sum_{i}^n A_{ii}^2 =   ||A||_F^2 - Var(\\sum_i^m v_i A v_i^T)/2$\n",
    "\n",
    "sample_times 需要比较大才有好效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "c3b14665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54708452",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m elist\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m vlist\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m)):\n\u001b[1;32m      5\u001b[0m     O \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m     sample_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "glist=[]\n",
    "elist=[]\n",
    "vlist=[]\n",
    "for _ in tqdm(range(30)):\n",
    "    O = 100\n",
    "    sample_times = 1000\n",
    "    A = torch.randn(O,O) \n",
    "    A = A+A.transpose(1,0)\n",
    "    v = torch.randint(2,(sample_times, O))*2-1\n",
    "    v = v.float()\n",
    "    ground_truth = (A**2).sum()\n",
    "    estima_value = (A**2).trace()\n",
    "    var = torch.einsum('bi,il,bl->b',v,A,v).var()/2\n",
    "    vlist.append(var.item())\n",
    "    glist.append(ground_truth.item())\n",
    "    elist.append(estima_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d381591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# xmap ={}\n",
    "# for i,(v,s,t) in enumerate(zip(vlist,glist,elist)):\n",
    "#     data.append([r\"$ ||A||_F^2 $\",s,i+0.24])  \n",
    "    \n",
    "#     data.append([r\"$Var(v A v^T)/2$\",v,i])  \n",
    "#     data.append([r\"$\\sum_{i}^n A_{ii}^2$\",t,i])  \n",
    "    \n",
    "# names_col = ['layer','value', 'block']\n",
    "# plot_df = pd.DataFrame(data=data,columns=names_col)\n",
    "# fig = px.bar(plot_df, x='block', y='value', color='layer' ,title='My plot',height=600)\n",
    "# fig.update_layout(showlegend=True) \n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f9f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1828aace",
   "metadata": {},
   "source": [
    "$\\sum_\\beta\\sum_{\\alpha\\neq\\beta} J_\\alpha^{\\gamma}J_\\beta^{\\gamma} \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13801cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=1\n",
    "I=10\n",
    "O=30\n",
    "model= MyModel(I, O).cuda()\n",
    "x    = torch.randn(B, I).cuda()\n",
    "y    = torch.randn(B, O).cuda()\n",
    "shape = y.shape\n",
    "sample_times=1000\n",
    "func_model, params =  make_functional(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "655d7065",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def CorrelationTerm(params,x,cotangents_variable):\n",
    "    '''\n",
    "    \\sum_\\beta\\sum_{\\alpha\\neq\\beta} J_\\alpha^{\\gamma}J_\\beta^{\\gamma}\n",
    "    '''\n",
    "    J = vmap(jacrev(func_model, argnums=1), (None, 0))(params, x) #(B, O, I)\n",
    "    B, O, I =J.shape\n",
    "    K = torch.ones(I,I) - torch.eye(I,I)\n",
    "    K = K.to(J.device)\n",
    "    # L^\\gamma = \\sum_\\beta\\sum_{\\alpha\\neq\\beta} J_\\alpha^{\\gamma}J_\\beta^{\\gamma}\n",
    "    C = torch.einsum('bij,jk,bik->bi',J,K,J) #(B,O)  L^\\gamma\n",
    "    C = (C**2).sum(-1) #(B) \\sum_\\gamma (L^\\gamma)^2\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "58881cdb",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def TrvJOJv_and_ETrAAT(params,x,cotangents_variable):\n",
    "    _, vJ_fn = functorch.vjp(lambda x:func_model(params,x), x)\n",
    "    vJ   = vJ_fn(cotangents_variable)[0]\n",
    "    dims = list(range(1,len(vJ.shape)))\n",
    "    vJO  = vJ.sum(dims,keepdims=True)-vJ # <vJ|1-I|\n",
    "    vJOJv= (vJ*vJO).sum(dims)#should sum over all dimension except batch\n",
    "    return vJOJv, functorch.jvp(lambda x:func_model(params,x), (x,), (vJO,))[1].norm()# average the batch_size also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "01d1da08",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Normlization_Term_2(params,x,cotangents_variables):\n",
    "    TrvJOJvs,ETrAATs =  vmap(TrvJOJv_and_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables)\n",
    "    return ETrAATs.mean() - torch.var(TrvJOJvs,0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c64d3fbc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.7047, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ground_truth = CorrelationTerm(params,x,None)[0]\n",
    "    print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ba8a4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_modifier = Nodal_GradientModifier(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69a8ccdd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    grad_modifier.func_model, params =  make_functional(model)\n",
    "    cotangents_variables = torch.randint(2,(sample_times,*shape)).cuda()*2-1\n",
    "    ETrAATs  = vmap(grad_modifier.get_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables)\n",
    "    cotangents_variables = torch.randint(2,(sample_times,*shape)).cuda()*2-1\n",
    "    TrvJOJvs = vmap(grad_modifier.get_TrvJOJv, (None, None, 0 ))(params, x,cotangents_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "61501e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(91.1853, device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrvJOJvs.var()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b2ae0005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.4774, device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ETrAATs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03d64a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2106140851974487\n",
      "-280.4579162597656\n"
     ]
    }
   ],
   "source": [
    "B=1\n",
    "I=10\n",
    "O=30\n",
    "model= MyModel(I, O).cuda()\n",
    "x    = torch.randn(B, I).cuda()\n",
    "y    = torch.randn(B, O).cuda()\n",
    "shape = y.shape\n",
    "sample_times=1000\n",
    "\n",
    "grad_modifier.func_model, params =  make_functional(model)\n",
    "with torch.no_grad():\n",
    "    ground_truth = grad_modifier.Normlization_Term_2_Full(params,x,None)\n",
    "with torch.no_grad(): \n",
    "    cotangents_variables = torch.randint(2,(sample_times,*shape)).cuda()*2-1\n",
    "    esimat_value = grad_modifier.Normlization_Term_2(params,x,cotangents_variables)\n",
    "print(ground_truth.item())\n",
    "print(esimat_value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6154a",
   "metadata": {},
   "source": [
    "#### 训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.afnonet import AFNONet\n",
    "# model = AFNONet((32,64),2,1,1).cuda()\n",
    "# func_model, params = make_functional(model)\n",
    "# x = torch.randn(4,1,32,64).cuda()\n",
    "# cotangents=torch.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eaa568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_memlab import MemReporter\n",
    "# reporter = MemReporter()\n",
    "# reporter.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8f5c15",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import functorch\n",
    "from functorch import jacrev,jacfwd\n",
    "from functorch import make_functional, vmap, grad\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018ebb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.afnonet import AFNONet\n",
    "# model = AFNONet((32,64),2,110,110).cuda()\n",
    "# func_model, params = make_functional(model)\n",
    "# x = torch.randn(4,110,32,64).cuda()\n",
    "# y = torch.randn(4,110,32,64).cuda()\n",
    "# optimzer= torch.optim.Adam(model.parameters())\n",
    "# shape = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7550a7c",
   "metadata": {
    "code_folding": [
     0,
     7,
     10,
     20,
     35,
     37
    ]
   },
   "outputs": [],
   "source": [
    "from model.GradientModifier import *\n",
    "# class Nodal_GradientModifier:\n",
    "#     def __init__(self,lambda1=1,lambda2=1,sample_times=10):\n",
    "#         self.lambda1 = lambda1\n",
    "#         self.lambda2 = lambda2\n",
    "#         self.sample_times = sample_times\n",
    "#         self.cotangents_sum_along_x_dimension = None\n",
    "#     def Normlization_Term_1(self,params,x):\n",
    "#         if self.cotangents_sum_along_x_dimension is None or self.cotangents_sum_along_x_dimension.shape!=x.shape:\n",
    "#             self.cotangents_sum_along_x_dimension = torch.ones_like(x)\n",
    "#         return ((functorch.jvp(lambda x:self.func_model(params,x), (x,), (self.cotangents_sum_along_x_dimension,))[1]-1)**2).mean()\n",
    "#     def TrvJOJv_and_ETrAAT(self,params,x,cotangents_variable):\n",
    "#         _, vJ_fn = functorch.vjp(lambda x:self.func_model(params,x), x)\n",
    "#         vJ   = vJ_fn(cotangents_variable)[0]\n",
    "#         dims = list(range(1,len(vJ.shape)))\n",
    "#         vJO  = vJ.sum(dims,keepdims=True)-vJ # <vJ|1-I|\n",
    "#         vJOJv= (vJO*vJ).sum(dim=dims)#should sum over all dimension except batch\n",
    "#         ETrAAT = functorch.jvp(lambda x:self.func_model(params,x), (x,), (vJO,))[1] # (B,Ouputdim)\n",
    "#         dims = list(range(1,len(ETrAAT.shape)))\n",
    "#         ETrAAT=ETrAAT.norm(dim=dims)\n",
    "#         return vJOJv, ETrAAT# DO NOT average the batch_size also\n",
    "#     def get_TrvJOJv(self,params,x,cotangents_variable):\n",
    "#         _, vJ_fn = functorch.vjp(lambda x:self.func_model(params,x), x)\n",
    "#         vJ   = vJ_fn(cotangents_variable)[0]\n",
    "#         dims = list(range(1,len(vJ.shape)))\n",
    "#         vJO  = vJ.sum(1,keepdims=True)-vJ # <vJ|1-I|\n",
    "#         vJOJv= (vJO*vJ).sum(dim=dims)#should sum over all dimension except batch\n",
    "#         return vJOJv\n",
    "#     def get_ETrAAT(self,params,x,cotangents_variable):\n",
    "#         _, vJ_fn = functorch.vjp(lambda x:self.func_model(params,x), x)\n",
    "#         vJ   = vJ_fn(cotangents_variable)[0]\n",
    "#         vJO  = vJ.sum(1,keepdims=True)-vJ # <vJ|1-I|\n",
    "#         ETrAAT = functorch.jvp(lambda x:self.func_model(params,x), (x,), (vJO,))[1] # (B,Ouputdim)\n",
    "#         dims = list(range(1,len(ETrAAT.shape)))\n",
    "#         ETrAAT=ETrAAT.norm(dim=dims)\n",
    "#         return ETrAAT\n",
    "#     def get_ETrAAT_times(self,params,x,cotangents_variables):\n",
    "#         return vmap(get_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables).mean()\n",
    "#     def get_TrvJOJv_times(self,params,x,cotangents_variables):\n",
    "#         return vmap(self.get_TrvJOJv, (None, None, 0 ))(params, x,cotangents_variables).mean()\n",
    "#     def Normlization_Term_2(self,params,x,cotangents_variables):\n",
    "#         while True:\n",
    "#             TrvJOJvs,ETrAATs =  vmap(self.TrvJOJv_and_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables)\n",
    "#             # (S,B) # (S,B)\n",
    "#             CorrelationTerm = ETrAATs.mean(0) - TrvJOJvs.var(0)/2 #(B,)\n",
    "#             # reject when CorrelationTerm < 0 \n",
    "#             if torch.all(CorrelationTerm>0):break\n",
    "#         return CorrelationTerm.mean()\n",
    "# #     def Normlization_Term_2(self,params,x,cotangents_variable):\n",
    "# #         '''\n",
    "# #         \\sum_\\beta\\sum_{\\alpha\\neq\\beta} J_\\alpha^{\\gamma}J_\\beta^{\\gamma}\n",
    "# #         '''\n",
    "# #         J = vmap(jacrev(self.func_model, argnums=1), (None, 0))(params, x) #(B, O, I)\n",
    "# #         B, O, I =J.shape\n",
    "# #         K = torch.ones(I,I) - torch.eye(I,I)\n",
    "# #         K = K.to(J.device)\n",
    "# #         # L^\\gamma = \\sum_\\beta\\sum_{\\alpha\\neq\\beta} J_\\alpha^{\\gamma}J_\\beta^{\\gamma}\n",
    "# #         C = torch.einsum('bij,jk,bik->bi',J,K,J) #(B,O)  L^\\gamma\n",
    "# #         C = (C**2).sum(-1) #(B) \\sum_\\gamma (L^\\gamma)^2\n",
    "# #         return C.mean()\n",
    "    \n",
    "#     def Normlization_Term_2_Full(self,params,x,cotangents_variables):\n",
    "#         return (((vmap(jacrev(self.func_model, argnums=1), (None, 0))(params, x)**2).sum(-1)-1)**2).mean()   \n",
    "    \n",
    "#     def backward(self,model, x, y , return_Normlization_Term_1=False, return_Normlization_Term_2=False):\n",
    "        \n",
    "#         self.func_model, params =  make_functional(model)\n",
    "#         shape = y.shape\n",
    "#         cotangents_variables = torch.randint(2,(self.sample_times,*shape)).cuda()*2-1\n",
    "#         with torch.no_grad():\n",
    "#             if self.lambda1 != 0:\n",
    "#                 Derivation_Term_1 = jacrev(self.Normlization_Term_1, argnums=0)(params, x)\n",
    "#             if self.lambda2 != 0:\n",
    "#                 Derivation_Term_2 = jacrev(self.Normlization_Term_2, argnums=0)(params, x,cotangents_variables)\n",
    "#         for i, param in enumerate(model.parameters()):\n",
    "#             delta_p = 0\n",
    "#             if self.lambda1 != 0:delta_p += self.lambda1*Derivation_Term_1[i]\n",
    "#             if self.lambda2 != 0:delta_p += self.lambda2*Derivation_Term_2[i]\n",
    "#             if param.grad is not None:\n",
    "#                 param.grad.data += delta_p\n",
    "#             else:\n",
    "#                 param.grad = delta_p\n",
    "#         with torch.no_grad():\n",
    "#             if return_Normlization_Term_1: \n",
    "#                 return self.Normlization_Term_1(params, x)\n",
    "#             if return_Normlization_Term_2:\n",
    "#                 return self.Normlization_Term_2(params, x,cotangents_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785994b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c1e4b4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# the position of gradient modifier(before `loss.backward` or after `loss.backward`) does not effect the result\n",
    "##############################################################################################################\n",
    "B=20\n",
    "I=10\n",
    "O=30\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.nn.Linear(in_chan, out_chan,bias=False)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)**2\n",
    "model= MyModel(I, O).cuda()\n",
    "x    = torch.randn(B, I).cuda()\n",
    "y    = torch.randn(B, O).cuda()\n",
    "func_model, params = make_functional(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(),1)\n",
    "\n",
    "model.load_state_dict(torch.load('debug/model.weight.pt'))\n",
    "x = torch.load('debug/input.pt')\n",
    "y = torch.load('debug/ouput.pt')\n",
    "\n",
    "#optimizer = SGD_Nodel(model.parameters(),1)\n",
    "#optimzer= torch.optim.Adam(model.parameters())\n",
    "\n",
    "accues= []\n",
    "grad_modifier = Nodal_GradientModifier()\n",
    "weight_beg = model.backbone.weight.cpu().detach().numpy()\n",
    "# for _ in tqdm(range(1000)):\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss = F.mse_loss(model(x),y)\n",
    "loss.backward()\n",
    "grad_modifier.backward(model,x,y)\n",
    "gradient = model.backbone.weight.grad.cpu().detach().numpy()\n",
    "\n",
    "optimizer.step()\n",
    "#     accues.append(accu.item())\n",
    "weight_end = model.backbone.weight.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf461a",
   "metadata": {
    "code_folding": [
     8,
     9
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011049509048461914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504510993fb54b9aa67f99ba0fdf5bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the goal about achieve zero loss is depended on the Batch size and the number of data\n",
    "##############################################################################################################\n",
    "%matplotlib inline\n",
    "from mltool.visualization import *\n",
    "from tqdm.notebook import tqdm\n",
    "B=20\n",
    "I=100\n",
    "O=300\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.nn.Linear(in_chan, out_chan,bias=False)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)**2\n",
    "model= MyModel(I, O).cuda()\n",
    "x    = torch.randn(B, I).cuda()\n",
    "y    = torch.randn(B, O).cuda()\n",
    "func_model, params = make_functional(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(),0.1)\n",
    "\n",
    "#optimizer = SGD_Nodel(model.parameters(),1)\n",
    "#optimzer= torch.optim.Adam(model.parameters())\n",
    "\n",
    "accues1= []\n",
    "accues2= []\n",
    "grad_modifier = NGmod_absolute(1,0.1)\n",
    "#weight_beg = model.backbone.weight.cpu().detach().numpy()\n",
    "for _ in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()\n",
    "    # loss = F.mse_loss(model(x),y)\n",
    "    # loss.backward()\n",
    "    #gradient = model.backbone.weight.grad.cpu().detach().numpy()\n",
    "    accu1,accu2=grad_modifier.backward(model,x,y,return_Normlization_Term_1=True,return_Normlization_Term_2=True)\n",
    "    optimizer.step()\n",
    "    accues1.append(accu1.item())\n",
    "    accues2.append(accu2.item())\n",
    "#weight_end = model.backbone.weight.cpu().detach().numpy()\n",
    "\n",
    "plt.plot(accues1,'r')\n",
    "plt.plot(accues2,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec52e0f1",
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012019872665405273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22ddc843a1d4c7e8addf8c731d3f295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the goal about achieve zero loss is depended on the Batch size and the number of data\n",
    "##############################################################################################################\n",
    "%matplotlib inline\n",
    "from mltool.visualization import *\n",
    "from tqdm.notebook import tqdm\n",
    "B=20\n",
    "I=10\n",
    "O=30\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.nn.Linear(in_chan, out_chan,bias=False)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)**2\n",
    "model= MyModel(I, O).cuda()\n",
    "x    = torch.randn(B, I).cuda()\n",
    "y    = torch.randn(B, O).cuda()\n",
    "func_model, params = make_functional(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(),1)\n",
    "\n",
    "#optimizer = SGD_Nodel(model.parameters(),1)\n",
    "#optimzer= torch.optim.Adam(model.parameters())\n",
    "\n",
    "accues= []\n",
    "grad_modifier = Nodal_GradientModifier(0,0.001,1000)\n",
    "#weight_beg = model.backbone.weight.cpu().detach().numpy()\n",
    "for _ in tqdm(range(10)):\n",
    "    optimizer.zero_grad()\n",
    "    # loss = F.mse_loss(model(x),y)\n",
    "    # loss.backward()\n",
    "    #gradient = model.backbone.weight.grad.cpu().detach().numpy()\n",
    "    accu=grad_modifier.backward(model,x,y,return_Normlization_Term_2=True)\n",
    "    optimizer.step()\n",
    "    accues.append(accu.item())\n",
    "#weight_end = model.backbone.weight.cpu().detach().numpy()\n",
    "\n",
    "plt.plot(accues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e3cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47bdc4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6432467\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(weight_end - weight_beg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c50946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimzer.param_groups[0]['params'][0].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91283f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cotangents = torch.ones(*shape).cuda()\n",
    "cotangents_variable  = torch.randint(2,shape).cuda()*2-1\n",
    "cotangents_variables = torch.randint(2,(10,*shape)).cuda()*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605845ca",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Normlization_Term_1 = lambda params,x:((functorch.jvp(lambda x:func_model(params,x), (x,), (cotangents,)\n",
    "                                      )[1]-1)**2).mean()\n",
    "with torch.no_grad():\n",
    "    Derivation_Term_1 = jacrev(Normlization_Term_1, argnums=0)(params, x)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6a5b5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_\\gamma A_{\\gamma\\gamma}^2=2(||A||_F^2) - Var[Tr_m(A)]=2\\{E[Tr_m(AA^T)] - Var[Tr_m(A)]\\}\n",
    "$$\n",
    "where $A=J(\\mathbf{1}-I)J^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5a905",
   "metadata": {},
   "source": [
    "$$\n",
    "||A||_F^2= Tr(AA^T)=E[< \\vec{v}A|A^T\\vec{v}>]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7175c",
   "metadata": {},
   "source": [
    "$$\n",
    "< \\vec{v}A| = < \\vec{v}J|(\\mathbf{1}-I)J^T|= < \\vec{u}J^T|=|J\\vec{u}>^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c0f314a",
   "metadata": {
    "code_folding": [
     0,
     7,
     13,
     18,
     20,
     22,
     25
    ]
   },
   "outputs": [],
   "source": [
    "def TrvJOJv_and_ETrAAT(params,x,cotangents_variable):\n",
    "    _, vJ_fn = functorch.vjp(lambda x:func_model(params,x), x)\n",
    "    vJ   = vJ_fn(cotangents_variable)[0]\n",
    "    dims = list(range(1,len(vJ.shape)))\n",
    "    vJO  = vJ.sum(dims,keepdims=True)-vJ # <vJ|1-I|\n",
    "    vJOJv= (vJ*vJO).sum(dims)#should sum over all dimension except batch\n",
    "    return vJOJv, functorch.jvp(lambda x:func_model(params,x), (x,), (vJO,))[1].norm()# average the batch_size also\n",
    "def get_TrvJOJv(params,x,cotangents_variable):\n",
    "    _, vJ_fn = functorch.vjp(lambda x:func_model(params,x), x)\n",
    "    vJ   = vJ_fn(cotangents_variable)[0]\n",
    "    vJO  = vJ.sum(1,keepdims=True)-vJ # <vJ|1-I|\n",
    "    vJOJv= (vJ*vJO).sum(-1)#should sum over all dimension except batch\n",
    "    return vJOJv\n",
    "def get_ETrAAT(params,x,cotangents_variable):\n",
    "    _, vJ_fn = functorch.vjp(lambda x:func_model(params,x), x)\n",
    "    vJ   = vJ_fn(cotangents_variable)[0]\n",
    "    vJO  = vJ.sum(1,keepdims=True)-vJ # <vJ|1-I|\n",
    "    return functorch.jvp(lambda x:func_model(params,x), (x,), (vJO,))[1].norm()# average the batch_size also\n",
    "def get_ETrAAT_times(params,x,cotangents_variables):\n",
    "    return vmap(get_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables).mean()\n",
    "def get_TrvJOJv_times(params,x,cotangents_variables):\n",
    "    return vmap(get_TrvJOJv, (None, None, 0 ))(params, x,cotangents_variables).mean()\n",
    "def Normlization_Term_2(params,x,cotangents_variables):\n",
    "    TrvJOJvs,ETrAATs =  vmap(TrvJOJv_and_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables)\n",
    "    return ETrAATs.mean() - torch.var(TrvJOJvs,0).mean()\n",
    "def Normlization_Term_2_Full(params,x):\n",
    "    return (((vmap(jacrev(func_model, argnums=1), (None, 0))(params, x)**2).sum(-1)-1)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6368280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Derivation_Term_2 = jacrev(Normlization_Term_2, argnums=0)(params, x,cotangents_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe3b2ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bccff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_from_optimizer = optimizer.param_groups[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64cc7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(p1,p2) in enumerate(zip(params_from_optimizer,params)):\n",
    "    if not torch.allclose(p1,p2):\n",
    "        print(f\"{i:03d}:p1.shape={p1.shape}:p2.shape={p2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11ac2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1e10d45",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# torch.cuda.empty_cache()\n",
    "# Derivation_Term_2=[0]*len(params)\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(range(32)):\n",
    "#         for j in tqdm(range(64)):\n",
    "#             small_fun = lambda params,x:func_model(params,x)[:,i,j]\n",
    "#             Normlization_Term_2= lambda params,x:((\n",
    "#                 (vmap(jacrev(small_fun, argnums=1), (None, 0))(params, x)**2).sum(-1)-1\n",
    "#                 )**2).mean()\n",
    "#             Derivation_Term_2_tuple = jacrev(Normlization_Term_2, argnums=0)(params, x)\n",
    "#             for k in range(len(Derivation_Term_2_tuple)):\n",
    "#                 Derivation_Term_2[k]+=Derivation_Term_2_tuple[k]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb91c3",
   "metadata": {},
   "source": [
    "输入 70x32x32 过大, 我们做出近邻假设:\n",
    "```\n",
    "                                     (x-h,y-h) (x  ,y-h) (x+h,y-h)\n",
    "位于 (x,y) 的 pixel 的响应只和他周围 一圈 (x-h,y  ) (x ,y  ) (x+h,y)\n",
    "                                     (x-h,y+h) (x  ,y+h) (x+h,y+h)\n",
    "有关\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377e4e3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76271014",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479b498",
   "metadata": {
    "code_folding": [
     10,
     138,
     153
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from typing import cast, List, Optional, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from .optimizer import Optimizer, _use_grad_for_differentiable\n",
    "\n",
    "__all__ = ['Adam', 'adam']\n",
    "\n",
    "class Adam_Nodal(Optimizer):\n",
    "    def __init__(self, params, lambda1=100,lambda2=100,sample_times=10,lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, *, foreach: Optional[bool] = None,\n",
    "                 maximize: bool = False, capturable: bool = False,\n",
    "                 differentiable: bool = False, fused: bool = False):\n",
    "        assert func_model is not None,\"you need provide the function-like model. For example, try `func_model, params = make_functional(model)` \"\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
    "                        maximize=maximize, foreach=foreach, capturable=capturable,\n",
    "                        differentiable=differentiable, fused=fused)\n",
    "        super(Adam, self).__init__(params, defaults)a\n",
    "        assert len(self.param_groups)==1, \"only one group parameter allowed \"\n",
    "        self.params = optimizer.param_groups[0]['params']\n",
    "        self.func_model = func_model\n",
    "        if fused:\n",
    "            if differentiable:\n",
    "                raise RuntimeError(\"`fused` cannot be `differentiable`\")\n",
    "            self._step_supports_amp_scaling = True\n",
    "            # TODO(crcrpar): [low prec params & their higher prec copy]\n",
    "            # Suppor AMP with FP16/BF16 model params which would need\n",
    "            # higher prec copy of params to do update math in higher prec to\n",
    "            # alleviate the loss of information.\n",
    "            if not all(\n",
    "                p.is_cuda and torch.is_floating_point(p)\n",
    "                for pg in self.param_groups for p in pg['params']\n",
    "            ):\n",
    "                raise RuntimeError(\"FusedAdam requires all the params to be CUDA, floating point\")\n",
    "                self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.sample_times = sample_times\n",
    "        self.cotangents_sum_along_x_dimension = None\n",
    "        \n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "            group.setdefault('maximize', False)\n",
    "            group.setdefault('foreach', None)\n",
    "            group.setdefault('capturable', False)\n",
    "            group.setdefault('differentiable', False)\n",
    "            group.setdefault('fused', False)\n",
    "        state_values = list(self.state.values())\n",
    "        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(state_values[0]['step'])\n",
    "        if not step_is_tensor:\n",
    "            for s in state_values:\n",
    "                s['step'] = torch.tensor(float(s['step']))\n",
    "\n",
    "                \n",
    "    def Normlization_Term_1(self,params,x):\n",
    "        if self.cotangents_sum_along_x_dimension is None or self.cotangents_sum_along_x_dimension.shape!=x.shape:\n",
    "            self.cotangents_sum_along_x_dimension = torch.ones_like(x)\n",
    "        return ((functorch.jvp(lambda x:self.func_model(params,x), (x,), (self.cotangents_sum_along_x_dimension,))[1]-1)**2).mean()\n",
    "    def TrvJOJv_and_ETrAAT(self,params,x,cotangents_variable):\n",
    "        _, vJ_fn = functorch.vjp(lambda x:self.func_model(params,x), x)\n",
    "        vJ   = vJ_fn(cotangents_variable)[0]\n",
    "        dims = list(range(1,len(vJ.shape)))\n",
    "        vJO  = vJ.sum(dims,keepdims=True)-vJ # <vJ|1-I|\n",
    "        vJOJv= (vJ*vJO).sum(dims)#should sum over all dimension except batch\n",
    "        return vJOJv, functorch.jvp(lambda x:self.func_model(params,x), (x,), (vJO,))[1].norm()# average the batch_size also\n",
    "    def get_TrvJOJv(self,params,x,cotangents_variable):\n",
    "        _, vJ_fn = functorch.vjp(lambda x:self.func_model(params,x), x)\n",
    "        vJ   = vJ_fn(cotangents_variable)[0]\n",
    "        vJO  = vJ.sum(1,keepdims=True)-vJ # <vJ|1-I|\n",
    "        vJOJv= (vJ*vJO).sum(-1)#should sum over all dimension except batch\n",
    "        return vJOJv\n",
    "    def get_ETrAAT(self,params,x,cotangents_variable):\n",
    "        _, vJ_fn = functorch.vjp(lambda x:self.func_model(params,x), x)\n",
    "        vJ   = vJ_fn(cotangents_variable)[0]\n",
    "        vJO  = vJ.sum(1,keepdims=True)-vJ # <vJ|1-I|\n",
    "        return functorch.jvp(lambda x:self.func_model(params,x), (x,), (vJO,))[1].norm()# average the batch_size also\n",
    "    def get_ETrAAT_times(self,params,x,cotangents_variables):\n",
    "        return vmap(get_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables).mean()\n",
    "    def get_TrvJOJv_times(self,params,x,cotangents_variables):\n",
    "        return vmap(self.get_TrvJOJv, (None, None, 0 ))(params, x,cotangents_variables).mean()\n",
    "    def Normlization_Term_2(self,params,x,cotangents_variables):\n",
    "        TrvJOJvs,ETrAATs =  vmap(self.TrvJOJv_and_ETrAAT, (None, None, 0 ))(params, x,cotangents_variables)\n",
    "        return ETrAATs.mean() - torch.var(TrvJOJvs,0).mean()\n",
    "    def Normlization_Term_2_Full(model, params,x):\n",
    "        return (((vmap(jacrev(self.func_model, argnums=1), (None, 0))(params, x)**2).sum(-1)-1)**2).mean()   \n",
    "    \n",
    "            \n",
    "    @_use_grad_for_differentiable\n",
    "    def step(self, _input,_output, model):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "            grad_scaler (:class:`torch.cuda.amp.GradScaler`, optional): A GradScaler which is\n",
    "                supplied from ``grad_scaler.step(optimizer)``.\n",
    "        \"\"\"\n",
    "        self._cuda_graph_capture_health_check()\n",
    "\n",
    "        # we will firstly update the grad from regularzation term\n",
    "        # in this case is \n",
    "        #     $L1 = \\sum_\\gamma(\\sum_\\alpha J_\\alpha^{\\gamma}-1)^2$\n",
    "        #     $L2 = \\lambda_2[\\sum_\\gamma A_{\\gamma\\gamma}^2]$ where $A=J(\\mathbf{1}-I)J^T$\n",
    "        #          for L2 part, we will use Hutchinson Method to estimate the value and do backprogation\n",
    "        #          $\\sum_\\gamma A_{\\gamma\\gamma}^2 = 2\\{E[\\Tr_m(AA^T)] - Var[\\Tr_m(A)]\\}$\n",
    "        \n",
    "        self.func_model, params =  make_functional(model)\n",
    "        shape = _output.shape\n",
    "        cotangents_variables = torch.randint(2,(self.sample_times,*shape)).cuda()*2-1\n",
    "        with torch.no_grad():\n",
    "            Derivation_Term_1 = jacrev(self.Normlization_Term_1, argnums=0)(params, _input)\n",
    "            Derivation_Term_2 = jacrev(self.Normlization_Term_2, argnums=0)(params, _input,cotangents_variables)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            beta1, beta2 = group['betas']\n",
    "\n",
    "            grad_scale = None\n",
    "            found_inf = None\n",
    "            if group['fused'] and grad_scaler is not None:\n",
    "                grad_scale = grad_scaler._get_scale_async()\n",
    "                device = grad_scale.device\n",
    "                grad_scale = _MultiDeviceReplicator(grad_scale)\n",
    "                found_inf = _get_fp16AMP_params(optimizer=self, grad_scaler=grad_scaler, device=device)\n",
    "\n",
    "            for p ,d1,d2 in zip(group['params'],Derivation_Term_1,Derivation_Term_2):\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    if p.grad.is_sparse:\n",
    "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                    grads.append(p.grad+d1+d2)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    # Lazy state initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = (\n",
    "                            torch.zeros((1,), dtype=torch.float, device=p.device)\n",
    "                            if self.defaults['capturable'] or self.defaults['fused']\n",
    "                            else torch.tensor(0.)\n",
    "                        )\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if group['amsgrad']:\n",
    "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                    if group['amsgrad']:\n",
    "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "                    if group['differentiable'] and state['step'].requires_grad:\n",
    "                        raise RuntimeError('`requires_grad` is not supported for `step` in differentiable mode')\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            adam(params_with_grad,\n",
    "                 grads,\n",
    "                 exp_avgs,\n",
    "                 exp_avg_sqs,\n",
    "                 max_exp_avg_sqs,\n",
    "                 state_steps,\n",
    "                 amsgrad=group['amsgrad'],\n",
    "                 beta1=beta1,\n",
    "                 beta2=beta2,\n",
    "                 lr=group['lr'],\n",
    "                 weight_decay=group['weight_decay'],\n",
    "                 eps=group['eps'],\n",
    "                 maximize=group['maximize'],\n",
    "                 foreach=group['foreach'],\n",
    "                 capturable=group['capturable'],\n",
    "                 differentiable=group['differentiable'],\n",
    "                 fused=group['fused'],\n",
    "                 grad_scale=grad_scale,\n",
    "                 found_inf=found_inf)\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af2e18",
   "metadata": {},
   "source": [
    "#### 如果我们直接计算出来梯度, 是不是可以减少中间变量的占用\n",
    "\n",
    "我们接下来要利用的内置函数 vpj, vjp, pjv 实现的是这几个量\n",
    "- $\\sum_\\alpha J_\\alpha^\\gamma$\n",
    "- $\\sum_\\alpha H_{\\alpha\\beta}^\\gamma$\n",
    "- $\\sum_\\alpha (J_\\alpha^\\gamma)^2$\n",
    "- $\\sum_\\alpha J_\\alpha^\\gamma H_{\\alpha\\beta}^\\gamma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd47ad3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### $\\sum_\\alpha J_\\alpha^\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "da899e32",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jvp_from_batch_J = batch_J.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9381bad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "首先比较 torch 原生的和functorch 的 jvp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4641fc59",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_pytorch_jvp,jvp_result_pytorch_jvp = torch.autograd.functional.jvp(model, x, torch.ones(7,3))\n",
    "output_functorch_jvp,jvp_result_functorch_jvp = functorch.jvp(model, (x,), (torch.ones(7,3),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "540633cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DistBackward0>)\n",
      "tensor(0., grad_fn=<DistBackward0>)\n",
      "tensor(3.4459e-07, grad_fn=<DistBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# result \n",
    "print(torch.dist(output_pytorch_jvp,output_functorch_jvp))\n",
    "print(torch.dist(jvp_result_pytorch_jvp,jvp_result_functorch_jvp))\n",
    "print(torch.dist(jvp_result_pytorch_jvp,jvp_from_batch_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c6fb6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "注意到上面的function 没有实现 vmap, 我们现在来尝试用vmap写出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "21ef609a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "func = lambda x:functorch.jvp(model, (x,), (torch.ones(3),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "139f7932",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_batch_functorch_jvp,jvp_batch_result_functorch_jvp = vmap(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "daa75c59",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5825e-07, grad_fn=<DistBackward0>)\n",
      "tensor(3.3542e-07, grad_fn=<DistBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.dist(output_batch_functorch_jvp,output_pytorch_jvp))\n",
    "print(torch.dist(jvp_batch_result_functorch_jvp,jvp_from_batch_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92d45a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "下面进行压力测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "65507261",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "B=200\n",
    "I=100\n",
    "O=300\n",
    "model= MyModel(I, O)\n",
    "x    = torch.randn(B, I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4a23dd5c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 µs ± 28.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output_pytorch_jvp,jvp_result_pytorch_jvp = torch.autograd.functional.jvp(model, x, torch.ones(B,I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b1d7bd9d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 µs ± 25.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output_pytorch_jvp,jvp_result_pytorch_jvp = functorch.jvp(model, (x,), (torch.ones(B,I),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d89050f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "func = lambda x:functorch.jvp(model, (x,), (torch.ones(I),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "389a86cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685 µs ± 40.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output_batch_functorch_jvp,jvp_batch_result_functorch_jvp = vmap(func)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858ebcf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "结论: 不用 vmap 会更加好!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82676b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#####  $\\sum_\\alpha H_{\\alpha\\beta}^\\gamma = \\partial_{\\beta}\\sum_\\alpha J^\\gamma_{\\alpha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "72fc86e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from functorch import make_functional, vmap, grad\n",
    "model= MyModel(3, 4)\n",
    "x    = torch.randn(7, 3)\n",
    "func_model, params = make_functional(model)\n",
    "w    = params[0]\n",
    "batch_J= vmap(jacrev(func_model, argnums=1), (None, 0))(params, x)\n",
    "batch_H= jacrev(vmap(jacrev(func_model, argnums=1), (None, 0)), argnums=0)(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "997934a5",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 3, 4, 3])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_H[0].shape# 后面的 (4,3) 是 weight 的形状, 7 是 batch, 4 是输出的Dim, 3是alpha "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ace874",
   "metadata": {
    "hidden": true
   },
   "source": [
    "指标 $\\beta$ 是指代的第几个 parameter, 也就是 `batch_H` 这个 tuple 里面的第几个量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e8a3a121",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2781e-06, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = lambda params,x:functorch.jvp(lambda x:func_model(params,x), (x,), (torch.ones(7,3),))[1]\n",
    "batch_H_sum_from_jvp= jacrev(func, argnums=0)(params, x)\n",
    "#### full computation\n",
    "batch_H= jacrev(vmap(jacrev(func_model, argnums=1), (None, 0)), argnums=0)(params, x)\n",
    "jvp_from_batch_H = batch_H[0].sum(2)\n",
    "torch.dist(batch_H[0].sum(2),batch_H_sum_from_jvp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc2c88",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dbf99",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_, vjp_fn = vjp(func_model, x)\n",
    "\n",
    "ft_jacobian, = vmap(vjp_fn)(unit_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9b8ca5d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "jvp(f, primals, tangents): Expected primals and tangents to have the same python structure. For example, if primals is a tuple of 3 tensors, tangents also must be. Got primals with structure TreeSpec(tuple, None, [TreeSpec(tuple, None, [*]), *]) and tangents with structure TreeSpec(tuple, None, [*, *])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [152], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_J\u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/functorch/_src/vmap.py:365\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 365\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [148], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(*args)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs:\u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/functorch/_src/eager_transforms.py:786\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(func, primals, tangents, strict, has_aux)\u001b[0m\n\u001b[1;32m    784\u001b[0m flat_tangents, tangents_spec \u001b[38;5;241m=\u001b[39m tree_flatten(tangents)\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m primals_spec \u001b[38;5;241m!=\u001b[39m tangents_spec:\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    787\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjvp_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Expected primals and tangents to have the same python \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructure. For example, if primals is a tuple of 3 tensors, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtangents also must be. Got primals with structure \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprimals_spec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand tangents with structure \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtangents_spec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    791\u001b[0m assert_non_empty_list_of_tensors(flat_primals, jvp_str, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimals\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    792\u001b[0m assert_non_empty_list_of_tensors(flat_tangents, jvp_str, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtangents\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: jvp(f, primals, tangents): Expected primals and tangents to have the same python structure. For example, if primals is a tuple of 3 tensors, tangents also must be. Got primals with structure TreeSpec(tuple, None, [TreeSpec(tuple, None, [*]), *]) and tangents with structure TreeSpec(tuple, None, [*, *])"
     ]
    }
   ],
   "source": [
    "batch_J= vmap(func, ((None, 0), (None,None)) \n",
    "             )( (params, x), (torch.ones(4),torch.ones(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464bad7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_, vjp_fn = vjp(partial(predict, weight, bias), x)\n",
    "\n",
    "ft_jacobian, = vmap(vjp_fn)(unit_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8ce7f93d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functorch import jvp\n",
    "x = torch.randn(5)\n",
    "y = torch.randn(5)\n",
    "f = lambda x, y: (x * y)\n",
    "_, output = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0e460b48",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.1409, -0.3539, -0.2227, -0.5354, -2.3300])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd1f7a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ea9b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_constrained_gradient(grad, hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb53a9",
   "metadata": {
    "code_folding": [
     1,
     6,
     19,
     95,
     148,
     229
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Adam(Optimizer):\n",
    "    r\"\"\"Implements Adam algorithm.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, *, foreach: Optional[bool] = None,\n",
    "                 maximize: bool = False, capturable: bool = False):\n",
    "        if not 0.0 <= lr:raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= weight_decay:raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
    "                        maximize=maximize, foreach=foreach, capturable=capturable)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "            group.setdefault('maximize', False)\n",
    "            group.setdefault('foreach', None)\n",
    "            group.setdefault('capturable', False)\n",
    "        state_values = list(self.state.values())\n",
    "        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(state_values[0]['step'])\n",
    "        if not step_is_tensor:\n",
    "            for s in state_values:\n",
    "                s['step'] = torch.tensor(float(s['step']))\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, hessian,closure=None):\n",
    "        self._cuda_graph_capture_health_check()\n",
    "\n",
    "        loss = None\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            beta1, beta2 = group['betas']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    if p.grad.is_sparse:\n",
    "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                    grads.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    # Lazy state initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \\\n",
    "                            if self.defaults['capturable'] else torch.tensor(0.)\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if group['amsgrad']:\n",
    "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                    if group['amsgrad']:\n",
    "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            adam(params_with_grad,\n",
    "                 grads,\n",
    "                 exp_avgs,\n",
    "                 exp_avg_sqs,\n",
    "                 max_exp_avg_sqs,\n",
    "                 state_steps,\n",
    "                 amsgrad=group['amsgrad'],\n",
    "                 beta1=beta1,\n",
    "                 beta2=beta2,\n",
    "                 lr=group['lr'],\n",
    "                 weight_decay=group['weight_decay'],\n",
    "                 eps=group['eps'],\n",
    "                 maximize=group['maximize'],\n",
    "                 foreach=group['foreach'],\n",
    "                 capturable=group['capturable'])\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def adam(params: List[Tensor],\n",
    "         grads: List[Tensor],\n",
    "         exp_avgs: List[Tensor],\n",
    "         exp_avg_sqs: List[Tensor],\n",
    "         max_exp_avg_sqs: List[Tensor],\n",
    "         state_steps: List[Tensor],\n",
    "         # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
    "         # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
    "         foreach: bool = None,\n",
    "         capturable: bool = False,\n",
    "         *,\n",
    "         amsgrad: bool,\n",
    "         beta1: float,\n",
    "         beta2: float,\n",
    "         lr: float,\n",
    "         weight_decay: float,\n",
    "         eps: float,\n",
    "         maximize: bool):\n",
    "    r\"\"\"Functional API that performs Adam algorithm computation.\n",
    "    See :class:`~torch.optim.Adam` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    if not all([isinstance(t, torch.Tensor) for t in state_steps]):\n",
    "        raise RuntimeError(\"API has changed, `state_steps` argument must contain a list of singleton tensors\")\n",
    "\n",
    "    if foreach is None:\n",
    "        # Placeholder for more complex foreach logic to be added when value is not set\n",
    "        foreach = False\n",
    "\n",
    "    if foreach and torch.jit.is_scripting():\n",
    "        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n",
    "\n",
    "    if foreach and not torch.jit.is_scripting():\n",
    "        func = _multi_tensor_adam\n",
    "    else:\n",
    "        func = _single_tensor_adam\n",
    "\n",
    "    func(params,\n",
    "         grads,\n",
    "         exp_avgs,\n",
    "         exp_avg_sqs,\n",
    "         max_exp_avg_sqs,\n",
    "         state_steps,\n",
    "         amsgrad=amsgrad,\n",
    "         beta1=beta1,\n",
    "         beta2=beta2,\n",
    "         lr=lr,\n",
    "         weight_decay=weight_decay,\n",
    "         eps=eps,\n",
    "         maximize=maximize,\n",
    "         capturable=capturable)\n",
    "\n",
    "\n",
    "def _single_tensor_adam(params: List[Tensor],\n",
    "                        grads: List[Tensor],\n",
    "                        exp_avgs: List[Tensor],\n",
    "                        exp_avg_sqs: List[Tensor],\n",
    "                        max_exp_avg_sqs: List[Tensor],\n",
    "                        state_steps: List[Tensor],\n",
    "                        *,\n",
    "                        amsgrad: bool,\n",
    "                        beta1: float,\n",
    "                        beta2: float,\n",
    "                        lr: float,\n",
    "                        weight_decay: float,\n",
    "                        eps: float,\n",
    "                        maximize: bool,\n",
    "                        capturable: bool):\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        grad       = grads[i] if not maximize else -grads[i]\n",
    "        exp_avg    = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        step_t = state_steps[i]\n",
    "\n",
    "        if capturable:\n",
    "            assert param.is_cuda and step_t.is_cuda, \"If capturable=True, params and state_steps must be CUDA tensors.\"\n",
    "\n",
    "        # update step\n",
    "        step_t += 1\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            grad = grad.add(param, alpha=weight_decay)\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
    "\n",
    "        if capturable:\n",
    "            step = step_t\n",
    "\n",
    "            # 1 - beta1 ** step can't be captured in a CUDA graph, even if step is a CUDA tensor\n",
    "            # (incurs \"RuntimeError: CUDA error: operation not permitted when stream is capturing\")\n",
    "            bias_correction1 = 1 - torch.pow(beta1, step)\n",
    "            bias_correction2 = 1 - torch.pow(beta2, step)\n",
    "\n",
    "            step_size = lr / bias_correction1\n",
    "            step_size_neg = step_size.neg()\n",
    "\n",
    "            bias_correction2_sqrt = bias_correction2.sqrt()\n",
    "\n",
    "            if amsgrad:\n",
    "                # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
    "                # Uses the max. for normalizing running avg. of gradient\n",
    "                # Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write\n",
    "                # (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)\n",
    "                denom = (max_exp_avg_sqs[i].sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(eps / step_size_neg)\n",
    "            else:\n",
    "                denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(eps / step_size_neg)\n",
    "\n",
    "            param.addcdiv_(exp_avg, denom)\n",
    "        else:\n",
    "            step = step_t.item()\n",
    "\n",
    "            bias_correction1 = 1 - beta1 ** step\n",
    "            bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "            step_size = lr / bias_correction1\n",
    "\n",
    "            bias_correction2_sqrt = math.sqrt(bias_correction2)\n",
    "\n",
    "            if amsgrad:\n",
    "                # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
    "                # Use the max. for normalizing running avg. of gradient\n",
    "                denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n",
    "            else:\n",
    "                denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
    "\n",
    "            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "\n",
    "def _multi_tensor_adam(params: List[Tensor],\n",
    "                       grads: List[Tensor],\n",
    "                       exp_avgs: List[Tensor],\n",
    "                       exp_avg_sqs: List[Tensor],\n",
    "                       max_exp_avg_sqs: List[Tensor],\n",
    "                       state_steps: List[Tensor],\n",
    "                       *,\n",
    "                       amsgrad: bool,\n",
    "                       beta1: float,\n",
    "                       beta2: float,\n",
    "                       lr: float,\n",
    "                       weight_decay: float,\n",
    "                       eps: float,\n",
    "                       maximize: bool,\n",
    "                       capturable: bool):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    if capturable:\n",
    "        assert all(p.is_cuda and step.is_cuda for p, step in zip(params, state_steps)), \\\n",
    "            \"If capturable=True, params and state_steps must be CUDA tensors.\"\n",
    "\n",
    "    if maximize:\n",
    "        grads = torch._foreach_neg(tuple(grads))  # type: ignore[assignment]\n",
    "\n",
    "    # update steps\n",
    "    torch._foreach_add_(state_steps, 1)\n",
    "\n",
    "    if weight_decay != 0:\n",
    "        torch._foreach_add_(grads, params, alpha=weight_decay)\n",
    "\n",
    "    # Decay the first and second moment running average coefficient\n",
    "    torch._foreach_mul_(exp_avgs, beta1)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)\n",
    "\n",
    "    torch._foreach_mul_(exp_avg_sqs, beta2)\n",
    "    torch._foreach_addcmul_(exp_avg_sqs, grads, grads, 1 - beta2)\n",
    "\n",
    "    if capturable:\n",
    "        # TODO: use foreach_pow if/when foreach_pow is added\n",
    "        bias_correction1 = [torch.pow(beta1, step) for step in state_steps]\n",
    "        bias_correction2 = [torch.pow(beta2, step) for step in state_steps]\n",
    "        # foreach_sub doesn't allow a scalar as the first arg\n",
    "        torch._foreach_sub_(bias_correction1, 1)\n",
    "        torch._foreach_sub_(bias_correction2, 1)\n",
    "        torch._foreach_neg_(bias_correction1)\n",
    "        torch._foreach_neg_(bias_correction2)\n",
    "\n",
    "        # foreach_div doesn't allow a scalar as the first arg\n",
    "        step_size = torch._foreach_div(bias_correction1, lr)\n",
    "        torch._foreach_reciprocal_(step_size)\n",
    "        torch._foreach_neg_(step_size)\n",
    "\n",
    "        bias_correction2_sqrt = torch._foreach_sqrt(bias_correction2)\n",
    "\n",
    "        if amsgrad:\n",
    "            # Maintains the maximum of all 2nd moment running avg. till now\n",
    "            max_exp_avg_sqs = torch._foreach_maximum(max_exp_avg_sqs, exp_avg_sqs)  # type: ignore[assignment]\n",
    "\n",
    "            # Use the max. for normalizing running avg. of gradient\n",
    "            max_exp_avg_sq_sqrt = torch._foreach_sqrt(max_exp_avg_sqs)\n",
    "            # Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write\n",
    "            # (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)\n",
    "            torch._foreach_div_(max_exp_avg_sq_sqrt, torch._foreach_mul(bias_correction2_sqrt, step_size))\n",
    "            eps_over_step_size = torch._foreach_div(step_size, eps)\n",
    "            torch._foreach_reciprocal_(eps_over_step_size)\n",
    "            denom = torch._foreach_add(max_exp_avg_sq_sqrt, eps_over_step_size)\n",
    "        else:\n",
    "            exp_avg_sq_sqrt = torch._foreach_sqrt(exp_avg_sqs)\n",
    "            torch._foreach_div_(exp_avg_sq_sqrt, torch._foreach_mul(bias_correction2_sqrt, step_size))\n",
    "            eps_over_step_size = torch._foreach_div(step_size, eps)\n",
    "            torch._foreach_reciprocal_(eps_over_step_size)\n",
    "            denom = torch._foreach_add(exp_avg_sq_sqrt, eps_over_step_size)\n",
    "\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom)\n",
    "    else:\n",
    "        bias_correction1 = [1 - beta1 ** step.item() for step in state_steps]\n",
    "        bias_correction2 = [1 - beta2 ** step.item() for step in state_steps]\n",
    "\n",
    "        step_size = [(lr / bc) * -1 for bc in bias_correction1]\n",
    "\n",
    "        bias_correction2_sqrt = [math.sqrt(bc) for bc in bias_correction2]\n",
    "\n",
    "        if amsgrad:\n",
    "            # Maintains the maximum of all 2nd moment running avg. till now\n",
    "            max_exp_avg_sqs = torch._foreach_maximum(max_exp_avg_sqs, exp_avg_sqs)  # type: ignore[assignment]\n",
    "\n",
    "            # Use the max. for normalizing running avg. of gradient\n",
    "            max_exp_avg_sq_sqrt = torch._foreach_sqrt(max_exp_avg_sqs)\n",
    "            torch._foreach_div_(max_exp_avg_sq_sqrt, bias_correction2_sqrt)\n",
    "            denom = torch._foreach_add(max_exp_avg_sq_sqrt, eps)\n",
    "        else:\n",
    "            exp_avg_sq_sqrt = torch._foreach_sqrt(exp_avg_sqs)\n",
    "            torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n",
    "            denom = torch._foreach_add(exp_avg_sq_sqrt, eps)\n",
    "\n",
    "        torch._foreach_addcdiv_(params, exp_avgs, denom, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92215445",
   "metadata": {
    "code_folding": [
     14
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhess_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from .architect import Architect\n",
    "from .history import History\n",
    "\n",
    "\n",
    "def _concat(xs):\n",
    "    return torch.cat([x.view(-1) for x in xs])\n",
    "\n",
    "\n",
    "# TODO: modify to include learnable edge weights.\n",
    "class ArchitectDARTS(Architect):\n",
    "    def __init__(self, model, args, writer):\n",
    "        super(ArchitectDARTS, self).__init__(model, args, writer)\n",
    "\n",
    "        self.network_momentum = args.train.momentum\n",
    "        self.network_weight_decay = args.train.weight_decay\n",
    "        if args.search.gd:\n",
    "            self.optimizer = torch.optim.SGD(\n",
    "                self._arch_parameters,\n",
    "                lr=args.search.arch_learning_rate,\n",
    "                weight_decay=args.search.arch_weight_decay,\n",
    "            )\n",
    "        else:\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                self._arch_parameters,\n",
    "                lr=args.search.arch_learning_rate,\n",
    "                betas=(0.5, 0.999),\n",
    "                weight_decay=args.search.arch_weight_decay,\n",
    "            )\n",
    "        self.history = History(\n",
    "            model, self, to_save=(\"alphas\", \"l2_norm\", \"l2_norm_from_init\")\n",
    "        )\n",
    "        self.history.dict[\"grads\"] = {}\n",
    "        for v in [\"alphas\", \"edges\"]:\n",
    "            self.history.dict[\"grads\"][v] = {}\n",
    "            for ct in self.cell_types:\n",
    "                self.history.dict[\"grads\"][v][ct] = []\n",
    "\n",
    "    def initialize_alphas(self):\n",
    "        k = self.n_edges\n",
    "        num_ops = self.model._num_ops\n",
    "        for ct in self.cell_types:\n",
    "            self.alphas[ct] = Variable(\n",
    "                1e-3 * torch.randn(k, num_ops).cuda(), requires_grad=True\n",
    "            )\n",
    "        self._arch_parameters = [self.alphas[ct] for ct in self.cell_types]\n",
    "\n",
    "    def initialize_edge_weights(self):\n",
    "        for ct in self.cell_types:\n",
    "            self.edges[ct] = Variable(\n",
    "                torch.ones(self.n_edges).cuda(), requires_grad=False\n",
    "            )\n",
    "\n",
    "    def get_alphas(self):\n",
    "        return {ct: F.softmax(self.alphas[ct], dim=-1) for ct in self.cell_types}\n",
    "\n",
    "    def get_edge_weights(self):\n",
    "        return self.edges\n",
    "\n",
    "    def step(self,input_train,target_train,input_valid, target_valid,eta,network_optimizer,unrolled,**kwargs\n",
    "    ):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.zero_arch_var_grad()\n",
    "        self.set_model_alphas()\n",
    "        self.set_model_edge_weights()\n",
    "\n",
    "        if unrolled:\n",
    "            self._backward_step_unrolled(\n",
    "                input_train,\n",
    "                target_train,\n",
    "                input_valid,\n",
    "                target_valid,\n",
    "                eta,\n",
    "                network_optimizer,\n",
    "            )\n",
    "        else:\n",
    "            self._backward_step(input_valid, target_valid)\n",
    "        self.optimizer.step()\n",
    "        self.steps += 1\n",
    "\n",
    "    def _backward_step(self, input_valid, target_valid):\n",
    "        loss = self.model._loss(input_valid, target_valid)\n",
    "        loss.backward()\n",
    "        for ct in self.cell_types:\n",
    "            self.history.dict[\"grads\"][\"alphas\"][ct].append(\n",
    "                self.alphas[ct].grad.data.cpu().numpy()\n",
    "            )\n",
    "\n",
    "    def _construct_model_from_theta(self, theta):\n",
    "        model_new = self.model.new()\n",
    "        model_dict = self.model.state_dict()\n",
    "\n",
    "        params = {}\n",
    "        offset = 0\n",
    "        for k, v in self.model.named_parameters():\n",
    "            v_length = np.prod(v.size())\n",
    "            params[k] = theta[offset : offset + v_length].view(v.size())\n",
    "            offset += v_length\n",
    "\n",
    "        assert offset == len(theta)\n",
    "        model_dict.update(params)\n",
    "        model_new.load_state_dict(model_dict)\n",
    "        self.initialize_new_model_arch_params(model_new)\n",
    "\n",
    "        return model_new.cuda()\n",
    "\n",
    "    def copy_architecture_params(self):\n",
    "        new_alphas = {\n",
    "            ct: Variable(self.alphas[ct].data.clone().cuda(), requires_grad=True)\n",
    "            for ct in self.cell_types\n",
    "        }\n",
    "        new_edges = {\n",
    "            ct: Variable(self.edges[ct].data.clone().cuda(), requires_grad=False)\n",
    "            for ct in self.cell_types\n",
    "        }\n",
    "        return new_alphas, new_edges\n",
    "\n",
    "    def initialize_new_model_arch_params(self, new_model):\n",
    "        self._new_alphas, self._new_edges = self.copy_architecture_params()\n",
    "        alphas = {ct: F.softmax(self._new_alphas[ct], dim=-1) for ct in self.cell_types}\n",
    "        new_model.set_alphas(alphas)\n",
    "        new_model.set_edge_weights(self._new_edges)\n",
    "        new_model.drop_path_prob = self.model.drop_path_prob\n",
    "\n",
    "    def _compute_unrolled_model(self, input, target, eta, network_optimizer):\n",
    "        loss = self.model._loss(input, target)\n",
    "        theta = _concat(self.model.parameters()).data\n",
    "        try:\n",
    "            moment = _concat(\n",
    "                network_optimizer.state[v][\"momentum_buffer\"]\n",
    "                for v in self.model.parameters()\n",
    "            ).mul_(self.network_momentum)\n",
    "        except:\n",
    "            moment = torch.zeros_like(theta)\n",
    "        dtheta = (\n",
    "            _concat(torch.autograd.grad(loss, self.model.parameters())).data\n",
    "            + self.network_weight_decay * theta\n",
    "        )\n",
    "        self.set_model_alphas()\n",
    "        self.set_model_edge_weights()\n",
    "        unrolled_model = self._construct_model_from_theta(\n",
    "            theta.sub(eta, moment + dtheta)\n",
    "        )\n",
    "        return unrolled_model\n",
    "\n",
    "    def _hessian_vector_product(self, vector, input, target, r=1e-2):\n",
    "        R = r / _concat(vector).norm()\n",
    "        for p, v in zip(self.model.parameters(), vector):\n",
    "            p.data.add_(R, v)\n",
    "        loss = self.model._loss(input, target)\n",
    "        grads_p = torch.autograd.grad(loss, self._arch_parameters)\n",
    "        self.set_model_alphas()\n",
    "        self.set_model_edge_weights()\n",
    "\n",
    "        for p, v in zip(self.model.parameters(), vector):\n",
    "            p.data.sub_(2 * R, v)\n",
    "        loss = self.model._loss(input, target)\n",
    "        grads_n = torch.autograd.grad(loss, self._arch_parameters)\n",
    "        self.set_model_alphas()\n",
    "        self.set_model_edge_weights()\n",
    "\n",
    "        for p, v in zip(self.model.parameters(), vector):\n",
    "            p.data.add_(R, v)\n",
    "\n",
    "        return [(x - y).div_(2 * R) for x, y in zip(grads_p, grads_n)]\n",
    "\n",
    "    def _backward_step_unrolled(\n",
    "        self,\n",
    "        input_train,\n",
    "        target_train,\n",
    "        input_valid,\n",
    "        target_valid,\n",
    "        eta,\n",
    "        network_optimizer,\n",
    "    ):\n",
    "        unrolled_model = self._compute_unrolled_model(\n",
    "            input_train, target_train, eta, network_optimizer\n",
    "        )\n",
    "        unrolled_loss = unrolled_model._loss(input_valid, target_valid)\n",
    "\n",
    "        unrolled_loss.backward()\n",
    "        dalpha = [self._new_alphas[ct].grad for ct in self.cell_types]\n",
    "        vector = [v.grad.data for v in unrolled_model.parameters()]\n",
    "        implicit_grads = self._hessian_vector_product(vector, input_train, target_train)\n",
    "\n",
    "        for g, ig in zip(dalpha, implicit_grads):\n",
    "            g.data.sub_(eta, ig.data)\n",
    "\n",
    "        for v, g in zip(self._arch_parameters, dalpha):\n",
    "            if v.grad is None:\n",
    "                v.grad = Variable(g.data)\n",
    "            else:\n",
    "                v.grad.data.copy_(g.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7097ff29",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.8910, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.8857],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [5.9064, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.4972]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pow_reducer(x):\n",
    "    return x.pow(3).sum()\n",
    "inputs = torch.rand(2, 2)\n",
    "torch.autograd.functional.hessian(pow_reducer, inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc77af441bfc2c62c4c17d61422da4a44e42f2508bbc97d57f2d3e249317e47a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
