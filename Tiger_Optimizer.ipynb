{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd2806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e782e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.othermodels import CK_LgNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fe49424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is pre-set model, we disable all config\n"
     ]
    }
   ],
   "source": [
    "model = CK_LgNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc3eaf87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, p in model.named_parameters() if 'beta' in name ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12ead4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = Tiger([{'params': [p for name, p in model.named_parameters() if 'bias' in name ],'type':'tensor_adding'},\n",
    "                   {'params': [p for name, p in model.named_parameters() if 'bias' not in name ],'type':'tensor_contraction'}\\\n",
    "                  ],lr =0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a443170",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10,2)\n",
    "b = torch.randn(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25388e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()(model(a),b)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbafb77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,2,3,4,5).norm(dim=-1,keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ce95316",
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Callable\n",
    "\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# update functions\n",
    "\n",
    "def update_fn(p, grad, m, lr, wd, beta1, beta2):\n",
    "    ### do dynamic lr configuration as the name of parameter\n",
    "    #### 'bias|beta|gamma' -> 0.5*lr\n",
    "    #### 'embeddings' -> lr*sqrt(p.)\n",
    "    #### 'others'     -> lr*sqrt(p.)\n",
    "    p.data.mul_(1 - lr * wd)\n",
    "    m.mul_(beta1).add(grad, alpha = 1 - beta2)\n",
    "    p.add(m.sign_(),alpha=-lr)\n",
    "\n",
    "# class\n",
    " \n",
    "class Tiger(Optimizer):\n",
    "    def __init__(self,params,\n",
    "        lr: float = 1e-4,\n",
    "        betas: Tuple[float, float] = (0.95, 0.95),\n",
    "        weight_decay: float = 0.0\n",
    "    ):\n",
    "        assert lr > 0.\n",
    "        assert all([0. <= beta <= 1. for beta in betas])\n",
    "        assert len(params) == 2, print('please provide 2 type parameter: tensor_adding| tensor_contraction. By {\\'type\\':\\'tensor_adding\\'}')\n",
    "        assert betas[0] == betas[1], print(\"tiger optimizer requires same beta1 and beta2\")\n",
    "        \n",
    "        defaults = dict(\n",
    "            lr = lr,\n",
    "            betas = betas,\n",
    "            weight_decay = weight_decay\n",
    "        )\n",
    "        \n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        self.update_fn = update_fn\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(\n",
    "        self,\n",
    "        closure: Optional[Callable] = None\n",
    "    ):\n",
    "\n",
    "        loss = None\n",
    "        if exists(closure):\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            \n",
    "            for p in filter(lambda p: exists(p.grad), group['params']):\n",
    "\n",
    "                grad, lr, wd, beta1, beta2, state = p.grad, group['lr'], group['weight_decay'], *group['betas'], self.state[p]\n",
    "                if group['type'] =='bias':\n",
    "                    lr, wd = lr*0.5 , 0\n",
    "                else:\n",
    "                    lr = lr*p.data.norm()\n",
    "                # init state - exponential moving average of gradient values\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "\n",
    "                exp_avg = state['exp_avg']\n",
    "\n",
    "                self.update_fn(\n",
    "                    p,\n",
    "                    grad,\n",
    "                    exp_avg,\n",
    "                    lr,\n",
    "                    wd,\n",
    "                    beta1,\n",
    "                    beta2\n",
    "                )\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
