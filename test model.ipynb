{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7562a342",
   "metadata": {
    "code_folding": [],
    "heading_collapsed": true
   },
   "source": [
    "#### 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fcfaa0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29c1beb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from networks.utils.Attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b50da85c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_path = \"checkpoints/WeathBench32x64MultibranchRandom/LgNet_MultiBranch-DecodeEncodeLgNet/ts_2_pretrain-2D706N_per_1_step/03_04_16_35_38553-seed_73001/pretrain_latest.pt\"\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61fe09df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lets pick out the 24 hour weight train\n",
    "new_ckpt_path = {}\n",
    "for key, val in ckpt['model'].items():\n",
    "    if \"net.layers.2\" in key : continue\n",
    "    if \"net.final\" in key : continue \n",
    "    if \"new_branch\" in key:\n",
    "        if \"new_branch.2.\" in key:\n",
    "            key = key.replace(\"new_branch.2.0.\",\"backbone.net.layers.2.\")\n",
    "            key = key.replace(\"new_branch.2.1.\",\"backbone.net.final.\")\n",
    "            new_ckpt_path[key] = val\n",
    "        continue\n",
    "    new_ckpt_path[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c377e742",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_path = \"checkpoints/WeathBench32x64/CK_LgNet/ts_2_pretrain-2D706N_per_24_step/branch_from_multibranch/pretrain_latest.pt\"\n",
    "new_ckpt={}\n",
    "new_ckpt['model'] = new_ckpt_path\n",
    "torch.save(new_ckpt,target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "685825ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.net.patch_embed.proj.weight\n",
      "backbone.net.patch_embed.proj.bias\n",
      "backbone.net.layers.0.blocks.0.norm.weight\n",
      "backbone.net.layers.0.blocks.0.norm.bias\n",
      "backbone.net.layers.0.blocks.0.attn.qkv.weight\n",
      "backbone.net.layers.0.blocks.0.attn.qkv.bias\n",
      "backbone.net.layers.0.blocks.0.attn.proj.weight\n",
      "backbone.net.layers.0.blocks.0.attn.proj.bias\n",
      "backbone.net.layers.0.blocks.0.norm2.weight\n",
      "backbone.net.layers.0.blocks.0.norm2.bias\n",
      "backbone.net.layers.0.blocks.0.mlp.fc1.weight\n",
      "backbone.net.layers.0.blocks.0.mlp.fc1.bias\n",
      "backbone.net.layers.0.blocks.0.mlp.fc2.weight\n",
      "backbone.net.layers.0.blocks.0.mlp.fc2.bias\n",
      "backbone.net.layers.0.blocks.1.norm.weight\n",
      "backbone.net.layers.0.blocks.1.norm.bias\n",
      "backbone.net.layers.0.blocks.1.attn.qkv.weight\n",
      "backbone.net.layers.0.blocks.1.attn.qkv.bias\n",
      "backbone.net.layers.0.blocks.1.attn.proj.weight\n",
      "backbone.net.layers.0.blocks.1.attn.proj.bias\n",
      "backbone.net.layers.0.blocks.1.norm2.weight\n",
      "backbone.net.layers.0.blocks.1.norm2.bias\n",
      "backbone.net.layers.0.blocks.1.mlp.fc1.weight\n",
      "backbone.net.layers.0.blocks.1.mlp.fc1.bias\n",
      "backbone.net.layers.0.blocks.1.mlp.fc2.weight\n",
      "backbone.net.layers.0.blocks.1.mlp.fc2.bias\n",
      "backbone.net.layers.0.blocks.2.norm.weight\n",
      "backbone.net.layers.0.blocks.2.norm.bias\n",
      "backbone.net.layers.0.blocks.2.attn.qkv.weight\n",
      "backbone.net.layers.0.blocks.2.attn.qkv.bias\n",
      "backbone.net.layers.0.blocks.2.attn.proj.weight\n",
      "backbone.net.layers.0.blocks.2.attn.proj.bias\n",
      "backbone.net.layers.0.blocks.2.norm2.weight\n",
      "backbone.net.layers.0.blocks.2.norm2.bias\n",
      "backbone.net.layers.0.blocks.2.mlp.fc1.weight\n",
      "backbone.net.layers.0.blocks.2.mlp.fc1.bias\n",
      "backbone.net.layers.0.blocks.2.mlp.fc2.weight\n",
      "backbone.net.layers.0.blocks.2.mlp.fc2.bias\n",
      "backbone.net.layers.0.blocks.3.norm.weight\n",
      "backbone.net.layers.0.blocks.3.norm.bias\n",
      "backbone.net.layers.0.blocks.3.attn.qkv.weight\n",
      "backbone.net.layers.0.blocks.3.attn.qkv.bias\n",
      "backbone.net.layers.0.blocks.3.attn.proj.weight\n",
      "backbone.net.layers.0.blocks.3.attn.proj.bias\n",
      "backbone.net.layers.0.blocks.3.norm2.weight\n",
      "backbone.net.layers.0.blocks.3.norm2.bias\n",
      "backbone.net.layers.0.blocks.3.mlp.fc1.weight\n",
      "backbone.net.layers.0.blocks.3.mlp.fc1.bias\n",
      "backbone.net.layers.0.blocks.3.mlp.fc2.weight\n",
      "backbone.net.layers.0.blocks.3.mlp.fc2.bias\n",
      "backbone.net.layers.1.blocks.0.norm.weight\n",
      "backbone.net.layers.1.blocks.0.norm.bias\n",
      "backbone.net.layers.1.blocks.0.attn.qkv.weight\n",
      "backbone.net.layers.1.blocks.0.attn.qkv.bias\n",
      "backbone.net.layers.1.blocks.0.attn.proj.weight\n",
      "backbone.net.layers.1.blocks.0.attn.proj.bias\n",
      "backbone.net.layers.1.blocks.0.norm2.weight\n",
      "backbone.net.layers.1.blocks.0.norm2.bias\n",
      "backbone.net.layers.1.blocks.0.mlp.fc1.weight\n",
      "backbone.net.layers.1.blocks.0.mlp.fc1.bias\n",
      "backbone.net.layers.1.blocks.0.mlp.fc2.weight\n",
      "backbone.net.layers.1.blocks.0.mlp.fc2.bias\n",
      "backbone.net.layers.1.blocks.1.norm.weight\n",
      "backbone.net.layers.1.blocks.1.norm.bias\n",
      "backbone.net.layers.1.blocks.1.attn.qkv.weight\n",
      "backbone.net.layers.1.blocks.1.attn.qkv.bias\n",
      "backbone.net.layers.1.blocks.1.attn.proj.weight\n",
      "backbone.net.layers.1.blocks.1.attn.proj.bias\n",
      "backbone.net.layers.1.blocks.1.norm2.weight\n",
      "backbone.net.layers.1.blocks.1.norm2.bias\n",
      "backbone.net.layers.1.blocks.1.mlp.fc1.weight\n",
      "backbone.net.layers.1.blocks.1.mlp.fc1.bias\n",
      "backbone.net.layers.1.blocks.1.mlp.fc2.weight\n",
      "backbone.net.layers.1.blocks.1.mlp.fc2.bias\n",
      "backbone.net.layers.1.blocks.2.norm.weight\n",
      "backbone.net.layers.1.blocks.2.norm.bias\n",
      "backbone.net.layers.1.blocks.2.attn.qkv.weight\n",
      "backbone.net.layers.1.blocks.2.attn.qkv.bias\n",
      "backbone.net.layers.1.blocks.2.attn.proj.weight\n",
      "backbone.net.layers.1.blocks.2.attn.proj.bias\n",
      "backbone.net.layers.1.blocks.2.norm2.weight\n",
      "backbone.net.layers.1.blocks.2.norm2.bias\n",
      "backbone.net.layers.1.blocks.2.mlp.fc1.weight\n",
      "backbone.net.layers.1.blocks.2.mlp.fc1.bias\n",
      "backbone.net.layers.1.blocks.2.mlp.fc2.weight\n",
      "backbone.net.layers.1.blocks.2.mlp.fc2.bias\n",
      "backbone.net.layers.1.blocks.3.norm.weight\n",
      "backbone.net.layers.1.blocks.3.norm.bias\n",
      "backbone.net.layers.1.blocks.3.attn.qkv.weight\n",
      "backbone.net.layers.1.blocks.3.attn.qkv.bias\n",
      "backbone.net.layers.1.blocks.3.attn.proj.weight\n",
      "backbone.net.layers.1.blocks.3.attn.proj.bias\n",
      "backbone.net.layers.1.blocks.3.norm2.weight\n",
      "backbone.net.layers.1.blocks.3.norm2.bias\n",
      "backbone.net.layers.1.blocks.3.mlp.fc1.weight\n",
      "backbone.net.layers.1.blocks.3.mlp.fc1.bias\n",
      "backbone.net.layers.1.blocks.3.mlp.fc2.weight\n",
      "backbone.net.layers.1.blocks.3.mlp.fc2.bias\n",
      "backbone.net.layers.2.blocks.0.norm.weight\n",
      "backbone.net.layers.2.blocks.0.norm.bias\n",
      "backbone.net.layers.2.blocks.0.attn.qkv.weight\n",
      "backbone.net.layers.2.blocks.0.attn.qkv.bias\n",
      "backbone.net.layers.2.blocks.0.attn.proj.weight\n",
      "backbone.net.layers.2.blocks.0.attn.proj.bias\n",
      "backbone.net.layers.2.blocks.0.norm2.weight\n",
      "backbone.net.layers.2.blocks.0.norm2.bias\n",
      "backbone.net.layers.2.blocks.0.mlp.fc1.weight\n",
      "backbone.net.layers.2.blocks.0.mlp.fc1.bias\n",
      "backbone.net.layers.2.blocks.0.mlp.fc2.weight\n",
      "backbone.net.layers.2.blocks.0.mlp.fc2.bias\n",
      "backbone.net.layers.2.blocks.1.norm.weight\n",
      "backbone.net.layers.2.blocks.1.norm.bias\n",
      "backbone.net.layers.2.blocks.1.attn.qkv.weight\n",
      "backbone.net.layers.2.blocks.1.attn.qkv.bias\n",
      "backbone.net.layers.2.blocks.1.attn.proj.weight\n",
      "backbone.net.layers.2.blocks.1.attn.proj.bias\n",
      "backbone.net.layers.2.blocks.1.norm2.weight\n",
      "backbone.net.layers.2.blocks.1.norm2.bias\n",
      "backbone.net.layers.2.blocks.1.mlp.fc1.weight\n",
      "backbone.net.layers.2.blocks.1.mlp.fc1.bias\n",
      "backbone.net.layers.2.blocks.1.mlp.fc2.weight\n",
      "backbone.net.layers.2.blocks.1.mlp.fc2.bias\n",
      "backbone.net.layers.2.blocks.2.norm.weight\n",
      "backbone.net.layers.2.blocks.2.norm.bias\n",
      "backbone.net.layers.2.blocks.2.attn.qkv.weight\n",
      "backbone.net.layers.2.blocks.2.attn.qkv.bias\n",
      "backbone.net.layers.2.blocks.2.attn.proj.weight\n",
      "backbone.net.layers.2.blocks.2.attn.proj.bias\n",
      "backbone.net.layers.2.blocks.2.norm2.weight\n",
      "backbone.net.layers.2.blocks.2.norm2.bias\n",
      "backbone.net.layers.2.blocks.2.mlp.fc1.weight\n",
      "backbone.net.layers.2.blocks.2.mlp.fc1.bias\n",
      "backbone.net.layers.2.blocks.2.mlp.fc2.weight\n",
      "backbone.net.layers.2.blocks.2.mlp.fc2.bias\n",
      "backbone.net.layers.2.blocks.3.norm.weight\n",
      "backbone.net.layers.2.blocks.3.norm.bias\n",
      "backbone.net.layers.2.blocks.3.attn.qkv.weight\n",
      "backbone.net.layers.2.blocks.3.attn.qkv.bias\n",
      "backbone.net.layers.2.blocks.3.attn.proj.weight\n",
      "backbone.net.layers.2.blocks.3.attn.proj.bias\n",
      "backbone.net.layers.2.blocks.3.norm2.weight\n",
      "backbone.net.layers.2.blocks.3.norm2.bias\n",
      "backbone.net.layers.2.blocks.3.mlp.fc1.weight\n",
      "backbone.net.layers.2.blocks.3.mlp.fc1.bias\n",
      "backbone.net.layers.2.blocks.3.mlp.fc2.weight\n",
      "backbone.net.layers.2.blocks.3.mlp.fc2.bias\n",
      "backbone.net.final.weight\n"
     ]
    }
   ],
   "source": [
    "for k in new_ckpt_path.keys():\n",
    "    print(f\"{k:10s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e73f677",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from networks.LGNet import LGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cab1ee2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = LGNet(img_size=(32,64),patch_size= [1, 1],in_chans= 71,out_chans= 138,embed_dim=1152//2,\n",
    "      window_size= (4,8),depths= [4, 4, 4],num_heads= [6, 6, 6],Weather_T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b52519a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64, 576])\n",
      "torch.Size([1, 32, 64, 576])\n",
      "torch.Size([1, 32, 64, 576])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(1,71,32,64)\n",
    "    B = x.shape[0]\n",
    "    x, T, H, W = model.net.patch_embed(x)  # x:[B, H*W, C]\n",
    "    x = x + model.net.pos_embed\n",
    "    x = model.net.pos_drop(x)\n",
    "    if len(model.net.window_size) == 3:\n",
    "        x = x.view(B, T, H, W, -1)\n",
    "    elif len(model.net.window_size) == 2:\n",
    "        x = x.view(B, H, W, -1)\n",
    "\n",
    "    for layer in model.net.layers:\n",
    "        x= layer(x);print(x.shape)\n",
    "\n",
    "    x = model.net.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa84a5d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c8574a",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=[1, 1, 1], in_c=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_c\n",
    "        self.embed_dim = embed_dim\n",
    "        if len(patch_size) == 2:\n",
    "            self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        elif len(patch_size) == 3:\n",
    "            self.proj = nn.Conv3d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(self.patch_size) == 3:\n",
    "            _, _, T, H, W = x.shape\n",
    "        elif len(self.patch_size) == 2:\n",
    "            _, _, H, W = x.shape\n",
    "\n",
    "        # 下采样patch_size倍\n",
    "        x = self.proj(x)\n",
    "        # _, _, T, H, W = x.shape\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        if len(self.patch_size) == 3:\n",
    "            return x, T//self.patch_size[-3], H//self.patch_size[-2], W//self.patch_size[-1]\n",
    "        elif len(self.patch_size) == 2:\n",
    "            return x, 1, H//self.patch_size[-2], W//self.patch_size[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f7e2c8",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbed_1Patch(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=[1, 1], in_c=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_c\n",
    "        self.embed_dim = embed_dim\n",
    "        assert tuple(patch_size)==(1,1)\n",
    "        self.proj = nn.Linear(in_c, embed_dim)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "\n",
    "        # 下采样patch_size倍\n",
    "        x = self.proj(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x, 1, H//self.patch_size[-2], W//self.patch_size[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eba28",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9157acc4",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "up_branch,dw_branch = torch.split(b[0],512,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ada7f33",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "base2  = 1000\n",
    "theta2 = torch.pow(base2, -torch.arange(512)/512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "28fea3b9",
   "metadata": {
    "code_folding": [
     1,
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Rotaty2DEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim=512, w=32, h=64, base_w=1000, base_h=1000):\n",
    "        super().__init__()\n",
    "        theta_h = torch.pow(base1, -torch.arange(embed_dim)/embed_dim)\n",
    "        mtheta_h= torch.einsum(\"i,j->ij\",torch.arange(w),theta_h)\n",
    "        cos_m_theta_h = torch.cos(mtheta_h).unsqueeze(1).repeat(1,h,1).reshape(1,w*h,embed_dim)\n",
    "        sin_m_theta_h = torch.sin(mtheta_h).unsqueeze(1).repeat(1,h,1).reshape(1,w*h,embed_dim)\n",
    "        theta_w = torch.pow(base1, -torch.arange(embed_dim)/embed_dim)\n",
    "        mtheta_w= torch.einsum(\"i,j->ij\",torch.arange(h),theta_w)\n",
    "        cos_m_theta_w = torch.cos(mtheta_w).unsqueeze(0).repeat(w,1,1).reshape(1,w*h,embed_dim)\n",
    "        sin_m_theta_w = torch.sin(mtheta_w).unsqueeze(0).repeat(w,1,1).reshape(1,w*h,embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.register_buffer(\"cos_m_theta_h\",cos_m_theta_h)\n",
    "        self.register_buffer(\"sin_m_theta_h\",sin_m_theta_h)\n",
    "        self.register_buffer(\"cos_m_theta_w\",cos_m_theta_w)\n",
    "        self.register_buffer(\"sin_m_theta_w\",sin_m_theta_w)\n",
    "    def forward(self,x):\n",
    "        assert x.shape[-1] == 2*self.embed_dim\n",
    "        up_branch, dw_branch = torch.split(x,self.embed_dim,-1)\n",
    "        L1 = up_branch*self.cos_m_theta_h - dw_branch*self.sin_m_theta_h\n",
    "        L2 = up_branch*self.sin_m_theta_h + dw_branch*self.cos_m_theta_h\n",
    "        L3 = up_branch*self.cos_m_theta_w - dw_branch*self.sin_m_theta_w\n",
    "        L4 = up_branch*self.sin_m_theta_w + dw_branch*self.cos_m_theta_w\n",
    "        return torch.stack([L1,L2,L3,L4],-1).flatten(-2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "53932321",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer = Rotaty2DEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d2df37f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a  = torch.randn(1, 2048, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e5656569",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 2048])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1da4cec7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,71,32,64).cuda()\n",
    "model1 = PatchEmbed((1,1),in_c=71, embed_dim=1024)\n",
    "model1 = model1.cuda()\n",
    "model2 = PatchEmbed_1Patch((1,1),in_c=71, embed_dim=1024)\n",
    "model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "394ce591",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.2 µs ± 300 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    b = model1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13eb6a09",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.3 µs ± 306 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    b = model2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ff284a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = LGNet(img_size=(64,128),patch_size= [1, 1],in_chans= 71,out_chans= 138,embed_dim=1152,\n",
    "      window_size= (4,8),depths= [4, 4, 4],num_heads= [6, 6, 6],Weather_T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "288ac165",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import timm, tome\n",
    "\n",
    "# Load a pretrained model, can be any vit / deit model.\n",
    "model2 = timm.create_model(\"vit_base_patch16_224\", pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f23f5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tome.patch.timm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b580a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tome.patch.timm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164aea98",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def apply_patch(model, trace_source: bool = False, prop_attn: bool = True):\n",
    "    \"\"\"\n",
    "    Applies ToMe to this transformer. Afterward, set r using model.r.\n",
    "    If you want to know the source of each token (e.g., for visualization), set trace_source = true.\n",
    "    The sources will be available at model._tome_info[\"source\"] afterward.\n",
    "    For proportional attention, set prop_attn to True. This is only necessary when evaluating models off\n",
    "    the shelf. For trianing and for evaluating MAE models off the self set this to be False.\n",
    "    \"\"\"\n",
    "    ToMeLgNet = make_tome_class(model.__class__)\n",
    "\n",
    "    model.__class__ = ToMeLgNet\n",
    "    model.r = 0\n",
    "    model._tome_info = {\n",
    "        \"r\": model.r,\n",
    "        \"size\": None,\n",
    "        \"source\": None,\n",
    "        \"trace_source\": trace_source,\n",
    "        \"prop_attn\": prop_attn,\n",
    "        \"class_token\": None,\n",
    "        \"distill_token\": False,\n",
    "    }\n",
    "\n",
    "    for layer in model.net.layers:\n",
    "        for module in layer.blocks.modules():\n",
    "            if isinstance(module, Windowattn_block):\n",
    "                module.__class__ = ToMeBlock\n",
    "                module._tome_info = model._tome_info\n",
    "            elif isinstance(module, SD_attn):\n",
    "                module.__class__ = ToMeAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5ae705d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ToMeLgNet = make_tome_class(model.__class__)\n",
    "\n",
    "model.__class__ = ToMeLgNet\n",
    "model.r = 0\n",
    "model._tome_info = {\n",
    "    \"r\": model.r,\n",
    "    \"size\": None,\n",
    "    \"source\": None,\n",
    "    \"trace_source\": False,\n",
    "    \"prop_attn\": True,\n",
    "    \"class_token\": None,\n",
    "    \"distill_token\": False,\n",
    "}\n",
    "for layer in model.net.layers:\n",
    "    for module in layer.blocks.modules():\n",
    "        if isinstance(module, Windowattn_block):\n",
    "            module.__class__ = ToMeBlock\n",
    "            module._tome_info = model._tome_info\n",
    "        elif isinstance(module, SD_attn):\n",
    "            module.__class__ = ToMeAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e34bcae3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "block = model.net.layers[0].blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb63d7be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SD_attn(\n",
       "  (rope_quad): rope2()\n",
       "  (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "  (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (position_enc): rope2()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b01cd9",
   "metadata": {
    "code_folding": [
     8
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from networks.LGNet import *\n",
    "from tome.merge import bipartite_soft_matching, merge_source, merge_wavg\n",
    "from tome.utils import parse_r\n",
    "\n",
    "\n",
    "class ToMeBlock(Block):\n",
    "    \"\"\"\n",
    "    Modifications:\n",
    "     - Apply ToMe between the attention and mlp blocks\n",
    "     - Compute and propogate token size and potentially the token sources.\n",
    "    \"\"\"\n",
    "\n",
    "    def _drop_path1(self, x):\n",
    "        return self.drop_path1(x) if hasattr(self, \"drop_path1\") else self.drop_path(x)\n",
    "\n",
    "    def _drop_path2(self, x):\n",
    "        return self.drop_path2(x) if hasattr(self, \"drop_path2\") else self.drop_path(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Note: this is copied from timm.models.vision_transformer.Block with modifications.\n",
    "        attn_size = self._tome_info[\"size\"] if self._tome_info[\"prop_attn\"] else None\n",
    "        x_attn, metric = self.attn(self.norm1(x), attn_size)\n",
    "        x = x + self._drop_path1(x_attn)\n",
    "\n",
    "        r = self._tome_info[\"r\"].pop(0)\n",
    "        if r > 0:\n",
    "            # Apply ToMe here\n",
    "            merge, _ = bipartite_soft_matching(\n",
    "                metric,\n",
    "                r,\n",
    "                self._tome_info[\"class_token\"],\n",
    "                self._tome_info[\"distill_token\"],\n",
    "            )\n",
    "            if self._tome_info[\"trace_source\"]:\n",
    "                self._tome_info[\"source\"] = merge_source(\n",
    "                    merge, x, self._tome_info[\"source\"]\n",
    "                )\n",
    "            x, self._tome_info[\"size\"] = merge_wavg(merge, x, self._tome_info[\"size\"])\n",
    "\n",
    "        x = x + self._drop_path2(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a4b436",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SD_attn(\n",
       "  (rope_quad): rope2()\n",
       "  (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "  (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (position_enc): rope2()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b54bdd",
   "metadata": {
    "code_folding": [
     0,
     40
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ToMeAttention(Attention):\n",
    "    \"\"\"\n",
    "    Modifications:\n",
    "     - Apply proportional attention\n",
    "     - Return the mean of k over heads from attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, size: torch.Tensor = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Note: this is copied from timm.models.vision_transformer.Attention with modifications.\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply proportional attention\n",
    "        if size is not None:\n",
    "            attn = attn + size.log()[:, None, None, :, 0]\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        # Return k as well here\n",
    "        return x, k.mean(1)\n",
    "\n",
    "\n",
    "def make_tome_class(transformer_class):\n",
    "    class ToMeVisionTransformer(transformer_class):\n",
    "        \"\"\"\n",
    "        Modifications:\n",
    "        - Initialize r, token size, and token sources.\n",
    "        \"\"\"\n",
    "\n",
    "        def forward(self, *args, **kwdargs) -> torch.Tensor:\n",
    "            self._tome_info[\"r\"] = parse_r(len(self.blocks), self.r)\n",
    "            self._tome_info[\"size\"] = None\n",
    "            self._tome_info[\"source\"] = None\n",
    "\n",
    "            return super().forward(*args, **kwdargs)\n",
    "\n",
    "    return ToMeVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828a8dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,71,64,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bf246b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../wpredict-master/checkpoint/lgnet_finetune/world_size8-swinvrnn64x128_lgnet_possloss_finetune_norandom/checkpoint_best.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../wpredict-master/checkpoint/lgnet_finetune/world_size8-swinvrnn64x128_lgnet_possloss_finetune_norandom/checkpoint_best.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m good_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((key\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),val) \u001b[38;5;28;01mfor\u001b[39;00m key,val \u001b[38;5;129;01min\u001b[39;00m weight[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgnet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:777\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    775\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    781\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../wpredict-master/checkpoint/lgnet_finetune/world_size8-swinvrnn64x128_lgnet_possloss_finetune_norandom/checkpoint_best.pth'"
     ]
    }
   ],
   "source": [
    "weight = torch.load(\"../wpredict-master/checkpoint/lgnet_finetune/world_size8-swinvrnn64x128_lgnet_possloss_finetune_norandom/checkpoint_best.pth\", map_location='cpu')\n",
    "good_weight = dict((key.replace(\"module.\",\"\"),val) for key,val in weight['model']['lgnet'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abaf2a40",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['max_logvar', 'min_logvar'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(good_weight,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c7aebb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c05537bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_model = convert_to_int8(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2b04ee0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,71,64,128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6110cc02",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb221d9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme/zhangtianning/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "convert_to_int8(model)\n",
    "model = model.cuda()\n",
    "c = model(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed6a8e46",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4017.6440, device='cuda:0', grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca828be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weightpath = \"checkpoints/WeathBench7066/GraphCastDGLSym/ts_2_pretrain-2D706N_per_1_step/02_12_16_39_55711-seed_73001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e95732",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.GraphCast import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9846491a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from train.pretrain import get_args\n",
    "args=get_args(os.path.join(weightpath,\"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa047dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5759433",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weigth = torch.load(os.path.join(weightpath,\"backbone.best.pt\"))['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b64cdc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            This is ===> GraphCast Model(DGL) <===\n",
      "            Information: \n",
      "                total mesh node:10242 total unique mesh edge:40950*2=81900 \n",
      "                total grid node 8192+2 = 8194 but activated grid 7790 \n",
      "                from activated grid to mesh, create 4*10242 - 6 = 40962 edges. (north and south pole repeat 4 times) \n",
      "                there are 404 unactivated grid node\n",
      "                when mapping node to grid, \n",
      "                from node to activated grid, there are 40030 edges\n",
      "                from node to unactivated grid, there are 3232 edges\n",
      "                thus, totally have 43262 edge. \n",
      "                #notice some grid only have 1-2 linked node but some grid may have 30 lined node\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "model1 = GraphCastDGLSym(img_size=(64,128), embed_dim=512,graphflag='mesh6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d46fc39",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for p in model1.grid_rect_embedding_layer.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32b72eaa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1   = model1.cuda()\n",
    "optimizer= torch.optim.SGD(model1.parameters(), lr=0.1)\n",
    "a= torch.randn(1,70,64,128).cuda()\n",
    "b= torch.randn(1,70,64,128).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6bd010a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "optimizer.zero_grad()\n",
    "loss = torch.nn.MSELoss()(model1(a),b)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5bf2a9a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1.northsouthembbed.requires_grad         = False\n",
    "model1.mesh_node_embedding.requires_grad      = False\n",
    "model1.mesh_mesh_bond_embedding.requires_grad = False\n",
    "model1.grid_mesh_bond_embedding.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3cc56dd8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 ms ± 54.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "optimizer.zero_grad()\n",
    "loss = torch.nn.MSELoss()(model1(a),b)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0061a12",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            This is ===> GraphCast Model(DGL) <===\n",
      "            Information: \n",
      "                total mesh node:10242 total unique mesh edge:40950*2=81900 \n",
      "                total grid node 8192+2 = 8194 but activated grid 7790 \n",
      "                from activated grid to mesh, create 4*10242 - 6 = 40962 edges. (north and south pole repeat 4 times) \n",
      "                there are 404 unactivated grid node\n",
      "                when mapping node to grid, \n",
      "                from node to activated grid, there are 40030 edges\n",
      "                from node to unactivated grid, there are 3232 edges\n",
      "                thus, totally have 43262 edge. \n",
      "                #notice some grid only have 1-2 linked node but some grid may have 30 lined node\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "model2 = LoRAGraphCastDGLSym(img_size=(64,128), embed_dim=512,graphflag='mesh6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b192ff61",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2   = model2.cuda()\n",
    "optimizer= torch.optim.SGD(model2.parameters(), lr=0.1)\n",
    "a= torch.randn(1,70,64,128).cuda()\n",
    "b= torch.randn(1,70,64,128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a1dfb4a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 ms ± 9.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "optimizer.zero_grad()\n",
    "loss = torch.nn.MSELoss()(model2(a),b)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93297850",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LoRAGraphCastDGLSym(GraphCastDGLBase): \n",
    "    def __init__(self, img_size=(32,64),  in_chans=70, out_chans=70, depth=6, embed_dim=128, graphflag='mesh5', nonlinear='swish', **kargs):\n",
    "        super().__init__()\n",
    "\n",
    "        g = self.build_dgl(graphflag,img_size)\n",
    "\n",
    "        #### build block ####\n",
    "        edge_flag = ('mesh', 'M2M', 'mesh')\n",
    "        self.grid2mesh = LoRANode2Edge2NodeBlockDGL('grid','G2M','mesh',embed_dim=embed_dim,do_source_update=True)\n",
    "        self.mesh2mesh = nn.ModuleList()\n",
    "        for i in range(depth):self.mesh2mesh.append(LoRANode2Edge2NodeBlockDGLSymmetry('mesh','M2M','mesh',embed_dim=embed_dim))        \n",
    "        self.mesh2grid = LoRANode2Edge2NodeBlockDGL('mesh','M2G','grid',embed_dim=embed_dim)\n",
    "\n",
    "        self.grid_rect_embedding_layer = nn.Linear(in_chans,embed_dim)\n",
    "        self.projection                = nn.Linear(embed_dim,out_chans)\n",
    "        \n",
    "        self.register_buffer('northsouthembbed',        torch.randn(2,embed_dim).requires_grad_(False))\n",
    "        self.register_buffer('mesh_node_embedding',     torch.randn(g.num_nodes('mesh'),1, embed_dim).requires_grad_(False))\n",
    "        self.register_buffer('grid_mesh_bond_embedding',torch.randn(g.num_edges('G2M'),1, embed_dim).requires_grad_(False))\n",
    "        self.register_buffer('mesh_grid_bond_template',torch.randn(g.num_edges('M2G'),1, embed_dim).requires_grad_(False))\n",
    "        self.g = g\n",
    "        self.embed_dim = embed_dim\n",
    "        M2Mweightorder1= g.edge_ids(self.M2M_edgeid2pair[:,0],self.M2M_edgeid2pair[:,1],etype='M2M')\n",
    "        M2Mweightorder2= g.edge_ids(self.M2M_edgeid2pair[:,1],self.M2M_edgeid2pair[:,0],etype='M2M')\n",
    "        mesh_mesh_bond_embedding = torch.randn(g.num_edges('M2M'),1, embed_dim)\n",
    "        mesh_mesh_bond_embedding[M2Mweightorder2] = mesh_mesh_bond_embedding[M2Mweightorder2]    \n",
    "        self.register_buffer('mesh_mesh_bond_embedding',mesh_mesh_bond_embedding.requires_grad_(False))\n",
    "        #self.mesh_mesh_bond_embedding  = nn.Parameter(mesh_mesh_bond_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e183d48a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            This is ===> GraphCast Model(DGL) <===\n",
      "            Information: \n",
      "                total mesh node: 2562 total unique mesh edge:10230*2=20460 \n",
      "                total grid node 2048+2 = 2050 but activated grid 1928 \n",
      "                from activated grid to mesh, create 4*2562 - 6 = 10242 edges. (north and south pole repeat 4 times) \n",
      "                there are 122 unactivated grid node\n",
      "                when mapping node to grid, \n",
      "                from node to activated grid, there are 10032 edges\n",
      "                from node to unactivated grid, there are 976 edges\n",
      "                thus, totally have 11008 edge. \n",
      "                #notice some grid only have 1-2 linked node but some grid may have 30 lined node\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = GraphCastDGLSym(img_size=(32,64), embed_dim=512)\n",
    "model1.load_state_dict(weigth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc5611",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for p in model1.grid_rect_embedding_layer.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3315a707",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9f27d28",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b  = torch.randn(1,2,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "134ad249",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(model2.grid2mesh.STE2E_E2E.main.weight,\n",
    "           model1.grid2mesh.STE2E_E2E\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b47deb83",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7202e-06, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = b@model1.grid2mesh.STE2E_E2E\n",
    "out2 = model2.grid2mesh.STE2E_E2E(b)\n",
    "torch.dist(out1,out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c2ff49c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            This is ===> GraphCast Model(DGL) <===\n",
      "            Information: \n",
      "                total mesh node: 2562 total unique mesh edge:10230*2=20460 \n",
      "                total grid node 2048+2 = 2050 but activated grid 1928 \n",
      "                from activated grid to mesh, create 4*2562 - 6 = 10242 edges. (north and south pole repeat 4 times) \n",
      "                there are 122 unactivated grid node\n",
      "                when mapping node to grid, \n",
      "                from node to activated grid, there are 10032 edges\n",
      "                from node to unactivated grid, there are 976 edges\n",
      "                thus, totally have 11008 edge. \n",
      "                #notice some grid only have 1-2 linked node but some grid may have 30 lined node\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LoRAGraphCastDGLSym(img_size=(32,64), embed_dim=512)\n",
    "new_weight = {}\n",
    "for key, w in weigth.items():\n",
    "    skip=False\n",
    "    for activate_key in ['STE2E_S2E','STE2E_T2E','STE2E_E2E',\n",
    "                         'ET2T_E2T','ET2T_T2T','S2S']:\n",
    "        if activate_key in key:\n",
    "            key += \".main.weight\"\n",
    "            new_weight[key] = w.transpose(1,0)\n",
    "            skip = True\n",
    "            continue\n",
    "            \n",
    "    if skip:continue\n",
    "    new_weight[key] = w\n",
    "model2.load_state_dict(new_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6bbe76f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,70,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a730053",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d76feb2a",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _input = a.clone()\n",
    "    B, P, W, H = _input.shape\n",
    "    device = next(model1.parameters()).device\n",
    "    if model1.device is None:\n",
    "        model1.device = device\n",
    "        model1.g = model1.g.to(device)\n",
    "    # (B,P,W,H) -> (B,W*H,P)\n",
    "    feature_along_latlot = model1.grid_rect_embedding_layer(\n",
    "        rearrange(_input, \"B P W H -> (W H) B P\"))\n",
    "    # (L,B,D)\n",
    "    grid_rect_embedding1 = feature_along_latlot[model1.G2M_grid2LaLotudePos]\n",
    "    grid_rect_embedding1 = torch.cat([rearrange(model1.northsouthembbed.repeat(B, 1, 1), \"B L D -> L B D\"),\n",
    "                                        grid_rect_embedding1])  # --> (L+2, B, D)\n",
    "    L = len(grid_rect_embedding1)\n",
    "    g = model1.g\n",
    "    g.nodes['grid'].data['feat'] = torch.nn.functional.pad(\n",
    "        grid_rect_embedding1, (0, 0, 0, 0, 0, model1.unactivated_grid))\n",
    "    g.nodes['mesh'].data['feat'] = model1.mesh_node_embedding\n",
    "    g.edges['G2M'].data['feat'] = model1.grid_mesh_bond_embedding\n",
    "    g.edges['M2M'].data['feat'] = model1.mesh_mesh_bond_embedding\n",
    "    g.edges['G2M'].data['coef'] = model1.G2M_edge_coef.to(device)\n",
    "    g.edges['M2G'].data['coef'] = model1.M2G_edge_coef.to(device)\n",
    "    # checknan(g,'initial');\n",
    "    g = model1.grid2mesh(g)\n",
    "#     # checknan(g,'grid2mesh');\n",
    "#     for layer_idx, mesh2mesh in enumerate(model1.mesh2mesh):\n",
    "#         g = mesh2mesh(g)  # checknan(g,f\"mesh2mesh_{layer_idx}\")\n",
    "#     g.edges['M2G'].data['feat'] = torch.nn.functional.pad(\n",
    "#         g.edges['G2M'].data['feat'][model1.reorder_edge_id_of_M2G_from_G2M], (0, 0, 0, 0, 0, model1.num_unactivated_edge))\n",
    "#     # luckly, the model1.reorder_edge_id_in_M2G is just np.arange(....) thus, use padding rather than create one can fast 50%\n",
    "#     # model1.mesh_grid_bond_template = torch.zeros(g.num_edges('M2G'),B,model1.embed_dim).to(g.edges['G2M'].data['feat'].device)\n",
    "#     # model1.mesh_grid_bond_template[model1.reorder_edge_id_in_M2G] = g.edges['G2M'].data['feat'][model1.reorder_edge_id_of_M2G_from_G2M]\n",
    "#     # g.edges['M2G'].data['feat'] = model1.mesh_grid_bond_template\n",
    "#     # g.nodes['grid'].data['feat'][L:] = 0 <--- only to align with GraphCastFast\n",
    "#     g = model1.mesh2grid(g)  # checknan(g,'mesh2grid')\n",
    "#     # (64,128,B,embed_dim)\n",
    "#     out = g.nodes['grid'].data['feat'][model1.M2G_LaLotudePos2grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edc05e36",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1270.1317)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(model1.g.nodes['grid'].data['feat'],\n",
    "           model2.g.nodes['grid'].data['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "890c95c0",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _input = a.clone()\n",
    "    B, P, W, H = _input.shape\n",
    "    device = next(model2.parameters()).device\n",
    "    if model2.device is None:\n",
    "        model2.device = device\n",
    "        model2.g = model2.g.to(device)\n",
    "    # (B,P,W,H) -> (B,W*H,P)\n",
    "    feature_along_latlot = model2.grid_rect_embedding_layer(\n",
    "        rearrange(_input, \"B P W H -> (W H) B P\"))\n",
    "    # (L,B,D)\n",
    "    grid_rect_embedding2 = feature_along_latlot[model2.G2M_grid2LaLotudePos]\n",
    "    grid_rect_embedding2 = torch.cat([rearrange(model2.northsouthembbed.repeat(B, 1, 1), \"B L D -> L B D\"),\n",
    "                                        grid_rect_embedding2])  # --> (L+2, B, D)\n",
    "    L = len(grid_rect_embedding2)\n",
    "    g = model2.g\n",
    "    g.nodes['grid'].data['feat'] = torch.nn.functional.pad(\n",
    "        grid_rect_embedding2, (0, 0, 0, 0, 0, model2.unactivated_grid))\n",
    "    g.nodes['mesh'].data['feat'] = model2.mesh_node_embedding\n",
    "    g.edges['G2M'].data['feat'] = model2.grid_mesh_bond_embedding\n",
    "    g.edges['M2M'].data['feat'] = model2.mesh_mesh_bond_embedding\n",
    "    g.edges['G2M'].data['coef'] = model2.G2M_edge_coef.to(device)\n",
    "    g.edges['M2G'].data['coef'] = model2.M2G_edge_coef.to(device)\n",
    "    # checknan(g,'initial');\n",
    "    g = model2.grid2mesh(g)\n",
    "#     # checknan(g,'grid2mesh');\n",
    "#     for layer_idx, mesh2mesh in enumerate(model2.mesh2mesh):\n",
    "#         g = mesh2mesh(g)  # checknan(g,f\"mesh2mesh_{layer_idx}\")\n",
    "#     g.edges['M2G'].data['feat'] = torch.nn.functional.pad(\n",
    "#         g.edges['G2M'].data['feat'][model2.reorder_edge_id_of_M2G_from_G2M], (0, 0, 0, 0, 0, model2.num_unactivated_edge))\n",
    "#     # luckly, the model2.reorder_edge_id_in_M2G is just np.arange(....) thus, use padding rather than create one can fast 50%\n",
    "#     # model2.mesh_grid_bond_template = torch.zeros(g.num_edges('M2G'),B,model2.embed_dim).to(g.edges['G2M'].data['feat'].device)\n",
    "#     # model2.mesh_grid_bond_template[model2.reorder_edge_id_in_M2G] = g.edges['G2M'].data['feat'][model2.reorder_edge_id_of_M2G_from_G2M]\n",
    "#     # g.edges['M2G'].data['feat'] = model2.mesh_grid_bond_template\n",
    "#     # g.nodes['grid'].data['feat'][L:] = 0 <--- only to align with GraphCastFast\n",
    "#     g = model2.mesh2grid(g)  # checknan(g,'mesh2grid')\n",
    "#     # (64,128,B,embed_dim)\n",
    "#     out = g.nodes['grid'].data['feat'][model2.M2G_LaLotudePos2grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b561e9f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8a7edf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e0386bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,70,32,64).cuda()\n",
    "b = torch.randn(1,70,32,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b796dcb3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 ms ± 1.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "loss = torch.nn.MSELoss()(b,model1(a))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53826a48",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1.northsouthembbed.requires_grad = False\n",
    "model1.mesh_node_embedding.requires_grad = False\n",
    "model1.mesh_mesh_bond_embedding.requires_grad = False\n",
    "model1.grid_mesh_bond_embedding.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ab13b1a",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.4 ms ± 1.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "loss = torch.nn.MSELoss()(b,model1(a))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebbf5a9",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel1\u001b[49m\u001b[38;5;241m.\u001b[39mgrid_rect_embedding_layer\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(p\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde477f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a412153",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d242cdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "local_mask = torch.ones(seq_len, seq_len, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4b352d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.],\n",
       "          [9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.conv2d(torch.ones(1,1,16,16),torch.ones(1,1,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "094d98fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397d4eb",
   "metadata": {
    "code_folding": [
     5,
     21,
     45,
     69,
     118
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AFTLocalConv2D(nn.Module):\n",
    "    \"\"\"\n",
    "    ### AFT Local Operation\n",
    "    $$Y_t = \\sigma(Q_t) \\odot\n",
    "     \\frac{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'}) \\odot V_{t'}}\n",
    "     {\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'})}$$\n",
    "    where,\n",
    "    \\begin{align}\n",
    "    w'_{t,t'} =\n",
    "    \\begin{cases}\n",
    "    w_{t,t'},  & {\\text{for } \\lvert t-t' \\rvert \\lt s} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    \\end{align}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, token_dim: (int,int), local_window_size: (int,int), bias: bool = True):\n",
    "        \"\"\"\n",
    "        * `d_model` is the number of features in the `query`, `key` and `value` vectors.\n",
    "        * `token_dim` is image size like $(32,64)$\n",
    "        * `local_window_size` is the local window size $s$\n",
    "        * `bias` is whether to have a bias parameter for transformations for $Q$, $K$ and $V$.\n",
    "        # input is a (B,head,W,H,embedding) tensor\n",
    "        # in each head dimension\n",
    "        #    do \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # Local window size $s$\n",
    "        self.local_window_size = local_window_size\n",
    "        # These transform the `query`, `key` and `value` vectors.\n",
    "        self.query      = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.key        = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.value      = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.pos_bias   = nn.Parameter(torch.zeros(seq_len, seq_len), requires_grad=True)\n",
    "        self.local_mask = nn.Parameter(self.create_local_mask(token_dim, local_window_size), requires_grad=False)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.output     = nn.Linear(d_model, d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_local_mask(token_dim, local_window_size):\n",
    "        \"\"\"\n",
    "        if the token_dim is seq_len then produce (seq_len,seq_len)\n",
    "        #### Create local mask\n",
    "        This creates a mask for\n",
    "        \\begin{align}\n",
    "        m_{t,t'} =\n",
    "        \\begin{cases}\n",
    "        1,  & {\\text{for } \\lvert t-t' \\rvert \\lt s} \\\\\n",
    "        0, & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "        \\end{align}\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize to ones\n",
    "        local_mask = torch.ones(seq_len, seq_len, dtype=torch.bool)\n",
    "        # Make $t' - t \\ge s$ zero\n",
    "        local_mask = torch.tril(local_mask, local_window_size - 1)\n",
    "        # Make $t - t' \\ge s$ zero\n",
    "        local_mask = torch.triu(local_mask, -(local_window_size - 1))\n",
    "\n",
    "        #\n",
    "        return local_mask\n",
    "\n",
    "    def forward(self, *,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        `query`, `key` and `value` are the tensors that store\n",
    "        collection of token embeddings for  *query*, *key* and *value*.\n",
    "        They have shape `[seq_len, batch_size, d_model]`.\n",
    "        `mask` has shape `[seq_len, seq_len, batch_size]` and\n",
    "        `mask[i, j, b]` indicates whether for batch `b`,\n",
    "        query at position `i` has access to key-value at position `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # `query`, `key` and `value`  have shape `[B,head,W,H,P]`\n",
    "        B,head,W,H,P = query.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            # `mask` has shape `[seq_len_q, seq_len_k, batch_size]`,\n",
    "            # where first dimension is the query dimension.\n",
    "            # If the query dimension is equal to $1$ it will be broadcasted.\n",
    "            assert mask.shape[0] == 1 or mask.shape[0] == query.shape[0]\n",
    "            assert mask.shape[1] == key.shape[0]\n",
    "            assert mask.shape[2] == 1 or mask.shape[2] == query.shape[1]\n",
    "\n",
    "        # Transform query, key and value embeddings\n",
    "        query   = self.query(query) # (B,head,W,H,P)\n",
    "        key     = self.key(key)     # (B,head,W,H,P)\n",
    "        value   = self.value(value) # (B,head,W,H,P)\n",
    "        max_key = key.max(dim=0, keepdims=True)[0]\n",
    "        key     = torch.exp(key - max_key)\n",
    "        \n",
    "        kv  = key * value # (B,head,P,W,H)\n",
    "        \n",
    "        convkvw = torch.nn.functional.conv2d(rearrange(kv,\" B D P W H -> (B D) P W H \"),# (B*head,P,W,H)\n",
    "                                             self.)\n",
    "        num =  + torch.einsum('bdwhp,bdwhp->bdp', exp_key, value)\n",
    "        # The denominator part $\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'})$\n",
    "        den = torch.einsum('ijb,jbd->ibd', exp_pos_bias, exp_key)\n",
    "\n",
    "        # Output $$Y_t = \\sigma(Q_t) \\odot\n",
    "        #         \\frac{\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'}) \\odot V_{t'}}\n",
    "        #         {\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'})}$$\n",
    "        y = self.activation(query) * num / den\n",
    "\n",
    "        # Output layer\n",
    "        return self.output(y)\n",
    "\n",
    "\n",
    "def _test_local_mask():\n",
    "    \"\"\"\n",
    "    Test local mask\n",
    "    \"\"\"\n",
    "    from labml.logger import inspect\n",
    "    inspect(AFTLocal.create_local_mask(10, 4))\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    _test_local_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1048cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.custom_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd1d919",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.FEDformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca70c0f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kargs={'img_size': (7, 7), 'patch_size': 1, 'patch_range': 7, 'in_chans': 73, 'out_chans': 70, 'fno_blocks': 4, 'embed_dim': 512, 'depth': 2, 'debug_mode': 0, 'double_skip': False, 'fno_bias': False, 'fno_softshrink': 0.0, 'history_length': 16, 'reduce_Field_coef': False, 'modes': (17, 33, 6), 'mode_select': 'normal', 'physics_num': 4, 'pred_len': 16, 'n_heads': 8, 'label_len': 8, 'canonical_fft': 1, 'unique_up_sample_channel': 70, 'share_memory': 0, 'dropout_rate': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e613cbc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fourier enhanced block used!\n",
      "for shape:[7 7] and pick modes:(17, 33)>=25.0, we pick 25.0 modes, the baseline modes is 25.0\n",
      "create a mode filter shape=torch.Size([7, 7, 9]) with 150 mode activate\n",
      "fourier enhanced block used!\n",
      "for shape:[7 7] and pick modes:(17, 33)>=25.0, we pick 25.0 modes, the baseline modes is 25.0\n",
      "create a mode filter shape=torch.Size([7, 7, 13]) with 150 mode activate\n",
      " fourier enhanced cross attention used!\n",
      "for shape:[7 7] and pick modes:(17, 33)>=25.0, we pick 25.0 modes, the baseline modes is 25.0\n",
      "for shape:[7 7] and pick modes:(17, 33)>=25.0, we pick 25.0 modes, the baseline modes is 25.0\n",
      " modes_q=150,  shape_q=torch.Size([7, 7, 13])\n",
      "modes_kv=150, shape_kv=torch.Size([7, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "model = FEDformer(**kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33f09507",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d40e5d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 74 parameters, totally 17801286 numbers\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=0\n",
    "data = []\n",
    "for name,p in model.named_parameters():\n",
    "    data.append([name,np.prod(p.shape),p.detach().numpy().shape])\n",
    "    #print(\"{:40} {:5} {}\".format(name,np.prod(p.shape),p.shape))\n",
    "    i+=np.prod(p.shape)\n",
    "    j+=1\n",
    "print(\"Totally {} parameters, totally {} numbers\".format(j,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9889e447",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5893d710",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pdata = pd.DataFrame(data,columns=['weight','size','shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501c7e1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>size</th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>decoder.projection.bias</td>\n",
       "      <td>70</td>\n",
       "      <td>(70,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>encoder.norm.batchnorm.bias</td>\n",
       "      <td>512</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>decoder.layers.0.self_attention.query_projecti...</td>\n",
       "      <td>512</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>decoder.layers.0.self_attention.key_projection...</td>\n",
       "      <td>512</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>decoder.layers.0.self_attention.value_projecti...</td>\n",
       "      <td>512</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>decoder.layers.0.cross_attention.key_projectio...</td>\n",
       "      <td>262144</td>\n",
       "      <td>(512, 512)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>encoder.attn_layers.0.attention.out_projection...</td>\n",
       "      <td>262144</td>\n",
       "      <td>(512, 512)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>decoder.layers.1.cross_attention.query_project...</td>\n",
       "      <td>262144</td>\n",
       "      <td>(512, 512)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>decoder.layers.0.cross_attention.inner_correla...</td>\n",
       "      <td>4915200</td>\n",
       "      <td>(8, 64, 64, 150)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>decoder.layers.1.cross_attention.inner_correla...</td>\n",
       "      <td>4915200</td>\n",
       "      <td>(8, 64, 64, 150)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               weight     size  \\\n",
       "73                            decoder.projection.bias       70   \n",
       "27                        encoder.norm.batchnorm.bias      512   \n",
       "30  decoder.layers.0.self_attention.query_projecti...      512   \n",
       "32  decoder.layers.0.self_attention.key_projection...      512   \n",
       "34  decoder.layers.0.self_attention.value_projecti...      512   \n",
       "..                                                ...      ...   \n",
       "40  decoder.layers.0.cross_attention.key_projectio...   262144   \n",
       "11  encoder.attn_layers.0.attention.out_projection...   262144   \n",
       "59  decoder.layers.1.cross_attention.query_project...   262144   \n",
       "37  decoder.layers.0.cross_attention.inner_correla...  4915200   \n",
       "58  decoder.layers.1.cross_attention.inner_correla...  4915200   \n",
       "\n",
       "               shape  \n",
       "73             (70,)  \n",
       "27            (512,)  \n",
       "30            (512,)  \n",
       "32            (512,)  \n",
       "34            (512,)  \n",
       "..               ...  \n",
       "40        (512, 512)  \n",
       "11        (512, 512)  \n",
       "59        (512, 512)  \n",
       "37  (8, 64, 64, 150)  \n",
       "58  (8, 64, 64, 150)  \n",
       "\n",
       "[74 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata.sort_values('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6e3d36",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.tools import getModelSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b83db9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51232766, 4098, 195.45313262939453)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getModelSize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "358a109b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64, 6, 70])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc     = torch.randn(1,32,64,6,70).cuda()\n",
    "x_mark_enc= torch.randn(1,6,4).cuda()\n",
    "x_mark_dec= torch.randn(1,1,4).cuda()\n",
    "model(x_enc,x_mark_enc,x_mark_dec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df7e69e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811 ms ± 20.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        model(x_enc,x_mark_enc,x_mark_dec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e26631ac",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696 ms ± 8.52 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        model(x_enc,x_mark_enc,x_mark_dec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6636d5f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q = torch.randn(1,2,3,4,5)\n",
    "k = torch.randn(1,2,3,4,5)\n",
    "v = torch.randn(1,2,3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24ffb3bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(q,k,v,None)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f91bdc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 2D patch model, the img_size will be force set (14, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "model = POverLapTimePosVallinaViT((14,32,64),(3,5,5),in_chans=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42984e92",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from train.pretrain import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e369b0c3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args = get_args(config_path=\"checkpoints/WeathBench7066PatchDataset/ViT_in_bulk-POverLapTimePosVallinaViT_Patch_(3, 5, 5)/time_step_2_pretrain-3D70N_every_1_step_random_dataset/12_07_22_50_36-seed_2018/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac20a5e",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use offline data mode <1>: only train use offline data\n",
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test.npy\n",
      "notice we will use around_index(12, 28, 64, 3, 3, 5, 5) to patch data\n"
     ]
    }
   ],
   "source": [
    "time_step = args.time_step if \"fourcast\" in args.mode else 3*24//6 + args.time_step\n",
    "dataset_kargs = copy.deepcopy(args.dataset_kargs)\n",
    "dataset_kargs['time_step'] = time_step\n",
    "if dataset_kargs['time_reverse_flag'] in ['only_forward','random_forward_backward']:\n",
    "    dataset_kargs['time_reverse_flag'] = 'only_forward'\n",
    "dataset_type = eval(args.dataset_type) if isinstance(args.dataset_type,str) else args.dataset_type\n",
    "#print(dataset_kargs)\n",
    "split = args.split if hasattr(args,'split') and args.split else \"test\"\n",
    "test_dataset = dataset_type(split=split, with_idx=True,dataset_tensor=None,record_load_tensor=None,**dataset_kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305f4d9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_dataset.cross_sample = False\n",
    "idx_a,batch = test_dataset[0]\n",
    "tensor_a, time_stamp_a, pos_a = batch[0]\n",
    "input_a = model.image_to_patches(tensor_a[None],pos_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d140a6bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = tensor_a[None]\n",
    "good_input_shape = model.patch_range\n",
    "now_input_shape  = tuple(x.shape[-3:])\n",
    "\n",
    "around_index = model.around_index_depend_input(now_input_shape)\n",
    "if model.global_position_feature is None:\n",
    "    grid = torch.Tensor(np.stack(np.meshgrid(\n",
    "            np.arange(model.img_size[0]),\n",
    "            np.arange(model.img_size[1]),\n",
    "            np.arange(model.img_size[2]))).transpose(0,2,1,3))\n",
    "    model.global_position_feature = model.get_direction_from_stamp(grid[None]) #(1, 3, Z,W, H)\n",
    "pos_feature  = model.global_position_feature.repeat(x.size(0),1,1,1,1).to(x.device) #(B, 3, Z,W, H)\n",
    "B,P,_,_,_ = x.shape\n",
    "x = torch.cat([x,pos_feature],1) #( B, 77, W, H)\n",
    "# x = x[...,around_index[:,:,:,0],around_index[:,:,:,1],around_index[:,:,:,2]] # (B,P,Z,W-4,H,Patch,Patch)\n",
    "# B,_,Z,W,H,_,_,_ = x.shape\n",
    "# model.input_shape_tmp=(B,Z,W,H,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6097a15",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 5, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f419a29",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c8ae98b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[10 10 10 10 10]\n",
      "   [10 10 10 10 10]\n",
      "   [10 10 10 10 10]\n",
      "   [10 10 10 10 10]\n",
      "   [10 10 10 10 10]]\n",
      "\n",
      "  [[11 11 11 11 11]\n",
      "   [11 11 11 11 11]\n",
      "   [11 11 11 11 11]\n",
      "   [11 11 11 11 11]\n",
      "   [11 11 11 11 11]]\n",
      "\n",
      "  [[12 12 12 12 12]\n",
      "   [12 12 12 12 12]\n",
      "   [12 12 12 12 12]\n",
      "   [12 12 12 12 12]\n",
      "   [12 12 12 12 12]]]\n",
      "\n",
      "\n",
      " [[[19 19 19 19 19]\n",
      "   [20 20 20 20 20]\n",
      "   [21 21 21 21 21]\n",
      "   [22 22 22 22 22]\n",
      "   [23 23 23 23 23]]\n",
      "\n",
      "  [[19 19 19 19 19]\n",
      "   [20 20 20 20 20]\n",
      "   [21 21 21 21 21]\n",
      "   [22 22 22 22 22]\n",
      "   [23 23 23 23 23]]\n",
      "\n",
      "  [[19 19 19 19 19]\n",
      "   [20 20 20 20 20]\n",
      "   [21 21 21 21 21]\n",
      "   [22 22 22 22 22]\n",
      "   [23 23 23 23 23]]]\n",
      "\n",
      "\n",
      " [[[18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]]\n",
      "\n",
      "  [[18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]]\n",
      "\n",
      "  [[18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]]]]\n"
     ]
    }
   ],
   "source": [
    "test_dataset.cross_sample = True\n",
    "idx_b,batch = test_dataset[0]\n",
    "(tensor_b, time_stamp_b, pos_b) = batch[0]\n",
    "input_b = model.image_to_patches(tensor_b[None],torch.Tensor(pos_b[None]).to(tensor_b.device))\n",
    "print(pos_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26d11170",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_tensor= x[...,pos_b[0],pos_b[1],pos_b[2]]\n",
    "torch.dist(selected_tensor,input_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa99d505",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = torch.Tensor(np.stack(np.meshgrid(\n",
    "                np.arange(model.img_size[0]),\n",
    "                np.arange(model.img_size[1]),\n",
    "                np.arange(model.img_size[2]))).transpose(0,2,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "207911f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image_to_patches() missing 1 required positional argument: 'pos_stamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m c \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpatches_to_image(y)\n",
      "\u001b[0;31mTypeError\u001b[0m: image_to_patches() missing 1 required positional argument: 'pos_stamp'"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,5,14,32,64)\n",
    "y = model.image_to_patches(a)\n",
    "c = model.patches_to_image(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9cd01d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 32, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5a3e3b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,70,3,5,5).cuda()\n",
    "p = torch.randn(1, 3,3,5,5).cuda()\n",
    "t = torch.randn(1,4).cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7a3f3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 70, 3, 5, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a,t,p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f07248",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513c0480",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 28, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.stack(np.meshgrid(\n",
    "            np.arange(img_size[0]),\n",
    "            np.arange(img_size[1]),\n",
    "        )).transpose(0,2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ecb91d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 32, 36])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.stack(np.meshgrid(\n",
    "    np.arange(self.img_size[0]),\n",
    "    np.arange(self.img_size[1]),\n",
    "    np.arange(self.img_size[2]))).transpose(0,2,1,3)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa71757",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 70, 14, 32, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a162291",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*16*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "96a17ea3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torach.randn(1,2,4,4)\n",
    "m = torch.randint(2,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0f6e64dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0000, -1.0000,  1.2643,  0.2839],\n",
       "          [-1.0000,  0.0204, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -0.7041],\n",
       "          [ 0.1266, -1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "         [[-1.0000, -1.0000, -2.2420, -0.1599],\n",
       "          [-1.0000,  1.4979, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -0.3242],\n",
       "          [ 0.3754, -1.0000, -1.0000, -1.0000]]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.masked_fill(m,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638e6af5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e1445",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#x = x.reshape(B,W,H,P,patch_range,patch_range)\n",
    "x = torch.randn(B,W,H,P,patch_range,patch_range)\n",
    "def get_x1(x,counting_matrix):\n",
    "    w_idx   = np.arange(0,L)\n",
    "    wes  = np.stack([w_idx, w_idx+1,w_idx+2, w_idx-1, w_idx-2],1)%L\n",
    "    x_idx   = np.arange(H)\n",
    "    xes = np.stack([x_idx, x_idx+1,x_idx+2, x_idx-1, x_idx-2],1)%H\n",
    "    yes = np.array([[2,  1,  0,  3,  4]])\n",
    "    x = torch.nn.functional.pad(x,(0,0, 0,0, 0,0, 0,0, 2 , 2 )) # only extend W\n",
    "    x     = x[:, wes, :,:,yes,:].sum(1) #(B, W, H, P, PS,PS) --> (W, B, H, P, PS)\n",
    "    x     = x[:, :, xes,:,yes].sum(1)  #(W, B, H, P, PS) --> (H, W, B, P)   \n",
    "    x     = x.permute(2,3,1,0)#(B,P, W,H)   \n",
    "    counting_matrix =counting_matrix.to(x.device)\n",
    "    x     = x/counting_matrix #(B,P, W,H)  / (1,1 , W,H)\n",
    "    return x\n",
    "def get_x2(x,counting_matrix):\n",
    "    w_idx   = np.arange(0,L)\n",
    "    wes  = np.stack([w_idx+2, w_idx+1, w_idx, w_idx-1, w_idx-2],1)%L\n",
    "    x_idx   = np.arange(H)\n",
    "    xes = np.stack([x_idx+2, x_idx+1, x_idx, x_idx-1, x_idx-2],1)%H\n",
    "    yes = np.array([[0 ,1, 2, 3, 4]])\n",
    "    x = torch.nn.functional.pad(x,(0,0, 0,0, 0,0, 0,0, 2 , 2 )) # only extend W\n",
    "    x     = x[:, wes, :,:,yes,:].sum(1) #(B, W, H, P, PS,PS) --> (W, B, H, P, PS)\n",
    "    x     = x[:, :, xes,:,yes].sum(1)  #(W, B, H, P, PS) --> (H, W, B, P)   \n",
    "    x     = x.permute(2,3,1,0)#(B,P, W,H)   \n",
    "    counting_matrix =counting_matrix.to(x.device)\n",
    "    x     = x/counting_matrix #(B,P, W,H)  / (1,1 , W,H)\n",
    "    return x\n",
    "print(torch.dist(get_x1(x,counting_matrix),get_x2(x,counting_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd2f30",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from vit_pytorch.vit import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decc415",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_num\n",
    "from utils.tools import getModelSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d656d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.othermodels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c032af",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CK_SWDformer_0505(BaseModel):\n",
    "        def __init__(self,*args,**kargs):\n",
    "            super().__init__()\n",
    "            print(\"this is pre-set model, we disable all config\")\n",
    "            self.backbone = SWD_former(patch_size= [1, 1],\n",
    "                                in_chans= 70,\n",
    "                                out_chans= 70,\n",
    "                                embed_dim= 768,\n",
    "                                window_size= (5,5),\n",
    "                                depths= [4, 4, 4],\n",
    "                                num_heads= [6, 6, 6],\n",
    "                                Weather_T=1,only_swin=True)\n",
    "        def forward(self,x):\n",
    "            return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e510c79",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CK_SWDformer_0505()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c13e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a=torch.randn(1,70,5,5)\n",
    "model(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2376dad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "W=32\n",
    "H=64\n",
    "hd=16\n",
    "D=768\n",
    "B=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74121e89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from networks.SWD_former import SWD_former,SWDfromer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4b125",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = SWD_former(patch_size= [1, 1],\n",
    "        in_chans= 70,\n",
    "        out_chans= 70,\n",
    "        embed_dim= 1128,\n",
    "        window_size= (32,64),\n",
    "        depths= [4, 4, 4],\n",
    "        num_heads= [6, 6, 6],\n",
    "        Weather_T=1,only_swin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9e62a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cc9ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.afnonet import AFNONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cca764",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_model_para_num(model)[1]/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d4e3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fde186",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1525b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cephdataset import WeathBench7066PatchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919fe77",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = torch.randn(1,70,32,64)\n",
    "x = model.backbone.to_patch_embedding(img);print(x.shape)\n",
    "b, n, _ = x.shape\n",
    "\n",
    "cls_tokens = repeat(model.backbone.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "x = torch.cat((cls_tokens, x), dim=1);print(x.shape)\n",
    "x += model.backbone.pos_embedding[:, :(n + 1)];print(x.shape)\n",
    "x = model.backbone.dropout(x);print(x.shape)\n",
    "\n",
    "x = model.backbone.transformer(x);print(x.shape)\n",
    "\n",
    "x = x.mean(dim = 1) if model.backbone.pool == 'mean' else x[:, 0];print(x.shape)\n",
    "\n",
    "x = model.backbone.to_latent(x);print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16fb69",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "property_names = test_dataset.vnames\n",
    "accu_list = torch.stack([p['accu'] for p in fourcastresult.values() if 'accu' in p]).numpy()\n",
    "total_num = len(accu_list)\n",
    "accu_list = accu_list.mean(0)# (fourcast_num,property_num)\n",
    "real_times = [(predict_time+1)*test_dataset.time_intervel*test_dataset.time_unit for predict_time in range(len(accu_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa91ac9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "snap_tables=[]\n",
    "snap_index = fourcastresult['snap_index']\n",
    "select_snap_start_time_point = snap_index[0]\n",
    "select_snap_show_property_id = snap_index[1]\n",
    "select_snap_property_name    = [property_names[iidd] for iidd in select_snap_show_property_id]\n",
    "for select_time_point in select_snap_start_time_point:\n",
    "    timestamp = test_dataset.datatimelist_pool['test'][select_time_point]\n",
    "    if select_time_point in fourcastresult: # in case do not record\n",
    "        linedata = fourcastresult[select_time_point]['snap_line']\n",
    "        for predict_time_point, tensor, label in linedata:\n",
    "            for name, value in zip(select_snap_property_name,tensor):\n",
    "                predict_timestamp = 0 if predict_time_point==0 else real_times[predict_time_point-1]\n",
    "                snap_tables.append([timestamp, name, predict_timestamp, value.item(), label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80baabe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "global_rmse_map = fourcastresult['global_rmse_map']\n",
    "mean_global_rmse_map = [t/total_num for t in global_rmse_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1aec1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93816d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b7a7c",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b44ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2deca2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = torch.nn.Linear(32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5f0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer       = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a90b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['foreach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e26396",
   "metadata": {
    "code_folding": [
     22
    ]
   },
   "outputs": [],
   "source": [
    "class LargeMLP_3D(nn.Module):\n",
    "    '''\n",
    "    input is (B, P, patch_range_1,patch_range_2,patch_range_3)\n",
    "    output is (B,P)\n",
    "    ''' \n",
    "    def __init__(self,img_size=None,patch_range=5,in_chans=20, out_chans=20,p=0.1,**kargs):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_range = 5\n",
    "        if self.patch_range == 5:\n",
    "            cl = [5*5*5*in_chans,5*5*5*10,5*5*5*20,\n",
    "                  5*5*5*30,5*5*5*30,5*5*5*20,\n",
    "                  5*5*5*10,5*5*5*1,out_chans]\n",
    "            nnlist = []\n",
    "            for i in range(len(cl)-2):\n",
    "                nnlist+=[nn.Linear(cl[i],cl[i+1]),nn.Dropout(p=p),nn.Tanh()]\n",
    "            nnlist+=[nn.Linear(cl[-2],cl[-1])]\n",
    "            self.backbone = nn.Sequential(*nnlist)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.center_index,self.around_index=get_center_around_indexes_3D(self.patch_range,self.img_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        The input either (B,P,patch_range,patch_range) or (B,P,w,h)\n",
    "        The output then is  (B,P) or (B,P,w-patch_range//2,h-patch_range//2)\n",
    "        ''' \n",
    "        assert len(x.shape) == 5 #(B,P,Z,W,H)\n",
    "        input_is_full_image = False\n",
    "        if x.shape[-3:] == self.img_size:\n",
    "            input_is_full_image = True\n",
    "            x = x[..., self.around_index[:, :, : , 0],self.around_index[:, :, : , 1],self.around_index[:, :, : , 2]] \n",
    "            # (B,P,Z-2,W-2,H,Patch,Patch,Patch)\n",
    "            x = x.permute(0, 2, 3, 4, 1, 5, 6, 7)\n",
    "            B, Z, W, H, P, _, _, _ = x.shape\n",
    "            x = x.flatten(0, 3)  # (B* Z-2 * W-2 * H, Property, Patch,Patch,Patch)\n",
    "        assert tuple(x.shape[-3:]) == (self.patch_range,self.patch_range,self.patch_range)\n",
    "        x = self.backbone(x.flatten(-4,-1)) # (B* W-4 * H,P)\n",
    "        if input_is_full_image:\n",
    "            x = x.reshape(B, Z, W, H, P).permute(0, 4, 1, 2,3) #(B, Z-2,W-2,H,P)  -> (B,P, Z-2,W-2,H)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LargeMLP_3D((14,32,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96424c93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.tools import getModelSize\n",
    "param_sum, buffer_sum, all_size = getModelSize(model)\n",
    "print(f\" Number of Parameters: {param_sum}, Number of Buffers: {buffer_sum}, Size of Model: {all_size:.4f} MB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074816d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.patch_model import NaiveConvModel2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad10412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveConvModel2D((32,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,20,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import get_center_around_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389817cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape=(32,64)\n",
    "patch_range=5\n",
    "center_index,around_index=get_center_around_indexes(patch_range,img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dad203",
   "metadata": {},
   "outputs": [],
   "source": [
    "around_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20928164",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_range=5\n",
    "delta = [list(range(-(patch_range//2),patch_range//2+1))]*2\n",
    "delta = np.meshgrid(*delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92177012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fd2da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = [list(range(-(patch_range//2),patch_range//2+1))]*len(center)\n",
    "delta = np.meshgrid(*delta)\n",
    "pos  = [c+dc for c,dc in zip(center,delta)]\n",
    "pos[-1]= pos[-1]%64\n",
    "\n",
    "px = x + dx\n",
    "py = y + dy\n",
    "np.stack([px.flatten(),py.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeda73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = np.arange(36).reshape(1,1,6,6)\n",
    "print(coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate[:,:,pos].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71673fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coordinate[:,:,x,y][0,0])\n",
    "print(coordinate[:,:,pos[0],pos[1]][0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.FEDformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66615d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f42098",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class moving_avg_spacetime(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        assert len(kernel_size)==3\n",
    "        self.kernel_size = np.array(kernel_size)\n",
    "        self.avg         = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "        self.pad_front   = self.kernel_size - 1-np.floor((self.kernel_size - 1) // 2)\n",
    "        self.pad_end     = np.floor((self.kernel_size - 1) // 2)\n",
    "        self.pad         = np.stack([self.pad_front,self.pad_end],1)[::-1].flatten().astype('int').tolist()\n",
    "        print(self.pad)\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        # the input must be (B,*Space,T,C)， the -1 dim is embed channel, the -2 dim is time channel\n",
    "        shape = x.shape\n",
    "        BSpace_shape = shape[:-2]\n",
    "        C = shape[-1]\n",
    "        permute_order = [0,-1] + list(range(1,len(shape)-1))\n",
    "        x = x.permute(*permute_order)#-->(B, *Space,T, C)-->(B, C, *Space,T)\n",
    "        x = self.avg(F.pad(x,self.pad, mode='replicate'))\n",
    "        permute_order = [0] + list(range(2,len(shape))) + [1]\n",
    "        x = x.permute(*permute_order)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2193e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = moving_avg_spacetime((5,3,2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(1,32,64,6,7)\n",
    "layer(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.physics_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.afnonet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245af06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=get_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = AFNONet((32,64),2,265,20,depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = DirectSpace_Feature_Model(args,backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.randn(1,4,5,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285eced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std_mean(layer(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Second_Derivative_Layer()\n",
    "\n",
    "a=torch.randn(1,1,32,64)\n",
    "\n",
    "layer(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=2\n",
    "P=4\n",
    "a=torch.randn(B,P,3,32,64).cuda()\n",
    "layer=  First_Derivative_Layer(dim=3).cuda()\n",
    "runtime_weight=layer.runtime_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.conv3d(a.flatten(0,1).unsqueeze(1),runtime_weight).reshape(*a.shape[:-1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x2 = torch.conv1d(a.flatten(0,-2).unsqueeze(1),runtime_weight[0,0]).reshape(*a.shape[:-1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dist(x,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(x,y)\n",
    "#plt.scatter([10],[10],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42446069",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_fourier_drive(x, position,pad):\n",
    "\n",
    "    if len(position)==1:position = position[0]\n",
    "    base_len          = np.array(x.shape[position])\n",
    "    interpolate_shape = base_len + (base_len-1)*pad\n",
    "    interpolate_shape = tuple(interpolate_shape)\n",
    "    if isinstance(position,int) or len(position) == 1\n",
    "        x = x.transpose(-1,position)\n",
    "        out_shape = x.shape[:-1]\n",
    "        x = x.reshape(x.size(0),-1,x.size(-1))\n",
    "        b = torch.nn.functional.interpolate(x,interpolate_shape,mode='linear',align_corners=True)\n",
    "        bf = torch.fft.rfft(b,dim=-1)\n",
    "        kbf= bf*torch.fft.rfftfreq(b.size(-1)).reshape(1,1,-1)\n",
    "        kb = torch.fft.irfft(kbf,dim=-1)\n",
    "        index = torch.arange(0,b.size(-1)+1,base_len-1)\n",
    "        kb = kb[...,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tkinter.messagebox import NO\n",
    "import numpy as np\n",
    "import torch,os,io,socket\n",
    "from torchvision import datasets, transforms\n",
    "hostname = socket.gethostname()\n",
    "from functools import lru_cache\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from utils.timefeatures import time_features\n",
    "import os\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efc1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatimelist  = np.arange(np.datetime64(\"1979-01-01\"), np.datetime64(\"2016-01-01\"), np.timedelta64(6, \"h\"))\n",
    "timestamp = time_features(pd.to_datetime(datatimelist)).transpose(1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d452ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(np.datetime64(\"1980-12-31\")).dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(datatimelist)[0].dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp[:10,[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b05b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JCmodels.fourcastnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AFNONet((32,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ac0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,4,5,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c828a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(a).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55908ea1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### complex version AdaptiveFourierNeuralOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331c5ac",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f64f13",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def multiply(input, weights):\n",
    "    return torch.einsum('...bd,bdk->...bk', input, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf839a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "B=1\n",
    "h=32;w=64;C=2\n",
    "num_blocks=1\n",
    "block_size=2\n",
    "\n",
    "w1 = torch.randn(num_blocks, block_size, block_size, dtype=torch.cfloat)\n",
    "b1 = torch.randn(num_blocks, block_size, dtype=torch.cfloat)\n",
    "w2 = torch.randn(num_blocks, block_size, block_size, dtype=torch.cfloat)\n",
    "b2 = torch.randn(num_blocks, block_size, dtype=torch.cfloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa8d54",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def original_realize(x,w1,b1,w2,b2):\n",
    "    x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho');\n",
    "    x = x.reshape(B, x.shape[1], x.shape[2], num_blocks, block_size)\n",
    "    x_real = F.relu(multiply(x.real, w1.real) - multiply(x.imag, w1.imag) + b1.real, inplace=True)\n",
    "    x_imag = F.relu(multiply(x.real, w1.imag) + multiply(x.imag, w1.real) + b1.imag, inplace=True)\n",
    "    x = torch.stack([x_real, x_imag], dim=-1)\n",
    "    x = torch.view_as_complex(x)\n",
    "    return x\n",
    "\n",
    "def complex_version_realize(x,w1,b1,w2,b2):\n",
    "    x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho');\n",
    "    x = x.reshape(B, x.shape[1], x.shape[2], num_blocks, block_size)\n",
    "    x = multiply(x,w1)+b1\n",
    "    x = nonlinear_activate(x)\n",
    "    return x\n",
    "x  = torch.randn(B, h, w, C)\n",
    "y1 = original_realize(x,w1,b1,w2,b2)\n",
    "y2 = complex_version_realize(x,w1,b1,w2,b2)\n",
    "torch.dist(y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f8cb3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.afnonet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ccd53",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### AFNONet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d11df",
   "metadata": {
    "code_folding": [
     0,
     1,
     6,
     70,
     103,
     132,
     185,
     192,
     203,
     206,
     213,
     223,
     226,
     242
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PartReLU_Complex(nn.Module):\n",
    "    def forward(self,x):\n",
    "        F.relu(x.real, inplace=True)\n",
    "        F.relu(x.imag, inplace=True)\n",
    "        return x\n",
    "\n",
    "class AdaptiveFourierNeuralOperator(nn.Module):\n",
    "    def __init__(self, dim, img_size, fno_blocks=4,fno_bias=True, fno_softshrink=False,nonlinear_activate=PartReLU_Complex()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = dim\n",
    "        self.img_size   = img_size\n",
    "        self.num_blocks = fno_blocks\n",
    "        self.block_size = self.hidden_size // self.num_blocks\n",
    "        assert self.hidden_size % self.num_blocks == 0\n",
    "\n",
    "        self.scale = 0.02\n",
    "        self.w1 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              self.block_size, dtype=torch.cfloat))\n",
    "        self.b1 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              dtype=torch.cfloat))\n",
    "        self.w2 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              self.block_size, dtype=torch.cfloat))\n",
    "        self.b2 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              dtype=torch.cfloat))\n",
    "        self.relu = nonlinear_activate\n",
    "\n",
    "        if fno_bias:\n",
    "            self.bias = nn.Conv1d(self.hidden_size, self.hidden_size, 1)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.softshrink = fno_softshrink\n",
    "\n",
    "    def multiply(self, input, weights):\n",
    "        return torch.einsum('...bd,bdk->...bk', input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape    \n",
    "        bias = self.bias(x.permute(0, 2, 1)).permute(0, 2, 1) if self.bias else 0\n",
    "        #timer.restart(2)\n",
    "        x = x.reshape(B, *self.img_size, C)\n",
    "        fft_dim = tuple(range(1,len(self.img_size)+1))\n",
    "        print(fft_dim)\n",
    "        #timer.record('reshape1','filter',2)\n",
    "        print(x.shape)\n",
    "        x = torch.fft.rfftn(x, dim=fft_dim, norm='ortho');\n",
    "        #timer.record('rfft2','filter',2)\n",
    "        print(x.shape)\n",
    "        x = x.reshape(*x.shape[:-1], self.num_blocks, self.block_size)\n",
    "        #timer.record('reshape2','filter',2)\n",
    "        x = self.multiply(x,self.w1)+self.b1\n",
    "        x = self.relu(x)\n",
    "        x = self.multiply(x,self.w2)+self.b2\n",
    "        x = F.softshrink(x, lambd=self.softshrink) if self.softshrink else x\n",
    "        #with torch.cuda.amp.autocast(enabled=False):\n",
    "        #x = x.float()   \n",
    "        #x = torch.view_as_complex(x)\n",
    "        #timer.record('reset','filter',2)\n",
    "        x = x.flatten(-2,-1)\n",
    "        #timer.record('reshape3','filter',2)\n",
    "        print(x.shape)\n",
    "        x = torch.fft.irfftn(x, s=self.img_size,dim=fft_dim, norm='ortho')\n",
    "        print(x.shape)\n",
    "        #x = x.half()\n",
    "        #timer.record('irfft2','filter',2)\n",
    "        x = x.reshape(B, N, C)\n",
    "        #timer.record('reshape4','filter',2)\n",
    "        return x + bias\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm, region_shape=(14,8), fno_blocks=3,double_skip=False, fno_bias=False, fno_softshrink=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = AdaptiveFourierNeuralOperator(dim, region_shape,fno_blocks=fno_blocks,fno_bias=fno_bias,fno_softshrink=fno_softshrink)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.double_skip = double_skip\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        #timer.restart(1)\n",
    "        x = self.norm1(x)\n",
    "        #timer.record('norm1','forward_features',1)\n",
    "        x = self.filter(x)\n",
    "        #timer.record('filter','forward_features',1)\n",
    "        if self.double_skip:\n",
    "            x += residual\n",
    "            residual = x;\n",
    "        #timer.record('residual','forward_features',1)\n",
    "        x = self.norm2(x)\n",
    "        #timer.record('norm2','forward_features',1)\n",
    "        x = self.mlp(x)\n",
    "        #timer.record('mlp','forward_features',1)\n",
    "        x = self.drop_path(x)\n",
    "        #timer.record('drop_path','forward_features',1)\n",
    "        x += residual\n",
    "        #timer.record('residual','forward_features',1)\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=None, patch_size=8, in_chans=13, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        if img_size is None:raise KeyError('img is None')\n",
    "        patch_size   = [patch_size]*len(img_size) if isinstance(patch_size,int) else patch_size\n",
    "        \n",
    "        num_patches=1\n",
    "        out_size=[]\n",
    "        for i_size,p_size in zip(img_size,patch_size):\n",
    "            if p_size%i_size:\n",
    "                num_patches*=i_size// p_size\n",
    "                out_size.append(i_size// p_size)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"the patch size ({patch_size}) cannot divide the img size {img_size}\")\n",
    "        self.img_size    = tuple(img_size)\n",
    "        self.patch_size  = tuple(patch_size)\n",
    "        self.num_patches = num_patches\n",
    "        self.out_size    = tuple(out_size)\n",
    "        conv_engine = [nn.Conv1d,nn.Conv2d,nn.Conv3d]\n",
    "        self.proj   = conv_engine[len(img_size)-1](in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, = x.shape[:2]\n",
    "        inp_size = x.shape[2:]\n",
    "        assert tuple(inp_size) == self.img_size, f\"Input image size ({inp_size}) doesn't match model set size ({self.img_size}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class AFNONet(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size=8, in_chans=20, out_chans=20, embed_dim=768, depth=12, mlp_ratio=4.,\n",
    "                 uniform_drop=False, drop_rate=0., drop_path_rate=0., norm_layer=None,\n",
    "                 dropcls=0, checkpoint_activations=False, fno_blocks=3,double_skip=False,\n",
    "                 fno_bias=False, fno_softshrink=False,debug_mode=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size is not None\n",
    "        self.checkpoint_activations=checkpoint_activations\n",
    "        self.embed_dim = embed_dim\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.img_size = img_size\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches      = self.patch_embed.num_patches\n",
    "        patch_size       = self.patch_embed.patch_size\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.final_shape = self.patch_embed.out_size\n",
    "\n",
    "        if uniform_drop:\n",
    "            dpr = [drop_path_rate for _ in range(depth)]  # stochastic depth decay rule\n",
    "        else:\n",
    "            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(dim=embed_dim, mlp_ratio=mlp_ratio, drop=drop_rate,\n",
    "                                           drop_path=dpr[i], \n",
    "                                           norm_layer=norm_layer,\n",
    "                                           region_shape=self.final_shape,\n",
    "                                           double_skip=double_skip,\n",
    "                                           fno_blocks=fno_blocks,\n",
    "                                           fno_bias=fno_bias,\n",
    "                                           fno_softshrink=fno_softshrink) for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        # self.num_features = out_chans * img_size[0] * img_size[1]\n",
    "        # self.representation_size = self.num_features * 8\n",
    "        # self.pre_logits = nn.Sequential(OrderedDict([\n",
    "        #     ('fc', nn.Linear(embed_dim, self.representation_size)),\n",
    "        #     ('act', nn.Tanh())\n",
    "        # ]))\n",
    "        conf_list = [{'kernel_size':[],'stride':[],'padding':[]},\n",
    "                     {'kernel_size':[],'stride':[],'padding':[]},\n",
    "                     {'kernel_size':[],'stride':[],'padding':[]}]\n",
    "        conv_set = {8:[[2,2,0],[2,2,0],[2,2,0]],\n",
    "                    4:[[2,2,0],[3,1,1],[2,2,0]],\n",
    "                    2:[[3,1,1],[3,1,1],[2,2,0]],\n",
    "                    1:[[3,1,1],[3,1,1],[3,1,1]],\n",
    "                   }\n",
    "        for patch in patch_size:\n",
    "            for slot in range(len(conf_list)):\n",
    "                conf_list[slot]['kernel_size'].append(conv_set[patch][slot][0])\n",
    "                conf_list[slot]['stride'].append(conv_set[patch][slot][1])\n",
    "                conf_list[slot]['padding'].append(conv_set[patch][slot][2])\n",
    "\n",
    "        transposeconv_engine = [nn.ConvTranspose1d,nn.ConvTranspose2d,nn.ConvTranspose3d][len(img_size)-1]\n",
    "        self.pre_logits = nn.Sequential(OrderedDict([\n",
    "            ('conv1', transposeconv_engine(embed_dim, out_chans*16, **conf_list[0])),\n",
    "            ('act1', nn.Tanh()),\n",
    "            ('conv2', transposeconv_engine(out_chans*16, out_chans*4, **conf_list[1])),\n",
    "            ('act2', nn.Tanh())\n",
    "        ]))\n",
    "\n",
    "        # Generator head\n",
    "        # self.head = nn.Linear(self.representation_size, self.num_features)\n",
    "        self.head = transposeconv_engine(out_chans*4, out_chans, **conf_list[2])\n",
    "\n",
    "        if dropcls > 0:\n",
    "            print('dropout %.2f before classifier' % dropcls)\n",
    "            self.final_dropout = nn.Dropout(p=dropcls)\n",
    "        else:\n",
    "            self.final_dropout = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.debug_mode=debug_mode\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x += self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        if not self.checkpoint_activations:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "        else:\n",
    "            x = checkpoint_sequential(self.blocks, 4, x)\n",
    "\n",
    "        x = self.norm(x).transpose(1, 2);print(x.shape)\n",
    "        x = torch.reshape(x, [-1, self.embed_dim, *self.final_shape])\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### we assume always feed the tensor (B, p*z, h, w)\n",
    "        B, P ,h, w = x.shape\n",
    "        x = x.reshape(B,-1,*self.img_size)\n",
    "        #timer.restart(level=0)\n",
    "        x = self.forward_features(x)\n",
    "        #timer.record('forward_features',level=0)\n",
    "        x = self.final_dropout(x)\n",
    "        #timer.record('final_dropout',level=0)\n",
    "        x = self.pre_logits(x)\n",
    "        #timer.record('pre_logits',level=0)\n",
    "        x = self.head(x)\n",
    "        #timer.record('head',level=0)\n",
    "        x = x.reshape(B,P,h,w)\n",
    "        #timer.show_stat()\n",
    "        #print(\"============================\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c91758",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = AFNONet(img_size=[10,32,64],patch_size=(1,8,8),depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094685c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,20,10,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f0892",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,3,4,32,64).flatten(1,2)\n",
    "torch.dist(a.reshape(1,-1,4,32,64).reshape(1,12,32,64),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc73c64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,12,32,64)\n",
    "torch.dist(a.reshape(1,3,4,32,64).reshape(1,-1,32,64),a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e691946",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### EulerEquationModel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2f336",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EulerEquationModel2(nn.Module):\n",
    "    def __init__(self, args, backbone):\n",
    "        super().__init__()\n",
    "        self.Dx= First_Derivative_Layer(position=-1, dim=3, mode=2)\n",
    "        self.Dy= First_Derivative_Layer(position=-2, dim=3, mode=2)\n",
    "        self.Dz= First_Derivative_Layer(position=-3, dim=3, mode=1)\n",
    "        self.thermal_factor  = nn.Parameter(torch.randn(3).reshape(1,3,1,1))\n",
    "        self.alpha = nn.Parameter(torch.randn(1))\n",
    "        #self.p_list         = nn.Parameter(torch.Tensor([10,8.5,5]).reshape(1,3,1,1),requires_grad=False)\n",
    "        self.backbone =  backbone\n",
    "        self.monitor = True\n",
    "    def forward(self, Field):\n",
    "        # u^{t+1} &= u^{t} + F_x - \\nabla (Vu)  + u \\nabla\\cdot V - \\partial_x\\phi\\\\\n",
    "        # v^{t+1} &= v^{t} + F_y - \\nabla (Vv)  + v \\nabla\\cdot V - \\partial_y\\phi\\\\\n",
    "        # T^{t+1} &= T^{t} + Q/C_v + \\frac{RT}{C_pp}\\omega  - \\nabla (VT) +T \\nabla\\cdot V\\\\\n",
    "        # \\phi^{t+1}&=\\phi^{t} + wg  - \\nabla (V\\phi)+ \\phi \\nabla\\cdot V \\\\\n",
    "        # 0&\\approx \\nabla\\cdot V\n",
    "        # input -> Field  = [u ,v, T, p] --> (Batch, 4, z, y ,x)\n",
    "        # need generate unknown data [Fx, Fy , Q, W, o]\n",
    "        b, si_z, i_y, i_x = Field.shape\n",
    "        s=4\n",
    "        i_z= si_z//4\n",
    "        MachineLearningPart = self.backbone(Field).reshape(b, s+1, i_z, i_y, i_x) #(Batch, 5, z, y ,x)\n",
    "        ExternalForce = MachineLearningPart[:,:4] #(Batch, 4, z, y ,x)\n",
    "        o = MachineLearningPart[:,4:5] #(Batch, 1, z, y ,x)\n",
    "        Field = Field.reshape(b, s, i_z, i_y,  i_x) #(Batch, 5, z, y ,x)\n",
    "        u = Field[:,0:1]#(Batch, 1, z, y ,x)\n",
    "        v = Field[:,1:2]#(Batch, 1, z, y ,x)\n",
    "        T = Field[:,2:3]#(Batch, 1, z, y ,x)\n",
    "        p = Field[:,3:4]#(Batch, 1, z, y ,x)\n",
    "        V = torch.cat([u,v,o],1)#(Batch, 3, z, y ,x)\n",
    "        Nabla_cdot_V = (self.Dx(u[:,0]) + self.Dy(v[:,0]) + self.Dz(o[:,0])).unsqueeze(1)#(Batch, 1, z, y ,x)\n",
    "        Nabla_V_Field= Nabla_cdot_V*Field #(Batch, 4, z, y ,x)\n",
    "        Vphysics     = torch.stack([V*u,V*v,V*T,V*p],1)#(Batch,4, 3, z, y ,x)\n",
    "        Vphysics_dx = self.Dx(Vphysics[:,:,0].flatten(0,1)).reshape(Field.shape)#(Batch, 4, z, y ,x)\n",
    "        Vphysics_dy = self.Dy(Vphysics[:,:,1].flatten(0,1)).reshape(Field.shape)#(Batch, 4, z, y ,x)\n",
    "        Vphysics_dz = self.Dz(Vphysics[:,:,2].flatten(0,1)).reshape(Field.shape)#(Batch, 4, z, y ,x)\n",
    "        PhysicsPart = -Vphysics_dx - Vphysics_dy - Vphysics_dz + Nabla_V_Field #(Batch,4,z, y ,x)\n",
    "        InteractionPart= torch.stack([self.Dx(p[:,0]),\n",
    "                                      self.Dy(p[:,0]),\n",
    "                                      self.thermal_factor*T[:,0]*o[:,0]],1)#(Batch,3,z, y ,x)\n",
    "        InteractionPart= F.pad(InteractionPart,(0,0,0,0,0,0,0,1)) #(Batch,4,z, y ,x)\n",
    "        Delta_Fd     = ExternalForce + self.alpha*(PhysicsPart + InteractionPart)\n",
    "        Field        = Field+ Delta_Fd\n",
    "        constrain    = Nabla_V_Field\n",
    "        if not self.monitor:\n",
    "            return Field.flatten(1,2),(constrain**2).mean()\n",
    "        else:\n",
    "            return Field.flatten(1,2),(constrain**2).mean(),{\"ExternalForceFactor\":(ExternalForce**2).mean().item(),\n",
    "                                                             \"PhysicsDrivenFactor\":(PhysicsPart**2).mean().item(),\n",
    "                                                             \"InteractionPart\":(InteractionPart**2).mean().item(),\n",
    "                                                             \"alpha\":self.alpha.item()}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b4e50d8356ba75a9636787f9051ee458aaf13e87df491415cc800c7b2b8a21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
