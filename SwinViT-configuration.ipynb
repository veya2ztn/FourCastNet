{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0247ba3",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62415a6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Our Implemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf039b2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cephdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee2d13e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "single_vnames = [\"2m_temperature\",\n",
    "                  \"10m_u_component_of_wind\",\n",
    "                  \"10m_v_component_of_wind\",\n",
    "                  \"total_cloud_cover\",\n",
    "                  \"total_precipitation\",\n",
    "                  \"toa_incident_solar_radiation\"]\n",
    "level_vnames= []\n",
    "for physics_name in [\"geopotential\", \"temperature\",\n",
    "                     \"specific_humidity\",\"relative_humidity\",\n",
    "                     \"u_component_of_wind\",\"v_component_of_wind\",\n",
    "                     \"vorticity\",\"potential_vorticity\"]:\n",
    "    for pressure_level in [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]:\n",
    "        level_vnames.append(f\"{pressure_level}hPa_{physics_name}\")\n",
    "all_vnames = single_vnames + level_vnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923d8f4d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0]: 2m_temperature\n",
      "[  1]: 10m_u_component_of_wind\n",
      "[  2]: 10m_v_component_of_wind\n",
      "[  3]: total_cloud_cover\n",
      "[  4]: total_precipitation\n",
      "[  5]: toa_incident_solar_radiation\n",
      "[  6]: 50hPa_geopotential\n",
      "[  7]: 100hPa_geopotential\n",
      "[  8]: 150hPa_geopotential\n",
      "[  9]: 200hPa_geopotential\n",
      "[ 10]: 250hPa_geopotential\n",
      "[ 11]: 300hPa_geopotential\n",
      "[ 12]: 400hPa_geopotential\n",
      "[ 13]: 500hPa_geopotential\n",
      "[ 14]: 600hPa_geopotential\n",
      "[ 15]: 700hPa_geopotential\n",
      "[ 16]: 850hPa_geopotential\n",
      "[ 17]: 925hPa_geopotential\n",
      "[ 18]: 1000hPa_geopotential\n",
      "[ 19]: 50hPa_temperature\n",
      "[ 20]: 100hPa_temperature\n",
      "[ 21]: 150hPa_temperature\n",
      "[ 22]: 200hPa_temperature\n",
      "[ 23]: 250hPa_temperature\n",
      "[ 24]: 300hPa_temperature\n",
      "[ 25]: 400hPa_temperature\n",
      "[ 26]: 500hPa_temperature\n",
      "[ 27]: 600hPa_temperature\n",
      "[ 28]: 700hPa_temperature\n",
      "[ 29]: 850hPa_temperature\n",
      "[ 30]: 925hPa_temperature\n",
      "[ 31]: 1000hPa_temperature\n",
      "[ 32]: 50hPa_specific_humidity\n",
      "[ 33]: 100hPa_specific_humidity\n",
      "[ 34]: 150hPa_specific_humidity\n",
      "[ 35]: 200hPa_specific_humidity\n",
      "[ 36]: 250hPa_specific_humidity\n",
      "[ 37]: 300hPa_specific_humidity\n",
      "[ 38]: 400hPa_specific_humidity\n",
      "[ 39]: 500hPa_specific_humidity\n",
      "[ 40]: 600hPa_specific_humidity\n",
      "[ 41]: 700hPa_specific_humidity\n",
      "[ 42]: 850hPa_specific_humidity\n",
      "[ 43]: 925hPa_specific_humidity\n",
      "[ 44]: 1000hPa_specific_humidity\n",
      "[ 45]: 50hPa_relative_humidity\n",
      "[ 46]: 100hPa_relative_humidity\n",
      "[ 47]: 150hPa_relative_humidity\n",
      "[ 48]: 200hPa_relative_humidity\n",
      "[ 49]: 250hPa_relative_humidity\n",
      "[ 50]: 300hPa_relative_humidity\n",
      "[ 51]: 400hPa_relative_humidity\n",
      "[ 52]: 500hPa_relative_humidity\n",
      "[ 53]: 600hPa_relative_humidity\n",
      "[ 54]: 700hPa_relative_humidity\n",
      "[ 55]: 850hPa_relative_humidity\n",
      "[ 56]: 925hPa_relative_humidity\n",
      "[ 57]: 1000hPa_relative_humidity\n",
      "[ 58]: 50hPa_u_component_of_wind\n",
      "[ 59]: 100hPa_u_component_of_wind\n",
      "[ 60]: 150hPa_u_component_of_wind\n",
      "[ 61]: 200hPa_u_component_of_wind\n",
      "[ 62]: 250hPa_u_component_of_wind\n",
      "[ 63]: 300hPa_u_component_of_wind\n",
      "[ 64]: 400hPa_u_component_of_wind\n",
      "[ 65]: 500hPa_u_component_of_wind\n",
      "[ 66]: 600hPa_u_component_of_wind\n",
      "[ 67]: 700hPa_u_component_of_wind\n",
      "[ 68]: 850hPa_u_component_of_wind\n",
      "[ 69]: 925hPa_u_component_of_wind\n",
      "[ 70]: 1000hPa_u_component_of_wind\n",
      "[ 71]: 50hPa_v_component_of_wind\n",
      "[ 72]: 100hPa_v_component_of_wind\n",
      "[ 73]: 150hPa_v_component_of_wind\n",
      "[ 74]: 200hPa_v_component_of_wind\n",
      "[ 75]: 250hPa_v_component_of_wind\n",
      "[ 76]: 300hPa_v_component_of_wind\n",
      "[ 77]: 400hPa_v_component_of_wind\n",
      "[ 78]: 500hPa_v_component_of_wind\n",
      "[ 79]: 600hPa_v_component_of_wind\n",
      "[ 80]: 700hPa_v_component_of_wind\n",
      "[ 81]: 850hPa_v_component_of_wind\n",
      "[ 82]: 925hPa_v_component_of_wind\n",
      "[ 83]: 1000hPa_v_component_of_wind\n",
      "[ 84]: 50hPa_vorticity\n",
      "[ 85]: 100hPa_vorticity\n",
      "[ 86]: 150hPa_vorticity\n",
      "[ 87]: 200hPa_vorticity\n",
      "[ 88]: 250hPa_vorticity\n",
      "[ 89]: 300hPa_vorticity\n",
      "[ 90]: 400hPa_vorticity\n",
      "[ 91]: 500hPa_vorticity\n",
      "[ 92]: 600hPa_vorticity\n",
      "[ 93]: 700hPa_vorticity\n",
      "[ 94]: 850hPa_vorticity\n",
      "[ 95]: 925hPa_vorticity\n",
      "[ 96]: 1000hPa_vorticity\n",
      "[ 97]: 50hPa_potential_vorticity\n",
      "[ 98]: 100hPa_potential_vorticity\n",
      "[ 99]: 150hPa_potential_vorticity\n",
      "[100]: 200hPa_potential_vorticity\n",
      "[101]: 250hPa_potential_vorticity\n",
      "[102]: 300hPa_potential_vorticity\n",
      "[103]: 400hPa_potential_vorticity\n",
      "[104]: 500hPa_potential_vorticity\n",
      "[105]: 600hPa_potential_vorticity\n",
      "[106]: 700hPa_potential_vorticity\n",
      "[107]: 850hPa_potential_vorticity\n",
      "[108]: 925hPa_potential_vorticity\n",
      "[109]: 1000hPa_potential_vorticity\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(all_vnames):\n",
    "    print(f\"[{i:3d}]: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ca91cb6",
   "metadata": {
    "code_folding": [
     3,
     13,
     21
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WeathBench32x64CK(WeathBench):\n",
    "    default_root = 'datasets/weatherbench32x64'\n",
    "    \n",
    "    def config_pool_initial(self):\n",
    "        CK_order = [1, 2, 0, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, \n",
    "               29, 30, 31, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, \n",
    "               68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n",
    "        config_pool={\n",
    "            'SWINRNN69' : (CK_order    ,'gauss_norm'   , self.mean_std[:,CK_order].reshape(2,69,1,1)      , identity, identity ),\n",
    "        }\n",
    "        self.constant_index = [0,2]\n",
    "        return config_pool\n",
    "        \n",
    "    def load_numpy_from_url(self,url):#the saved numpy is not buffer, so use normal reading\n",
    "        if \"s3://\" in url:\n",
    "            if self.client is None:self.client=Client(conf_path=\"~/petreloss.conf\")\n",
    "            with io.BytesIO(self.client.get(url)) as f:\n",
    "                array = np.load(f)\n",
    "        else:\n",
    "            array = np.load(url)\n",
    "        return array\n",
    "    \n",
    "    def get_item(self,idx,reversed_part=False):\n",
    "        year, hour = self.single_data_path_list[idx]\n",
    "        url  = f\"{self.root}/{year}/{year}-{hour:04d}.npy\"\n",
    "        print(url)\n",
    "        odata = np.load(url)\n",
    "        data = odata[self.channel_choice]\n",
    "        data = (data - self.mean)/self.std\n",
    "        cons = self.constants[self.constant_index]\n",
    "        return np.concatenate([cons,data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae23ed35",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use dataset in datasets/weatherbench32x64\n"
     ]
    }
   ],
   "source": [
    "dataset2 = WeathBench32x64CK('valid',dataset_flag='SWINRNN69',time_intervel=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725f142",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50f0d017",
   "metadata": {
    "code_folding": [
     7,
     14,
     17,
     20,
     22,
     31,
     32,
     87,
     102,
     109,
     116,
     134,
     143,
     159,
     162,
     165,
     176,
     195,
     199
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import io\n",
    "import torch\n",
    "import time\n",
    "\n",
    "Years = {\n",
    "    'train': range(1979, 2016),\n",
    "    'valid': range(2018, 2019),\n",
    "    'test': range(2016, 2018),\n",
    "    'all': range(1979, 2019)\n",
    "}\n",
    "\n",
    "multi_level_vnames = [\n",
    "    \"z\", \"t\", \"q\", \"r\", \"u\", \"v\", \"vo\", \"pv\",\n",
    "]\n",
    "single_level_vnames = [\n",
    "    \"t2m\", \"u10\", \"v10\", \"tcc\", \"tp\", \"tisr\",\n",
    "]\n",
    "long2shortname_dict = {\"geopotential\": \"z\", \"temperature\": \"t\", \"specific_humidity\": \"q\", \"relative_humidity\": \"r\", \"u_component_of_wind\": \"u\", \"v_component_of_wind\": \"v\", \"vorticity\": \"vo\", \"potential_vorticity\": \"pv\", \\\n",
    "    \"2m_temperature\": \"t2m\", \"10m_u_component_of_wind\": \"u10\", \"10m_v_component_of_wind\": \"v10\", \"total_cloud_cover\": \"tcc\", \"total_precipitation\": \"tp\", \"toa_incident_solar_radiation\": \"tisr\"}\n",
    "constants = [\n",
    "    \"lsm\", \"slt\", \"orography\"\n",
    "]\n",
    "height_level = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
    "\n",
    "multi_level_dict_param = {\"z\":height_level, \"t\": height_level, \"q\": height_level, \"r\": height_level}\n",
    "\n",
    "\n",
    "\n",
    "class WeatherBench(Dataset):\n",
    "    def __init__(self, data_dir='dataset/weatherbench32x64', split='train', **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.use_mem = kwargs.get('use_mem', False)\n",
    "        self.length = kwargs.get('length', 1)\n",
    "        self.file_stride = kwargs.get('file_stride', 1)\n",
    "        self.sample_stride = kwargs.get('sample_stride', 1)\n",
    "        self.output_meanstd = kwargs.get(\"output_meanstd\", False)\n",
    "\n",
    "        vnames_type = kwargs.get(\"vnames\", {})\n",
    "        self.constants_types = vnames_type.get('constants', [])\n",
    "        self.single_level_vnames = vnames_type.get('single_level_vnames', single_level_vnames)\n",
    "        self.multi_level_dict =multi_level_dict= vnames_type.get('multi_level_vnames_dict', {\"z\": height_level, \"v\": height_level})\n",
    "        multi_height_dict = vnames_type.get(\"height_level\", None)\n",
    "\n",
    "\n",
    "        self.single_level_vnames_index = [single_level_vnames.index(level) for level in self.single_level_vnames]\n",
    "        if multi_level_dict is not None:\n",
    "            index = [[multi_level_vnames.index(i), height_level.index(j)] for i in multi_level_dict for j in multi_level_dict[i]]\n",
    "            self.total_data_index = self.single_level_vnames_index + [6+j+13*i for i,j in index]\n",
    "        elif multi_height_dict is not None:\n",
    "            index = [[multi_level_vnames.index(i), height_level.index(j)] for j in multi_height_dict for i in multi_height_dict[j]]\n",
    "            self.total_data_index = self.single_level_vnames_index + [6+j+13*i for i,j in index]\n",
    "\n",
    "\n",
    "        constants_index = [constants.index(constant) for constant in self.constants_types]\n",
    "        \n",
    "        print(f\"constants_index={constants_index}\")\n",
    "        print(f\"total_data_index={self.total_data_index}\")\n",
    "        \n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        #self.client = Client(conf_path=\"~/petreloss.conf\")\n",
    "        years = Years[split]\n",
    "        self.file_list = self.init_file_list(years)\n",
    "\n",
    "        # print(constants_index)\n",
    "        if len(constants_index) > 0:\n",
    "            self.constants_data = self.get_constants_data(constants_index)\n",
    "        else:\n",
    "            self.constants_data = None\n",
    "        # if self.constants_data is not None:\n",
    "        #     print(self.constants_data.shape)\n",
    "        # print(\"get meanstd...\")\n",
    "        dataset_meanstd = self._get_meanstd()\n",
    "        # print(dataset_meanstd.shape)\n",
    "        # print(\"get meanstd complete\")\n",
    "        self.data_mean = dataset_meanstd[0]\n",
    "        self.data_std = dataset_meanstd[1]\n",
    "        # print(\"mean shape:\", self.data_mean.shape)\n",
    "        # print(\"std shape:\", self.data_std.shape)\n",
    "        if self.use_mem:\n",
    "            self.data = self.preload_era5_ceph()\n",
    "            # print(self.data.shape)\n",
    "\n",
    "\n",
    "    def init_file_list(self, years):\n",
    "        file_list = []\n",
    "        for year in years:\n",
    "            if year == 1979:                                    # 1979年数据只有8753个，缺少第一天前7小时数据，所以这里我们从第二天开始算起\n",
    "                for hour in range(17, 8753, self.file_stride):\n",
    "                    file_list.append([year, hour])\n",
    "            else:\n",
    "                if year % 4 == 0:\n",
    "                    max_item = 8784\n",
    "                else:\n",
    "                    max_item = 8760\n",
    "                for hour in range(0, max_item, self.file_stride):\n",
    "                    file_list.append([year, hour])\n",
    "        return file_list\n",
    "\n",
    "    def _get_meanstd(self):\n",
    "        url = f\"{self.data_dir}/mean_std.npy\"\n",
    "        # with io.BytesIO(self.client.get(url)) as f:\n",
    "        #     array = np.load(f)                                  #(2, C)\n",
    "        array = self.load_numpy_from_url(url)\n",
    "        return array\n",
    "\n",
    "    def get_constants_data(self, constants_index):\n",
    "        url = f\"{self.data_dir}/constants.npy\"\n",
    "        # with io.BytesIO(self.client.get(url)) as f:\n",
    "        #     array = np.load(f)                                  #(3, H, W)\n",
    "        array = self.load_numpy_from_url(url)[constants_index]\n",
    "        return array\n",
    "\n",
    "    def preload_era5_ceph(self):\n",
    "        arrays = []\n",
    "        pbar = range(len(self.file_list))\n",
    "        for inum in pbar:\n",
    "            # end_time = time.time() \n",
    "            level_data = self._load_array(inum)\n",
    "            # if self.constants_data is not None:\n",
    "            #     array = torch.cat((self.constants_data, level_data), dim=0).unsqueeze(0)\n",
    "            # else:\n",
    "            array = level_data.unsqueeze(0)\n",
    "            arrays.append(array)\n",
    "            # print(\"step time:\", time.time()-end_time)\n",
    "        arrays = torch.cat(arrays, dim=0)\n",
    "        arrays = arrays - self.data_mean.unsqueeze(-1).unsqueeze(-1)\n",
    "        arrays = arrays / self.data_std.unsqueeze(-1).unsqueeze(-1)\n",
    "        # print(arrays.shape)\n",
    "        return arrays\n",
    "\n",
    "    def load_numpy_from_url(self,url):#the saved numpy is not buffer, so use normal reading\n",
    "        if \"s3://\" in url:\n",
    "            if self.client is None:self.client=Client(conf_path=\"~/petreloss.conf\")\n",
    "            with io.BytesIO(self.client.get(url)) as f:\n",
    "                array = np.load(f)\n",
    "        else:\n",
    "            array = np.load(url)\n",
    "        return torch.from_numpy(array)\n",
    "    \n",
    "    def _load_array(self, index):\n",
    "        year, hour = self.file_list[index]\n",
    "        url = f\"{self.data_dir}/{year}/{year}-{hour:04d}.npy\"\n",
    "        print(url)\n",
    "        # end_time = time.time()\n",
    "        # with io.BytesIO(self.client.get(url)) as f:\n",
    "        #     array = np.load(f)                          # C H W\n",
    "        # print(\"load array time:\", time.time()-end_time)\n",
    "\n",
    "        array = self.load_numpy_from_url(url)\n",
    "        # array -= self.data_mean.unsqueeze(-1).unsqueeze(-1)\n",
    "        # array /= self.data_std.unsqueeze(-1).unsqueeze(-1)\n",
    "        # array = array[self.total_data_index]\n",
    "        # print(array)\n",
    "        return array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) - (self.length-1) * self.sample_stride\n",
    "\n",
    "    def get_meanstd(self):\n",
    "        return self.data_mean[self.total_data_index], self.data_std[self.total_data_index]\n",
    "\n",
    "    def get_clim_daily(self):\n",
    "        url = f\"{self.data_dir}/time_means_daily.npy\"\n",
    "        # with io.BytesIO(self.client.get(url)) as f:\n",
    "        #     array = np.load(f)                                  #(8760, 110, 32, 64)\n",
    "        # array = torch.from_numpy(array)\n",
    "        array = self.load_numpy_from_url(url)\n",
    "        time_index = list(range(0, 8760, self.file_stride))\n",
    "        array = (array - self.data_mean.unsqueeze(-1).unsqueeze(-1)) / self.data_std.unsqueeze(-1).unsqueeze(-1)\n",
    "        array = array[time_index].transpose(0, 1)[self.total_data_index].transpose(0,1)\n",
    "        return array\n",
    "    \n",
    "    def feature_name(self):\n",
    "        # single_vnames = [\"2m_temperature\",\n",
    "        #               \"10m_u_component_of_wind\",\n",
    "        #               \"10m_v_component_of_wind\",\n",
    "        #               \"total_cloud_cover\",\n",
    "        #               \"total_precipitation\",\n",
    "        #               \"toa_incident_solar_radiation\"]\n",
    "        # level_vnames= []\n",
    "        # for physics_name in [\"geopotential\", \"temperature\",\n",
    "        #                     \"specific_humidity\",\"relative_humidity\",\n",
    "        #                     \"u_component_of_wind\",\"v_component_of_wind\",\n",
    "        #                     \"vorticity\",\"potential_vorticity\"]:\n",
    "        #     for pressure_level in [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]:\n",
    "        #         level_vnames.append(f\"{pressure_level}hPa_{physics_name}\")\n",
    "        # all_vnames = single_vnames + level_vnames\n",
    "        multilevelindex = [f\"{i}{j}\" for i in self.multi_level_dict for j in self.multi_level_dict[i]]\n",
    "        vanames = self.single_level_vnames + multilevelindex\n",
    "        return vanames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = min(index, len(self.file_list) - (self.length-1) * self.sample_stride - 1)\n",
    "        array_seq = []\n",
    "        for i in range(self.length):\n",
    "            if self.use_mem:\n",
    "                if self.constants_data is not None:\n",
    "                    return_data = torch.cat((self.constants_data, self.data[index + i * self.sample_stride][self.total_data_index]), dim=0)\n",
    "                else:\n",
    "                    return_data = self.data[index + i * self.sample_stride][self.total_data_index]\n",
    "                array_seq.append(return_data)\n",
    "            else:\n",
    "                data_after_norm = (self._load_array(index + i * self.sample_stride)-self.data_mean.unsqueeze(-1).unsqueeze(-1)) / self.data_std.unsqueeze(-1).unsqueeze(-1)\n",
    "                if self.constants_data is not None:\n",
    "                    return_data = torch.cat((self.constants_data, data_after_norm[self.total_data_index]), dim=0)\n",
    "                else:\n",
    "                    return_data = data_after_norm[self.total_data_index]\n",
    "\n",
    "                array_seq.append(return_data)\n",
    "        return array_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df9ac38",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d714cff4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"../wpredict-wp32x64_2/configs/swinvrnn/lgnet.yaml\", 'r') as cfg_file:\n",
    "    cfg_params = yaml.load(cfg_file, Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7aea380",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constants_index=[0, 2]\n",
      "total_data_index=[1, 2, 0, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n"
     ]
    }
   ],
   "source": [
    "cfg_params['dataset']['test']['data_dir']='datasets/weatherbench32x64'\n",
    "datasets = WeatherBench(split='test',**cfg_params['dataset']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bc1cfff",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17538"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05b32a60",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17533"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92a94e23",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/weatherbench32x64/2016/2016-0003.npy\n",
      "datasets/weatherbench32x64/2016/2016-0009.npy\n",
      "datasets/weatherbench32x64/2016/2016-0003.npy\n",
      "datasets/weatherbench32x64/2016/2016-0009.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index= 3\n",
    "np.linalg.norm(datasets[index][1].numpy()-dataset2[index][1],axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b64fe9e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/weatherbench32x64/2016/2016-0012.npy\n",
      "datasets/weatherbench32x64/2016/2016-0002.npy\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "index = min(index, len(datasets.file_list) - (datasets.length-1) * datasets.sample_stride - 1)\n",
    "year, hour = datasets.file_list[index]\n",
    "url = f\"{datasets.data_dir}/{year}/{year}-{hour:04d}.npy\"\n",
    "print(url)\n",
    "array = datasets.load_numpy_from_url(url)\n",
    "data_after_norm = (array-datasets.data_mean.unsqueeze(-1).unsqueeze(-1)) / datasets.data_std.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "year, hour = dataset2.single_data_path_list[index]\n",
    "url  = f\"{dataset2.root}/{year}/{year}-{hour:04d}.npy\"\n",
    "print(url)\n",
    "odata = np.load(url)\n",
    "# data = odata[self.channel_choice]\n",
    "# data = (data - self.mean)/self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f492f501",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(data_origin-odata,axis=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46e726",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956ff01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71de6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.pretrain import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f048b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args= get_args(\"checkpoints/WeathBench32x64CK/SWIN_Feature-LoRAGraphCastDGLSym/ts_22_fourcast-2D706N_per_6_step/02_19_18_00_34835-seed_73001/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b045c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 14:39:26,122 model args: img_size= (32, 64)\n",
      "2023-02-21 14:39:26,124 model args: patch_size= 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log at debug\n",
      "wandb id: None\n",
      "wandb is off, the recorder list is  ['tensorboard'], we pass wandb\n",
      "\n",
      "            This is ===> GraphCast Model(DGL) <===\n",
      "            Information: \n",
      "                total mesh node: 2562 total unique mesh edge:10230*2=20460 \n",
      "                total grid node 2048+2 = 2050 but activated grid 1928 \n",
      "                from activated grid to mesh, create 4*2562 - 6 = 10242 edges. (north and south pole repeat 4 times) \n",
      "                there are 122 unactivated grid node\n",
      "                when mapping node to grid, \n",
      "                from node to activated grid, there are 10032 edges\n",
      "                from node to unactivated grid, there are 976 edges\n",
      "                thus, totally have 11008 edge. \n",
      "                #notice some grid only have 1-2 linked node but some grid may have 30 lined node\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 14:39:26,957 use model ==> SWIN_Feature\n",
      "2023-02-21 14:39:26,959 Rank: 0, Local_rank: 0 | Number of Parameters: 26296320, Number of Buffers: 0, Size of Model: 100.3125 MB\n",
      "\n",
      "2023-02-21 14:39:29,023 use lr_scheduler:<timm.scheduler.cosine_lr.CosineLRScheduler object at 0x7f2234c45fa0>\n"
     ]
    }
   ],
   "source": [
    "args.use_wandb=0\n",
    "args.gpu = args.local_rank = gpu  = local_rank = 0\n",
    "##### parse args: dataset_kargs / model_kargs / train_kargs  ###########\n",
    "args= parse_default_args(args)\n",
    "SAVE_PATH = get_ckpt_path(args)\n",
    "SAVE_PATH = \"debug\"\n",
    "args.SAVE_PATH = str(SAVE_PATH)\n",
    "#args.pretrain_weight = os.path.join(args.SAVE_PATH,'pretrain_latest.pt')\n",
    "########## inital log ###################\n",
    "logsys = create_logsys(args,False)\n",
    "args.distributed = False\n",
    "\n",
    "if args.distributed:\n",
    "    if args.dist_url == \"env://\" and args.rank == -1:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "    if args.multiprocessing_distributed:\n",
    "        # For multiprocessing distributed training, rank needs to be the\n",
    "        # global rank among all the processes\n",
    "        args.rank = args.rank * ngpus_per_node + local_rank\n",
    "    logsys.info(f\"start init_process_group,backend={args.dist_backend}, init_method={args.dist_url},world_size={args.world_size}, rank={args.rank}\")\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "model           = build_model(args)\n",
    "#param_groups    = timm.optim.optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "optimizer,lr_scheduler,criterion = build_optimizer(args,model)\n",
    "loss_scaler     = torch.cuda.amp.GradScaler(enabled=True)\n",
    "logsys.info(f'use lr_scheduler:{lr_scheduler}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ee63ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.subweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = os.path.join(ckpt_path,\"backbone.best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e72e27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model           \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects_local/FourCastNet/train/pretrain.py:2861\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   2859\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mdeterministic   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# the key for continue training.\u001b[39;00m\n\u001b[1;32m   2860\u001b[0m logsys \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mlogsys\n\u001b[0;32m-> 2861\u001b[0m \u001b[43mlogsys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel args: img_size= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mimg_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2862\u001b[0m logsys\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel args: patch_size= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mpatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2863\u001b[0m args\u001b[38;5;241m.\u001b[39mmodel_kargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_up_sample_channel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "model  = build_model(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
