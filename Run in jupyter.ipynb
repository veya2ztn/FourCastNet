{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be5e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4f0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88ecb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(16,32,64,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bf6413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = torch.fft.rfftn(a[:1], dim=(1,2), norm='ortho')\n",
    "out2 = torch.fft.rfftn(a, dim=(1,2), norm='ortho')[0:1]\n",
    "out3 = vmap(lambda x:torch.fft.rfftn(x,dim=(0,1), norm='ortho'))(a)[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "221d4e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5997e-05)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(out1,out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e76e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f6c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f25f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad518c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme/zhangtianning/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /nvme/zhangtianning/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cephdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab07f72",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from train.pretrain import *\n",
    "from train.pretrain import get_args\n",
    "from mltool.universal_model_util import get_model_para_detail\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d8affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/WeathBench7066/AFNONet/ts_3_finetune-2D706N_per_1_step/01_09_02_41_31-seed_73001/\"\n",
    "args=get_args(os.path.join(ckpt_path,\"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4495fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_type = \"GraphCastFast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e70daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.port = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863485e8",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 20:26:09,741 model args: img_size= (32, 64)\n",
      "2023-02-13 20:26:09,742 model args: patch_size= 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log at debug\n",
      "wandb id: None\n",
      "wandb is off, the recorder list is  ['tensorboard'], we pass wandb\n",
      "\n",
      "        This is ===> GraphCast Model (Fast) <===\n",
      "        Information: \n",
      "            total mesh node: 2562 total unique mesh edge:10230 \n",
      "            total grid node 2048+2 = 2050 but activated grid  1926 +  2\n",
      "            from activated grid to mesh, create 4*2562 = 10242 edge\n",
      "            there are 122 unactivated grid node\n",
      "            when mapping node to grid, \n",
      "            from node to activated grid, there are 10242 \n",
      "            from node to unactivated grid, there are 976 edge\n",
      "            thus, totally have 11218 edge. \n",
      "            #notice some grid only have 1-2 linked node but some grid may have 30 lined node\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 20:26:10,280 use model ==> GraphCastFast\n",
      "2023-02-13 20:26:10,281 Rank: 0, Local_rank: 0 | Number of Parameters: 59722054, Number of Buffers: 0, Size of Model: 227.8216 MB\n",
      "\n",
      "2023-02-13 20:26:11,871 use lr_scheduler:<timm.scheduler.cosine_lr.CosineLRScheduler object at 0x7f12c5fc7430>\n"
     ]
    }
   ],
   "source": [
    "args.use_wandb=0\n",
    "args.gpu = args.local_rank = gpu  = local_rank = 0\n",
    "##### parse args: dataset_kargs / model_kargs / train_kargs  ###########\n",
    "args= parse_default_args(args)\n",
    "SAVE_PATH = get_ckpt_path(args)\n",
    "SAVE_PATH = \"debug\"\n",
    "args.SAVE_PATH = str(SAVE_PATH)\n",
    "#args.pretrain_weight = os.path.join(args.SAVE_PATH,'pretrain_latest.pt')\n",
    "########## inital log ###################\n",
    "logsys = create_logsys(args,False)\n",
    "args.distributed = False\n",
    "\n",
    "if args.distributed:\n",
    "    if args.dist_url == \"env://\" and args.rank == -1:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "    if args.multiprocessing_distributed:\n",
    "        # For multiprocessing distributed training, rank needs to be the\n",
    "        # global rank among all the processes\n",
    "        args.rank = args.rank * ngpus_per_node + local_rank\n",
    "    logsys.info(f\"start init_process_group,backend={args.dist_backend}, init_method={args.dist_url},world_size={args.world_size}, rank={args.rank}\")\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "model           = build_model(args)\n",
    "#param_groups    = timm.optim.optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "optimizer,lr_scheduler,criterion = build_optimizer(args,model)\n",
    "loss_scaler     = torch.cuda.amp.GradScaler(enabled=True)\n",
    "logsys.info(f'use lr_scheduler:{lr_scheduler}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1c74659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.mode = 'pretrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c2aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = os.path.join(ckpt_path,\"backbone.best.pt\")\n",
    "args.pretrain_weight = \"\"#pretrain_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "287c52be",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 19:25:14,386 loading weight from \n",
      "2023-02-13 19:25:14,388 done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dont find path, please check, we pass........\n"
     ]
    }
   ],
   "source": [
    "logsys.info(f\"loading weight from {args.pretrain_weight}\")\n",
    "start_epoch, start_step, min_loss = load_model(model.module if args.distributed else model, optimizer, lr_scheduler, loss_scaler, path=args.pretrain_weight, \n",
    "                    only_model= (args.mode=='fourcast') or (args.mode=='finetune' and not args.continue_train) ,loc = 'cuda:{}'.format(args.gpu))\n",
    "if args.more_epoch_train:\n",
    "    assert args.pretrain_weight\n",
    "    print(f\"detect more epoch training, we will do a copy processing for {args.pretrain_weight}\")\n",
    "    os.system(f'cp {args.pretrain_weight} {args.pretrain_weight}-epoch{start_epoch}')\n",
    "logsys.info(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fb9cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1f2304ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test.npy\n"
     ]
    }
   ],
   "source": [
    "args.valid_batch_size = 4\n",
    "args.dataset_kargs['use_offline_data']=0\n",
    "test_dataset,  test_dataloader = get_test_dataset(args,test_dataset_tensor=None,test_record_load=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc331e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test.npy\n",
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 19:25:17,672 use dataset ==> WeathBench7066\n",
      "2023-02-13 19:25:17,673 Start training for 100 epochs\n"
     ]
    }
   ],
   "source": [
    "args.debug  = 1 \n",
    "train_dataset, val_dataset, train_dataloader,val_dataloader = get_train_and_valid_dataset(args,\n",
    "               train_dataset_tensor=None,train_record_load=None,\n",
    "               valid_dataset_tensor=None,valid_record_load=None)\n",
    "logsys.info(f\"use dataset ==> {train_dataset.__class__.__name__}\")\n",
    "logsys.info(f\"Start training for {args.epochs} epochs\")\n",
    "master_bar = logsys.create_master_bar(args.epochs)\n",
    "accu_list = ['valid_loss']\n",
    "metric_dict = logsys.initial_metric_dict(accu_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e0e78",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### speed test check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38a2f20",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52085fc0",
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 ms ± 829 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = [torch.randn(4,70,32,64).cuda().half(),\n",
    "         torch.randn(4,70,32,64).cuda().half(),\n",
    "         torch.randn(4,70,32,64).cuda().half()]\n",
    "model.activate_stamps     = [[1, 2], [2]]\n",
    "model.activate_error_coef = [[0, 1, 1, 1.5, 'quantity'],\n",
    "                             [0, 2, 2, 1.5, 'quantity'],\n",
    "                             [1, 2, 2, 3.0, 'quantity']]\n",
    "model.consistancy_alpha   = None\n",
    "model.train()\n",
    "logsys.train()\n",
    "with torch.cuda.amp.autocast(enabled=model.use_amp):\n",
    "    loss, abs_loss, iter_info_pool,ltmv_pred,target  =run_one_iter(model, batch, criterion, 'train', gpu, train_dataset)\n",
    "loss_scaler.scale(loss).backward()   \n",
    "loss_scaler.step(optimizer)\n",
    "loss_scaler.update()   \n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4c4346a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 ms ± 683 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = [torch.randn(4,70,32,64).cuda().half(),\n",
    "         torch.randn(4,70,32,64).cuda().half(),\n",
    "         torch.randn(4,70,32,64).cuda().half()]\n",
    "model.activate_stamps     = None\n",
    "model.activate_error_coef = None\n",
    "model.consistancy_alpha   = [1,0,0]\n",
    "model.train()\n",
    "logsys.train()\n",
    "with torch.cuda.amp.autocast(enabled=model.use_amp):\n",
    "    loss, abs_loss, iter_info_pool,ltmv_pred,target  =run_one_iter(model, batch, criterion, 'train', gpu, train_dataset)\n",
    "loss_scaler.scale(loss).backward()   \n",
    "loss_scaler.step(optimizer)\n",
    "loss_scaler.update()   \n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1a9c8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befbe72f",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58deec26",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AFNONet' object has no attribute 'clim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# we will also record the variance for each slot, assume the input is a time series of data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# [ (B, P, W, H) -> (B, P, W, H) -> .... -> (B, P, W, H) ,(B, P, W, H)]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# we would record the variance of each batch on the location (W,H)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m error_information \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m clim \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclim\u001b[49m\n\u001b[1;32m     16\u001b[0m start \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m:model\u001b[38;5;241m.\u001b[39mhistory_length] \u001b[38;5;66;03m# start must be a list    \u001b[39;00m\n\u001b[1;32m     18\u001b[0m snap_line \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1504\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1503\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1504\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AFNONet' object has no attribute 'clim'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    accu_series=[]\n",
    "    rmse_series=[]\n",
    "    rmse_maps = []\n",
    "    hmse_series=[]\n",
    "    extra_info = {}\n",
    "    time_step_1_mode=False\n",
    "    batch_variance_line_pred = [] \n",
    "    batch_variance_line_true = []\n",
    "    # we will also record the variance for each slot, assume the input is a time series of data\n",
    "    # [ (B, P, W, H) -> (B, P, W, H) -> .... -> (B, P, W, H) ,(B, P, W, H)]\n",
    "    # we would record the variance of each batch on the location (W,H)\n",
    "    error_information = None\n",
    "    clim = model.clim\n",
    "    \n",
    "    start = batch[0:model.history_length] # start must be a list    \n",
    "    \n",
    "    snap_line = []\n",
    "    if (snap_index is not None) and (0 not in [len(t) for t in snap_index]):  \n",
    "        for i,tensor in enumerate(start):\n",
    "            # each tensor is like (B, 70, 32, 64) or (B, P, Z, W, H)\n",
    "            snap_line.append([len(snap_line), get_tensor_value(tensor,snap_index, time=model.history_length),'input'])\n",
    "    \n",
    "    # approx_epsilon_lists = []\n",
    "    # last_pred = last_target = None\n",
    "    i = model.history_length\n",
    "    while i<len(batch):#for i in range(model.history_length,len(batch), model.pred_len):# i now is the target index\n",
    "        if do_error_propagration_monitor and i < do_error_propagration_monitor:\n",
    "            # in this mode, we will concat all batch to accelarate computing. so far, only support batch=[X_{t},X_{t+1},X_{t+2}]\n",
    "            monitor_batch = torch.stack(batch[:do_error_propagration_monitor],1) #[X_{t},X_{t+1},...,X_{t+2}]-> (B,T,P,W,H)\n",
    "            ltmv_preds, ltmv_trues,error_information = once_forward_error_evaluation(model,monitor_batch)\n",
    "            i = do_error_propagration_monitor-1\n",
    "            start = [ltmv_preds[:,-1]]\n",
    "            time_list  = range(1,do_error_propagration_monitor)\n",
    "            ltmv_trues = ltmv_trues.transpose(0,1) #(B,T,P,W,H) -> (T,B,P,W,H)\n",
    "            ltmv_preds = ltmv_preds.transpose(0,1) #(B,T,P,W,H) -> (T,B,P,W,H)\n",
    "        else:\n",
    "            end = batch[i:i+model.pred_len]\n",
    "            end = end[0] if len(end) == 1 else end\n",
    "            print(start[-1][0].mean(0))\n",
    "            ltmv_pred, target, extra_loss, extra_info_from_model_list, start = once_forward(model,i,start,end,dataset,time_step_1_mode)\n",
    "            print(ltmv_pred[0].mean(0))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a71fcc46",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "==================\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    UVTPHSC = batch[0]\n",
    "    with torch.cuda.amp.autocast(enabled=model.use_amp):\n",
    "        p    = model.UVTPHSC2p(UVTPHSC)\n",
    "        x    = torch.cat([UVTPHSC,p],1)\n",
    "        print(x.isnan().any())\n",
    "        \n",
    "        shape = x.shape\n",
    "        #print(x.shape)\n",
    "        # argue the w resolution.\n",
    "        pad = model.UVTPHSCp2uvth.backbone.get_w_resolution_pad(shape)\n",
    "        if pad is not None:x = F.pad(x.flatten(0,1),(0,0,pad,pad),mode='replicate').reshape(*shape[:-2],-1,shape[-1])\n",
    "        #print(x.shape)\n",
    "        B = x.shape[0]\n",
    "        ot_shape = x.shape[2:]\n",
    "        x = x.reshape(B,-1,*model.UVTPHSCp2uvth.backbone.img_size)# (B, p, z, h, w) or (B, p, h, w)\n",
    "        #timer.restart(level=0)\n",
    "        #print(torch.std_mean(x))\n",
    "        print(x.isnan().any())\n",
    "        \n",
    "        #x = model.UVTPHSCp2uvth.backbone.forward_features(x);#print(torch.std_mean(x))\n",
    "        x = model.UVTPHSCp2uvth.backbone.patch_embed(x)\n",
    "        print(x.isnan().any())\n",
    "        x += model.UVTPHSCp2uvth.backbone.pos_embed\n",
    "        print(x.isnan().any())\n",
    "        x = model.UVTPHSCp2uvth.backbone.pos_drop(x)\n",
    "        print(x.isnan().any())\n",
    "        #print(torch.std_mean(x))\n",
    "        print(\"==================\")\n",
    "        if not model.UVTPHSCp2uvth.backbone.checkpoint_activations:\n",
    "            for num,blk in enumerate(model.UVTPHSCp2uvth.backbone.blocks):\n",
    "                x = blk(x);#print(torch.std_mean(x))\n",
    "                print(x.isnan().any())\n",
    "                if num==2:break\n",
    "        else:\n",
    "            x = checkpoint_sequential(model.UVTPHSCp2uvth.backbone.blocks, 4, x)\n",
    "#         print(\"==================\")\n",
    "#         x = model.UVTPHSCp2uvth.backbone.norm(x).transpose(1, 2);\n",
    "#         print(x.isnan().any())\n",
    "#         x = torch.reshape(x, [-1, model.UVTPHSCp2uvth.backbone.embed_dim, *model.UVTPHSCp2uvth.backbone.final_shape])\n",
    "#         print(x.isnan().any())\n",
    "        #timer.record('forward_features',level=0)\n",
    "#         x = model.UVTPHSCp2uvth.backbone.final_dropout(x)\n",
    "        \n",
    "        \n",
    "#         #timer.record('final_dropout',level=0)\n",
    "#         x = model.UVTPHSCp2uvth.backbone.pre_logits(x);#print(torch.std_mean(x))\n",
    "        \n",
    "#         print(x.isnan().any())\n",
    "#         #timer.record('pre_logits',level=0)\n",
    "#         x = model.UVTPHSCp2uvth.backbone.head(x)  # print(torch.std_mean(x))\n",
    "        \n",
    "#         print(x.isnan().any())\n",
    "#         if model.UVTPHSCp2uvth.backbone.history_length >1:\n",
    "#             x = x.flatten(1,2).transpose(1,-1)\n",
    "#             x = model.UVTPHSCp2uvth.backbone.last_Linear_layer(x)\n",
    "#             x = x.transpose(1,-1)\n",
    "#             ot_shape=ot_shape[1:]\n",
    "#         #timer.record('head',level=0)\n",
    "#         x = x.reshape(B,-1,*ot_shape)\n",
    "#         if pad is not None:\n",
    "#             x = x[...,pad:-pad,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7b24f",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d256ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args.dataset_kargs['cross_sample']=args.cross_sample=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f2ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['batch_size']=args.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145f9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['cross_sample']=args.cross_sample=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "963beb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['use_offline_data']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831b3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['patch_range']=(3,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9282be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.debug  = 1\n",
    "args.dataset_kargs['batch_size'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "959b737c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use offline data mode <2>: train/valid/test use offline data\n",
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test_2D70N.npy\n",
      "use offline data mode <2>: train/valid/test use offline data\n",
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test_2D70N.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 20:28:56,408 use dataset ==> WeathBench7066\n",
      "2023-02-13 20:28:56,409 Start training for 100 epochs\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, train_dataloader,val_dataloader = get_train_and_valid_dataset(args,\n",
    "               train_dataset_tensor=None,train_record_load=None,\n",
    "               valid_dataset_tensor=None,valid_record_load=None)\n",
    "logsys.info(f\"use dataset ==> {train_dataset.__class__.__name__}\")\n",
    "logsys.info(f\"Start training for {args.epochs} epochs\")\n",
    "master_bar = logsys.create_master_bar(args.epochs)\n",
    "accu_list = ['valid_loss']\n",
    "metric_dict = logsys.initial_metric_dict(accu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f524f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['cross_sample']=args.cross_sample=1\n",
    "train_dataset.cross_sample  =1\n",
    "train_dataloader  = torch.utils.data.DataLoader(train_dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d02eb672",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.cross_sample  =0\n",
    "train_dataloader  = torch.utils.data.DataLoader(train_dataset,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6872d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset.cross_sample  =0 \n",
    "# val_dataloader = torch.utils.data.DataLoader(val_dataset,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8879877",
   "metadata": {
    "code_folding": [
     4,
     7,
     10
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load everything, start_training......\r"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "start_step = 0\n",
    "data_loader = train_dataloader\n",
    "status = 'train'\n",
    "if status == 'train':\n",
    "    model.train()\n",
    "    logsys.train()\n",
    "elif status == 'valid':\n",
    "    model.eval()\n",
    "    logsys.eval()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "accumulation_steps = model.accumulation_steps # should be 16 for finetune. but I think its ok.\n",
    "half_model = next(model.parameters()).dtype == torch.float16\n",
    "\n",
    "data_cost  = []\n",
    "train_cost = []\n",
    "rest_cost  = []\n",
    "now = time.time()\n",
    "\n",
    "Fethcher   = RandomSelectPatchFetcher if( status =='train' and \\\n",
    "                                          data_loader.dataset.use_offline_data and \\\n",
    "                                          data_loader.dataset.split=='train' and \\\n",
    "                                          'Patch' in data_loader.dataset.__class__.__name__) else Datafetcher\n",
    "device     = next(model.parameters()).device\n",
    "prefetcher = Fethcher(data_loader,device)\n",
    "#raise\n",
    "batches    = len(data_loader)\n",
    "\n",
    "inter_b    = logsys.create_progress_bar(batches,unit=' img',unit_scale=data_loader.batch_size)\n",
    "gpu        = dist.get_rank() if hasattr(model,'module') else 0\n",
    "\n",
    "if start_step == 0:optimizer.zero_grad()\n",
    "intervel = batches//100 + 1\n",
    "\n",
    "\n",
    "total_diff,total_num  = torch.Tensor([0]).to(device), torch.Tensor([0]).to(device)\n",
    "nan_count = 0\n",
    "Nodeloss1 = Nodeloss2 = Nodeloss12 = -1\n",
    "\n",
    "inter_b.lwrite(f\"load everything, start_{status}ing......\", end=\"\\r\")\n",
    "preds = []\n",
    "reals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31f0155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.cross_sample  =0\n",
    "train_dataloader  = torch.utils.data.DataLoader(train_dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e47fe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.cross_sample  = 1\n",
    "val_dataloader  = torch.utils.data.DataLoader(val_dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30a04216",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ec29f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4389189d",
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     20
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load everything, start_validing......\r"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "start_step = 0\n",
    "data_loader = train_dataloader\n",
    "status = 'valid'\n",
    "if status == 'train':\n",
    "    model.train()\n",
    "    logsys.train()\n",
    "elif status == 'valid':\n",
    "    model.eval()\n",
    "    logsys.eval()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "accumulation_steps = model.accumulation_steps # should be 16 for finetune. but I think its ok.\n",
    "half_model = next(model.parameters()).dtype == torch.float16\n",
    "\n",
    "data_cost  = []\n",
    "train_cost = []\n",
    "rest_cost  = []\n",
    "now = time.time()\n",
    "\n",
    "Fethcher   = RandomSelectPatchFetcher if( status =='train' and \\\n",
    "                                          data_loader.dataset.use_offline_data and \\\n",
    "                                          data_loader.dataset.split=='train' and \\\n",
    "                                          'Patch' in data_loader.dataset.__class__.__name__) else Datafetcher\n",
    "device     = next(model.parameters()).device\n",
    "prefetcher = Fethcher(data_loader,device)\n",
    "#raise\n",
    "batches    = len(data_loader)\n",
    "\n",
    "inter_b    = logsys.create_progress_bar(batches,unit=' img',unit_scale=data_loader.batch_size)\n",
    "gpu        = dist.get_rank() if hasattr(model,'module') else 0\n",
    "\n",
    "if start_step == 0:optimizer.zero_grad()\n",
    "intervel = batches//100 + 1\n",
    "\n",
    "\n",
    "total_diff,total_num  = torch.Tensor([0]).to(device), torch.Tensor([0]).to(device)\n",
    "nan_count = 0\n",
    "Nodeloss1 = Nodeloss2 = Nodeloss12 = -1\n",
    "\n",
    "inter_b.lwrite(f\"load everything, start_{status}ing......\", end=\"\\r\")\n",
    "preds = []\n",
    "reals = []\n",
    "while inter_b.update_step():\n",
    "    #if inter_b.now>10:break\n",
    "    step = inter_b.now\n",
    "    batch = prefetcher.next()\n",
    "    #print(batch[0].shape)\n",
    "    #raise\n",
    "    if step < start_step:continue\n",
    "    #batch = data_loader.dataset.do_normlize_data(batch)\n",
    "\n",
    "    batch = make_data_regular(batch,half_model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25dd4f83",
   "metadata": {
    "code_folding": [
     44,
     51,
     55,
     66,
     90,
     98,
     110,
     140,
     142,
     148,
     154,
     160,
     166,
     179,
     191
    ]
   },
   "outputs": [],
   "source": [
    "def run_one_iter_highlevel_fast(model, batch, criterion, status, gpu, dataset):\n",
    "    assert model.history_length == 1\n",
    "    assert model.pred_len == 1\n",
    "    assert len(batch)>1\n",
    "    assert len(batch) <= len(model.activate_stamps) + 1\n",
    "    iter_info_pool={}\n",
    "    \n",
    "    if model.history_length > len(batch):\n",
    "        print(f\"you want to use history={model.history_length}\")\n",
    "        print(f\"but your input batch(timesteps) only has len(batch)={len(batch)}\")\n",
    "        raise\n",
    "    now_level_batch = torch.stack(batch,1) #[(B,P,W,H),(B,P,W,H),...,(B,P,W,H)] -> (B,L,P,W,H)\n",
    "    # input is a tenosor (B,L,P,W,H)\n",
    "    # The generated intermediate is recorded as \n",
    "    # X0 x1 y2 z3\n",
    "    # X1 x2 y3 z4\n",
    "    # X2 x3 y4\n",
    "    # X3 x4\n",
    "    B,L = now_level_batch.shape[:2]\n",
    "    tshp= now_level_batch.shape[2:]\n",
    "    all_level_batch = [now_level_batch]\n",
    "    all_level_record= [list(range(L))] #[0,1,2,3]]\n",
    "    ####################################################\n",
    "    # we will do once forward at begin to gain \n",
    "    # X0 X1 X2 X3\n",
    "    # |  |  |  |\n",
    "    # x1 x2 x3 x4\n",
    "    # |  |  |\n",
    "    # y2 y3 y4\n",
    "    # |  |\n",
    "    # z3 z4\n",
    "    ### the problem is we may cut some path by feeding an extra option.\n",
    "    ### for example, we may achieve a computing graph as\n",
    "    # X0 X1 X2 X3\n",
    "    # |  |  |  \n",
    "    # x1 x2 x3 \n",
    "    # |   \n",
    "    # y2 \n",
    "    # |  \n",
    "    # z3 \n",
    "    # so we need a flag \n",
    "    ####################################################\n",
    "    train_channel_from_this_stamp,train_channel_from_next_stamp,pred_channel_for_next_stamp = feature_pick_check(model)\n",
    "\n",
    "    for i in range(len(model.activate_stamps)): # generate L , L-1, L-2\n",
    "        # the least coding here\n",
    "        # now_level_batch = model(now_level_batch[:,:(L-i)].flatten(0,1)).reshape(B,(L-i),*tshp)  \n",
    "        # all_level_batch.append(now_level_batch)\n",
    "        activate_stamp      = model.activate_stamps[i]\n",
    "        last_activate_stamp = all_level_record[-1]\n",
    "        picked_stamp = []\n",
    "        for t in activate_stamp:\n",
    "            picked_stamp.append(last_activate_stamp.index(t-1)) # if t-1 not in last_activate_stamp, raise Error\n",
    "        start = [now_level_batch[:,picked_stamp].flatten(0,1)]\n",
    "\n",
    "        if pred_channel_for_next_stamp or train_channel_from_next_stamp:\n",
    "            if pred_channel_for_next_stamp  : assert t<=L # save key when prediction need last stamp information\n",
    "            if train_channel_from_next_stamp: assert t< L           \n",
    "            target_stamp = []\n",
    "            for t in activate_stamp:\n",
    "                target_stamp.append(last_activate_stamp.index(t) if t   in last_activate_stamp  else last_activate_stamp.index(t-1))\n",
    "                # if the target stamp not appear in this batch, use current stamp fill, but we need prohibit this prediction \n",
    "                # to do next forward prediction. Thus we limit in t < L \n",
    "            # notice when activate pred_channel_for_next_stamp, the unpredicted part should be filled by the part from next stamp \n",
    "            # but the loss should be calculate only on the predicted part.\n",
    "            end = now_level_batch[:,target_stamp].flatten(0,1)\n",
    "        else:\n",
    "            end = None\n",
    "        _, _, _, _, start    = once_forward_normal(model,i,start,end,dataset,False)\n",
    "        now_level_batch      = start[-1].reshape(B,len(picked_stamp),*tshp)  \n",
    "        #now_level_batch     = model(now_level_batch[:,picked_stamp].flatten(0,1)).reshape(B,len(picked_stamp),*tshp)  \n",
    "        all_level_batch.append(now_level_batch)\n",
    "        #all_level_record.append([last_activate_stamp[t]+1 for t in picked_stamp])\n",
    "        all_level_record.append(activate_stamp)\n",
    "\n",
    "    ####################################################\n",
    "    ################ calculate error ###################\n",
    "    iter_info_pool={}\n",
    "    loss = 0\n",
    "    diff = 0\n",
    "    loss_count = diff_count = len(model.activate_error_coef)\n",
    "    for (level_1, level_2, stamp, coef,_type) in model.activate_error_coef:\n",
    "        tensor1 = all_level_batch[level_1][:,all_level_record[level_1].index(stamp)]\n",
    "        tensor2 = all_level_batch[level_2][:,all_level_record[level_2].index(stamp)]\n",
    "        if 'quantity' in _type:\n",
    "            if _type == 'quantity':\n",
    "                error   = criterion(tensor1,tensor2)\n",
    "            elif _type == 'quantity_log':\n",
    "                error   = ((tensor1-tensor2)**2+1).log().mean()\n",
    "            else:raise NotImplementedError\n",
    "        elif 'alpha' in _type:\n",
    "            last_tensor1 = all_level_batch[level_1-1][:,all_level_record[level_1-1].index(stamp-1)]\n",
    "            last_tensor2 = all_level_batch[level_2-1][:,all_level_record[level_2-1].index(stamp-1)]\n",
    "            if _type == 'alpha':\n",
    "                error   = torch.mean(  ((tensor1-tensor2)**2) / ((last_tensor1-last_tensor2)**2+1e-4)  )\n",
    "            elif _type == 'alpha_log':\n",
    "                error   = torch.mean(  ((tensor1-tensor2)**2+1).log() - ((last_tensor1-last_tensor2)**2+1).log() )\n",
    "            else:raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        iter_info_pool[f\"{status}_error_{level_1}_{level_2}_{stamp}\"] = error.item()\n",
    "        loss   += coef*error\n",
    "        if level_1 ==0 and (level_2 == stamp):# to be same as normal train \n",
    "            diff += coef*error\n",
    "    loss = loss/loss_count\n",
    "    diff = diff/loss_count\n",
    "    return loss, diff, iter_info_pool, None, all_level_batch\n",
    "\n",
    "def run_one_iter_normal(model, batch, criterion, status, gpu, dataset):\n",
    "    iter_info_pool={}\n",
    "    loss = 0\n",
    "    diff = 0\n",
    "    random_run_step = np.random.randint(1,len(batch)) if len(batch)>1 else 0\n",
    "    time_step_1_mode=False\n",
    "    if len(batch) == 1 and isinstance(batch[0],(list,tuple)) and len(batch[0])>1:\n",
    "        batch = batch[0] # (Field, FieldDt)\n",
    "        time_step_1_mode=True\n",
    "    if model.history_length > len(batch):\n",
    "        print(f\"you want to use history={model.history_length}\")\n",
    "        print(f\"but your input batch(timesteps) only has len(batch)={len(batch)}\")\n",
    "        raise\n",
    "    pred_step = 0\n",
    "    start = batch[0:model.history_length] # start must be a list\n",
    "    full_fourcast_error_list = []\n",
    "    hidden_fourcast_list = [] \n",
    "    # length depend on pred_len time_step=2 --> 1+1\n",
    "    #                time_step=3 --> 1+2+2\n",
    "    #                time_step=4 --> 1+2+3\n",
    "    # use_consistancy_alpha to control activate amplitude\n",
    "    \n",
    "    save_list = []\n",
    "    for i in range(model.history_length,len(batch), model.pred_len):# i now is the target index\n",
    "        end = batch[i:i+model.pred_len]\n",
    "        end = end[0] if len(end) == 1 else end\n",
    "\n",
    "        ltmv_pred, target, extra_loss, extra_info_from_model_list, start = once_forward(model,i,start,end,dataset,time_step_1_mode)\n",
    "        \n",
    "            \n",
    "        if extra_loss !=0:\n",
    "            iter_info_pool[f'{status}_extra_loss_gpu{gpu}_timestep{i}'] = extra_loss.item()\n",
    "        for extra_info_from_model in extra_info_from_model_list:\n",
    "            for name, value in extra_info_from_model.items():\n",
    "                iter_info_pool[f'{status}_on_{status}_{name}_timestep{i}'] = value\n",
    "        \n",
    "        ltmv_pred = dataset.do_normlize_data([ltmv_pred])[0]\n",
    "\n",
    "        if 'Delta' in dataset.__class__.__name__:\n",
    "            loss  += criterion(ltmv_pred,target)+ extra_loss\n",
    "            with torch.no_grad():\n",
    "                normlized_field_predict = dataset.combine_base_delta(start[-1][0], start[-1][1]) \n",
    "                normlized_field_real    = dataset.combine_base_delta(      end[0],       end[1])  \n",
    "                abs_loss = criterion(normlized_field_predict,normlized_field_real)            \n",
    "        elif 'deseasonal' in dataset.__class__.__name__:\n",
    "            loss  += criterion(ltmv_pred,target)+ extra_loss\n",
    "            with torch.no_grad():\n",
    "                normlized_field_predict = dataset.addseasonal(start[-1][0], start[-1][1])\n",
    "                normlized_field_real    = dataset.addseasonal(end[0], end[1])\n",
    "                abs_loss = criterion(normlized_field_predict,normlized_field_real)\n",
    "        elif '68pixelnorm' in dataset.__class__.__name__:\n",
    "            loss  += criterion(ltmv_pred,target)+ extra_loss\n",
    "            with torch.no_grad():\n",
    "                normlized_field_predict = dataset.recovery(start[-1])\n",
    "                normlized_field_real    = dataset.recovery(end)\n",
    "                abs_loss = criterion(normlized_field_predict,normlized_field_real)\n",
    "        else:\n",
    "            normlized_field_predict = ltmv_pred\n",
    "            normlized_field_real = target\n",
    "            abs_loss = criterion[pred_step](ltmv_pred,target) if isinstance(criterion,(dict,list)) else criterion(ltmv_pred,target)\n",
    "            loss += abs_loss + extra_loss\n",
    "        \n",
    "        diff += abs_loss\n",
    "        print(abs_loss)\n",
    "        pred_step+=1\n",
    "        \n",
    "        if hasattr(model,\"consistancy_alpha\") and model.consistancy_alpha and loss < model.consistancy_activate_wall: \n",
    "            hidden_fourcast_list,full_fourcast_error_list,extra_loss2 = full_fourcast_forward(model,criterion,full_fourcast_error_list,ltmv_pred,target,hidden_fourcast_list)\n",
    "            save_list.append([ltmv_pred] + hidden_fourcast_list)\n",
    "            if hasattr(model,\"vertical_constrain\") and model.vertical_constrain and len(hidden_fourcast_list)>=2:\n",
    "                all_hidden_fourcast_list = [ltmv_pred]+hidden_fourcast_list\n",
    "                first_level_error_tensor = all_hidden_fourcast_list[-1] - all_hidden_fourcast_list[-2] #epsilon_2^I\n",
    "                for il in range(len(all_hidden_fourcast_list)-2):\n",
    "                    hidden_error_tensor = all_hidden_fourcast_list[-2] - all_hidden_fourcast_list[il]\n",
    "                    verticalQ = torch.mean((hidden_error_tensor*first_level_error_tensor)**2) # <epsilon_2^I|epsilon_2^II-epsilon_2^I>\n",
    "                    iter_info_pool[f'{status}_vertical_error_{i}_{il}_gpu{gpu}'] =  verticalQ.item()\n",
    "                    extra_loss2+= model.vertical_constrain*verticalQ # we only\n",
    "            if not model.consistancy_eval:loss+= extra_loss2\n",
    "            \n",
    "                \n",
    "        iter_info_pool[f'{status}_abs_loss_gpu{gpu}_timestep{i}'] =  abs_loss.item()\n",
    "        if status != \"train\":\n",
    "            iter_info_pool[f'{status}_accu_gpu{gpu}_timestep{i}']     =  compute_accu(normlized_field_predict,normlized_field_real).mean().item()\n",
    "            iter_info_pool[f'{status}_rmse_gpu{gpu}_timestep{i}']     =  compute_rmse(normlized_field_predict,normlized_field_real).mean().item()\n",
    "        if model.random_time_step_train and i >= random_run_step:\n",
    "            break\n",
    "    if hasattr(model,\"consistancy_alpha\") and model.consistancy_alpha and loss < model.consistancy_activate_wall:\n",
    "        ltmv_pred, target, extra_loss, extra_info_from_model_list, start = once_forward(model,i,start,None,dataset,time_step_1_mode) \n",
    "        ltmv_pred = dataset.do_normlize_data([ltmv_pred])[0]\n",
    "        hidden_fourcast_list,full_fourcast_error_list,extra_loss2 = full_fourcast_forward(model,criterion,full_fourcast_error_list,ltmv_pred,None,hidden_fourcast_list)\n",
    "        if not model.consistancy_eval:loss+= extra_loss2\n",
    "        for iii, val in enumerate(full_fourcast_error_list):\n",
    "            if val >0: iter_info_pool[f'{status}_full_fourcast_error_{iii}_gpu{gpu}'] =  val\n",
    "        save_list.append([ltmv_pred] + hidden_fourcast_list)\n",
    "    # loss = loss/(len(batch) - 1)\n",
    "    # diff = diff/(len(batch) - 1)\n",
    "    loss = loss/pred_step\n",
    "    diff = diff/pred_step\n",
    "    return loss, diff, iter_info_pool, ltmv_pred,save_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "914df35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.activate_stamps, model.activate_error_coef =([[1,2,3],[2,3],[3]], [[0,1,1, 2.5, \"quantity\"], \n",
    "                           [0,2,2, 2.5, \"quantity\"],\n",
    "                           [1,2,2, 2.5, \"quantity\"],\n",
    "                           [1,3,3, 2.5, \"quantity\"],\n",
    "                           [2,3,3, 2.5, \"quantity\"]\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78bd3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9006b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        loss1, diff1, iter_info_pool1, ltmv_pred1, save_list1 = run_one_iter_highlevel_fast(model, batch, criterion, status, gpu, train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f572c65",
   "metadata": {},
   "source": [
    "$(X_{t}),(X_{t+1}),(X_{t+2})$\n",
    "\n",
    "$f(X_{t}),f(X_{t+1}),f(X_{t+2})$\n",
    "\n",
    "$f(f(X_{t})),f(f(X_{t+1}))$\n",
    "\n",
    "$f(f(f(X_{t})))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83363008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0086, device='cuda:0')\n",
      "tensor(1.8527, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.consistancy_eval  = 0\n",
    "model.consistancy_alpha = [1,1,1]\n",
    "model.vertical_constrain= 0\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        loss2, diff2, iter_info_pool2, ltmv_pred2, save_list2 = run_one_iter_normal(model, batch, criterion, status, gpu, train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01d828cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.9307, device='cuda:0'), tensor(1.9307, device='cuda:0'))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff1,diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d31202cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.2378, device='cuda:0'), tensor(4.2378, device='cuda:0'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1,loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7476367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(len(t)) for t in save_list2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec99bf",
   "metadata": {},
   "source": [
    "None --> None | $f(X_{t}), X_{t+1}$\n",
    "\n",
    "f(f(X_{t})) --> $f(f(X_{t})),f(X_{t+1}),X_{t+2}$\n",
    "   \n",
    "f(f(f(X_{t}))) --> $f(f(f(X_{t}))),f(f(X_{t+1})),f(X_{t+2})| X_{t+3}=None$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc3e86",
   "metadata": {},
   "source": [
    "> 傅里叶变换罪大恶极， 12层的傅里叶变换直接带来精度误差的放大积累"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fdf16d97",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def process(model,x):\n",
    "#     shape = x.shape\n",
    "#     #print(x.shape)\n",
    "#     # argue the w resolution.\n",
    "#     pad = model.get_w_resolution_pad(shape)\n",
    "#     if pad is not None:\n",
    "#         x = F.pad(x.flatten(0,1),(0,0,pad,pad),mode='replicate').reshape(*shape[:-2],-1,shape[-1])\n",
    "#     #print(x.shape)\n",
    "#     B = x.shape[0]\n",
    "#     ot_shape = x.shape[2:]\n",
    "#     x = x.reshape(B,-1,*model.img_size)# (B, p, z, h, w) or (B, p, h, w)\n",
    "#     #timer.restart(level=0)\n",
    "#     #print(torch.std_mean(x))\n",
    "# ####    x = model.forward_features(x);#print(torch.std_mean(x))\n",
    "#     B = x.shape[0]\n",
    "#     x = model.patch_embed(x)\n",
    "#     x += model.pos_embed\n",
    "#     x = model.pos_drop(x)\n",
    "#     #print(torch.std_mean(x))\n",
    "#     for i,blk in enumerate(model.blocks):\n",
    "#         #x = blk(x);#print(torch.std_mean(x))\n",
    "#         x = blk.norm1(x)\n",
    "        \n",
    "#         #timer.record('norm1','forward_features',1)\n",
    "#         x = blk.filter(x)\n",
    "#         if i==0:break\n",
    "#         #timer.record('filter','forward_features',1)\n",
    "#         if blk.double_skip:\n",
    "#             x += residual\n",
    "#             residual = x;\n",
    "            \n",
    "#         #timer.record('residual','forward_features',1)\n",
    "#         x = blk.norm2(x)\n",
    "#         #timer.record('norm2','forward_features',1)\n",
    "#         x = blk.mlp(x)\n",
    "#         #timer.record('mlp','forward_features',1)\n",
    "#         x = blk.drop_path(x)\n",
    "#         #timer.record('drop_path','forward_features',1)\n",
    "        \n",
    "#         x += residual\n",
    "        \n",
    "# #    x = model.norm(x).transpose(1, 2);\n",
    "# #    x = torch.reshape(x, [-1, model.embed_dim, *model.final_shape])\n",
    "# #     #timer.record('forward_features',level=0)\n",
    "# #     x = model.final_dropout(x)\n",
    "# #     #timer.record('final_dropout',level=0)\n",
    "# #     x = model.pre_logits(x);#print(torch.std_mean(x))\n",
    "# #     #timer.record('pre_logits',level=0)\n",
    "# #     x = model.head(x)  # print(torch.std_mean(x))\n",
    "# #     if model.history_length >1:\n",
    "# #         x = x.flatten(1,2).transpose(1,-1)\n",
    "# #         x = model.last_Linear_layer(x)\n",
    "# #         x = x.transpose(1,-1)\n",
    "# #         ot_shape=ot_shape[1:]\n",
    "# #     #timer.record('head',level=0)\n",
    "# #     x = x.reshape(B,-1,*ot_shape)\n",
    "# #     if pad is not None:\n",
    "# #         x = x[...,pad:-pad,:]\n",
    "\n",
    "#     return x    \n",
    "\n",
    "# now_level_batch = torch.stack(batch,1)\n",
    "# with torch.no_grad():\n",
    "#     should1 = process(model,now_level_batch[:,0])\n",
    "#     should2 = process(model,now_level_batch[0])[0]\n",
    "#     print(torch.dist(should1, should2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c710c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0.0003, device='cuda:0')\n",
      "tensor(0.0002, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0.0004, device='cuda:0')\n",
      "tensor(0.0003, device='cuda:0')\n",
      "tensor(0.0002, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.dist(save_list2[0][0],save_list1[1][:,0]))\n",
    "print(torch.dist(save_list2[0][1],save_list1[0][:,1]))\n",
    "print(torch.dist(save_list2[1][0],save_list1[2][:,0]))\n",
    "print(torch.dist(save_list2[1][1],save_list1[1][:,1]))\n",
    "print(torch.dist(save_list2[1][2],save_list1[0][:,2]))\n",
    "print(torch.dist(save_list2[2][0],save_list1[3][:,0]))\n",
    "print(torch.dist(save_list2[2][1],save_list1[2][:,1]))\n",
    "print(torch.dist(save_list2[2][2],save_list1[1][:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b6952",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'master_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmaster_bar\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m start_epoch:\u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m):model\u001b[38;5;241m.\u001b[39mset_epoch(epoch\u001b[38;5;241m=\u001b[39mepoch,epoch_total\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'master_bar' is not defined"
     ]
    }
   ],
   "source": [
    "# for epoch in mastera_bar:\n",
    "#     if epoch < start_epoch:continue\n",
    "#     if hasattr(model,'set_epoch'):model.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "#     if hasattr(model,'module') and hasattr(model.module,'set_epoch'):model.module.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "#     logsys.record('learning rate',optimizer.param_groups[0]['lr'],epoch, epoch_flag='epoch')\n",
    "#     train_loss = run_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,logsys,'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tensor=None;\n",
    "train_record_load=None;\n",
    "valid_dataset_tensor=None;\n",
    "valid_record_load=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b458ea",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================> start training <==========================\n",
    "print(f\"entering {args.mode} training in {next(model.parameters()).device}\")\n",
    "now_best_path = SAVE_PATH / 'backbone.best.pt'\n",
    "latest_ckpt_p = SAVE_PATH / 'pretrain_latest.pt'\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, train_dataloader,val_dataloader = get_train_and_valid_dataset(args,\n",
    "               train_dataset_tensor=train_dataset_tensor,train_record_load=train_record_load,\n",
    "               valid_dataset_tensor=valid_dataset_tensor,valid_record_load=valid_record_load)\n",
    "logsys.info(f\"use dataset ==> {train_dataset.__class__.__name__}\")\n",
    "logsys.info(f\"Start training for {args.epochs} epochs\")\n",
    "metric_list = ['loss']\n",
    "master_bar        = logsys.create_master_bar(args.epochs)\n",
    "master_bar.set_multiply_graph(figsize=(9,3),engine=[['plot','plot']],labels=[metric_list])\n",
    "for epoch in master_bar:\n",
    "    if epoch < start_epoch:continue\n",
    "    if hasattr(model,'set_epoch'):model.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "    if hasattr(model,'module') and hasattr(model.module,'set_epoch'):model.module.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "    logsys.record('learning rate',optimizer.param_groups[0]['lr'],epoch)\n",
    "    train_loss = run_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,logsys,'train')\n",
    "    if (not args.more_epoch_train) and (lr_scheduler is not None):lr_scheduler.step(epoch)\n",
    "    #torch.cuda.empty_cache()\n",
    "    #train_loss = single_step_evaluate(train_dataloader, model, criterion,epoch,logsys,status='train') if 'small' in args.train_set else -1\n",
    "    val_loss   = run_one_epoch(epoch, start_step, model, criterion, val_dataloader, optimizer, loss_scaler,logsys,'valid')\n",
    "\n",
    "    if (not args.distributed) or (args.rank == 0 and local_rank == 0) :\n",
    "        logsys.info(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "        logsys.record('train', train_loss, epoch)\n",
    "        logsys.record('valid', val_loss, epoch)\n",
    "        if use_wandb:wandb.log({\"epoch\":epoch,'train':train_loss,'valid':val_loss})\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            if epoch > args.epochs//10:\n",
    "                logsys.info(f\"saving best model ....\")\n",
    "                save_model(model, path=now_best_path, only_model=True)\n",
    "                logsys.info(f\"done;\")\n",
    "            #if last_best_path is not None:os.system(f\"rm {last_best_path}\")\n",
    "            #last_best_path= now_best_path\n",
    "            logsys.info(f\"The best accu is {val_loss}\")\n",
    "        logsys.record('best_loss', min_loss, epoch)\n",
    "        update_experiment_info(experiment_hub_path,epoch,args)\n",
    "        if epoch>args.save_warm_up:\n",
    "            logsys.info(f\"saving latest model ....\")\n",
    "            save_model(model, epoch+1, 0, optimizer, lr_scheduler, loss_scaler, min_loss, latest_ckpt_p)\n",
    "            logsys.info(f\"done ....\")\n",
    "\n",
    "if os.path.exists(now_best_path) and args.do_final_fourcast:\n",
    "    logsys.info(f\"we finish training, then start test on the best checkpoint {now_best_path}\")\n",
    "    start_epoch, start_step, min_loss = load_model(model.module if args.distributed else model, path=now_best_path, only_model=True)\n",
    "    run_fourcast(args, model,logsys)\n",
    "if use_wandb:wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fbe32",
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "if local_rank == 0:\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "master_bar        = logsys.create_master_bar(args.epochs)\n",
    "last_best_path = None\n",
    "for epoch in master_bar:\n",
    "    if epoch < start_epoch:continue\n",
    "    train_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,lr_scheduler, min_loss,logsys)\n",
    "    lr_scheduler.step(epoch)\n",
    "    #torch.cuda.empty_cache()\n",
    "    train_loss = single_step_evaluate(train_dataloader, model, criterion,epoch,logsys)\n",
    "    #train_loss = -1\n",
    "    val_loss   = single_step_evaluate(val_dataloader, model, criterion,epoch,logsys)\n",
    "\n",
    "    if rank == 0 and local_rank == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "        logsys.record('train', train_loss, epoch)\n",
    "        logsys.record('valid', val_loss, epoch)\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            print(f\"saving best model ....\")\n",
    "            now_best_path = SAVE_PATH / f'backbone.best.pt'\n",
    "            if epoch>args.save_warm_up:save_model(model, path=now_best_path, only_model=True)\n",
    "            #if last_best_path is not None:os.system(f\"rm {last_best_path}\")\n",
    "            #last_best_path= now_best_path\n",
    "            print(f\"done; the best accu is {val_loss}\")\n",
    "        logsys.record('best_loss', min_loss, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c43ef",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    ngpus = ngpus_per_node = torch.cuda.device_count()\n",
    "    args.world_size = -1\n",
    "    args.dist_file  = None\n",
    "    args.rank       = 0\n",
    "    args.dist_backend = \"nccl\"\n",
    "    args.multiprocessing_distributed = ngpus>1\n",
    "    if not hasattr(args,'train_set'):args.train_set='large'\n",
    "    ip = os.environ.get(\"MASTER_ADDR\", \"127.0.0.1\")\n",
    "    port = os.environ.get(\"MASTER_PORT\", \"54247\")\n",
    "    hosts = int(os.environ.get(\"WORLD_SIZE\", \"1\"))  # number of nodes\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))  # node id\n",
    "    gpus = torch.cuda.device_count()  # gpus per node\n",
    "    args.dist_url = f\"tcp://{ip}:{port}\"\n",
    "    if args.world_size == -1 and \"SLURM_NPROCS\" in os.environ:\n",
    "        args.world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "        args.rank       = int(os.environ[\"SLURM_PROCID\"])\n",
    "        jobid           = os.environ[\"SLURM_JOBID\"]\n",
    "\n",
    "        hostfile        = \"dist_url.\" + jobid  + \".txt\"\n",
    "        if args.dist_file is not None:\n",
    "            args.dist_url = \"file://{}.{}\".format(os.path.realpath(args.dist_file), jobid)\n",
    "        elif args.rank == 0:\n",
    "            import socket\n",
    "            ip = socket.gethostbyname(socket.gethostname())\n",
    "            port = find_free_port()\n",
    "            args.dist_url = \"tcp://{}:{}\".format(ip, port)\n",
    "            #with open(hostfile, \"w\") as f:f.write(args.dist_url)\n",
    "        else:\n",
    "            import os\n",
    "            import time\n",
    "            while not os.path.exists(hostfile):\n",
    "                time.sleep(1)\n",
    "            with open(hostfile, \"r\") as f:\n",
    "                args.dist_url = f.read()\n",
    "        print(\"dist-url:{} at PROCID {} / {}\".format(args.dist_url, args.rank, args.world_size))\n",
    "    else:\n",
    "        args.world_size = 1\n",
    "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "    train_dataset_tensor=valid_dataset_tensor=None\n",
    "\n",
    "    print(\"======== loading data ==========\")\n",
    "    if 'small' in args.train_set:\n",
    "        if not args.fourcast:\n",
    "            train_dataset_tensor = load_small_dataset_in_memory('train').share_memory_()\n",
    "            valid_dataset_tensor = load_small_dataset_in_memory('valid').share_memory_()\n",
    "        else:\n",
    "            train_dataset_tensor = load_small_dataset_in_memory('test').share_memory_()\n",
    "            valid_dataset_tensor = None\n",
    "    else:\n",
    "        if args.fourcast:\n",
    "            train_dataset_tensor = load_test_dataset_in_memory(years=[2018],root=\"/nvme/zhangtianning/datasets/ERA5\").share_memory_()\n",
    "            valid_dataset_tensor = None\n",
    "    print(\"=======done==========\")\n",
    "    print(train_dataset_tensor.shape)\n",
    "    if args.multiprocessing_distributed:\n",
    "        args.world_size = ngpus_per_node * args.world_size\n",
    "        torch.multiprocessing.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args,train_dataset_tensor,valid_dataset_tensor))\n",
    "    else:\n",
    "        main_worker(0, ngpus_per_node, args,train_dataset_tensor,valid_dataset_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b4e50d8356ba75a9636787f9051ee458aaf13e87df491415cc800c7b2b8a21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
