{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad518c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cephdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab07f72",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from train.pretrain import *\n",
    "from train.pretrain import get_args\n",
    "from mltool.universal_model_util import get_model_para_detail\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d8affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_11_05_03_43-seed_76545\"\n",
    "args=get_args(os.path.join(ckpt_path,\"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863485e8",
   "metadata": {
    "code_folding": [
     7,
     12
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 11:43:38,912 model args: img_size= (5, 5)\n",
      "2022-12-01 11:43:38,912 model args: patch_size= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log at debug\n",
      "wandb id: None\n",
      "wandb is off, the recorder list is  ['tensorboard'], we pass wandb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 11:43:39,458 use model ==> PatchWrapper\n",
      "2022-12-01 11:43:39,460 Rank: 0, Local_rank: 0 | Number of Parameters: 46439577, Number of Buffers: 0, Size of Model: 177.1529 MB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 2D patch model, the img_size will be force set (32,64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 11:43:41,070 use lr_scheduler:<timm.scheduler.cosine_lr.CosineLRScheduler object at 0x7fbd05ef8eb0>\n"
     ]
    }
   ],
   "source": [
    "args.use_wandb=0\n",
    "args.gpu = args.local_rank = gpu  = local_rank = 0\n",
    "##### parse args: dataset_kargs / model_kargs / train_kargs  ###########\n",
    "args= parse_default_args(args)\n",
    "SAVE_PATH = get_ckpt_path(args)\n",
    "SAVE_PATH = \"debug\"\n",
    "args.SAVE_PATH = str(SAVE_PATH)\n",
    "#args.pretrain_weight = os.path.join(args.SAVE_PATH,'pretrain_latest.pt')\n",
    "########## inital log ###################\n",
    "logsys = create_logsys(args,False)\n",
    "\n",
    "\n",
    "if args.distributed:\n",
    "    if args.dist_url == \"env://\" and args.rank == -1:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "    if args.multiprocessing_distributed:\n",
    "        # For multiprocessing distributed training, rank needs to be the\n",
    "        # global rank among all the processes\n",
    "        args.rank = args.rank * ngpus_per_node + local_rank\n",
    "    logsys.info(f\"start init_process_group,backend={args.dist_backend}, init_method={args.dist_url},world_size={args.world_size}, rank={args.rank}\")\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "model           = build_model(args)\n",
    "#param_groups    = timm.optim.optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "optimizer,lr_scheduler,criterion = build_optimizer(args,model)\n",
    "loss_scaler     = torch.cuda.amp.GradScaler(enabled=True)\n",
    "logsys.info(f'use lr_scheduler:{lr_scheduler}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c2aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = os.path.join(ckpt_path,\"pretrain_latest.pt\")\n",
    "args.pretrain_weight = pretrain_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287c52be",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 11:43:44,411 loading weight from checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_11_05_03_43-seed_76545/pretrain_latest.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_11_05_03_43-seed_76545/pretrain_latest.pt...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 11:43:44,872 done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model weight success...........\n",
      "loading optimizer weight success...........\n",
      "loading lr_scheduler weight success...........\n",
      "loading loss_scaler weight success...........\n",
      "loading model success...........\n"
     ]
    }
   ],
   "source": [
    "logsys.info(f\"loading weight from {args.pretrain_weight}\")\n",
    "start_epoch, start_step, min_loss = load_model(model.module if args.distributed else model, optimizer, lr_scheduler, loss_scaler, path=args.pretrain_weight, \n",
    "                    only_model= (args.mode=='fourcast') or (args.mode=='finetune' and not args.continue_train) ,loc = 'cuda:{}'.format(args.gpu))\n",
    "if args.more_epoch_train:\n",
    "    assert args.pretrain_weight\n",
    "    print(f\"detect more epoch training, we will do a copy processing for {args.pretrain_weight}\")\n",
    "    os.system(f'cp {args.pretrain_weight} {args.pretrain_weight}-epoch{start_epoch}')\n",
    "logsys.info(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d256ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args.dataset_kargs['cross_sample']=args.cross_sample=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['batch_size']=args.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['cross_sample']=args.cross_sample=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959b737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/train.npy\n",
      "notice we will use around_index(28, 64, 2, 5, 5) to patch data\n",
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/valid.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 11:44:06,858 use dataset ==> WeathBench7066PatchDataset\n",
      "2022-12-01 11:44:06,859 Start training for 320 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notice we will use around_index(28, 64, 2, 5, 5) to patch data\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, train_dataloader,val_dataloader = get_train_and_valid_dataset(args,\n",
    "               train_dataset_tensor=None,train_record_load=None,\n",
    "               valid_dataset_tensor=None,valid_record_load=None)\n",
    "logsys.info(f\"use dataset ==> {train_dataset.__class__.__name__}\")\n",
    "logsys.info(f\"Start training for {args.epochs} epochs\")\n",
    "master_bar = logsys.create_master_bar(args.epochs)\n",
    "accu_list = ['valid_loss']\n",
    "metric_dict = logsys.initial_metric_dict(accu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6bd7a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.arange(32*64).reshape(32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57df74f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 64, 2, 5, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.around_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9252bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataset.around_index[:,:,0]\n",
    "y = train_dataset.around_index[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f524f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_kargs['cross_sample']=args.cross_sample=1\n",
    "train_dataset.cross_sample  =1\n",
    "train_dataloader  = torch.utils.data.DataLoader(train_dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02eb672",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.cross_sample  =0\n",
    "train_dataloader  = torch.utils.data.DataLoader(train_dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6872d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset.cross_sample  =0 \n",
    "# val_dataloader = torch.utils.data.DataLoader(val_dataset,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5a892",
   "metadata": {
    "code_folding": [
     4,
     7,
     10
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load everything, start_validing......\r"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "start_step = 0\n",
    "data_loader = train_dataloader\n",
    "status = 'valid'\n",
    "if status == 'train':\n",
    "    model.train()\n",
    "    logsys.train()\n",
    "elif status == 'valid':\n",
    "    model.eval()\n",
    "    logsys.eval()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "accumulation_steps = model.accumulation_steps # should be 16 for finetune. but I think its ok.\n",
    "half_model = next(model.parameters()).dtype == torch.float16\n",
    "\n",
    "data_cost  = []\n",
    "train_cost = []\n",
    "rest_cost  = []\n",
    "now = time.time()\n",
    "\n",
    "Fethcher   = RandomSelectPatchFetcher if( status =='train' and \\\n",
    "                                          data_loader.dataset.use_offline_data and \\\n",
    "                                          data_loader.dataset.split=='train' and \\\n",
    "                                          'Patch' in data_loader.dataset.__class__.__name__) else Datafetcher\n",
    "device     = next(model.parameters()).device\n",
    "prefetcher = Fethcher(data_loader,device)\n",
    "#raise\n",
    "batches    = len(data_loader)\n",
    "\n",
    "inter_b    = logsys.create_progress_bar(batches,unit=' img',unit_scale=data_loader.batch_size)\n",
    "gpu        = dist.get_rank() if hasattr(model,'module') else 0\n",
    "\n",
    "if start_step == 0:optimizer.zero_grad()\n",
    "intervel = batches//100 + 1\n",
    "\n",
    "\n",
    "total_diff,total_num  = torch.Tensor([0]).to(device), torch.Tensor([0]).to(device)\n",
    "nan_count = 0\n",
    "Nodeloss1 = Nodeloss2 = Nodeloss12 = -1\n",
    "\n",
    "inter_b.lwrite(f\"load everything, start_{status}ing......\", end=\"\\r\")\n",
    "preds = []\n",
    "reals = []\n",
    "while inter_b.update_step():\n",
    "    #if inter_b.now>10:break\n",
    "    step = inter_b.now\n",
    "    batch = prefetcher.next()\n",
    "    #print(batch[0].shape)\n",
    "    #raise\n",
    "    if step < start_step:continue\n",
    "    #batch = data_loader.dataset.do_normlize_data(batch)\n",
    "\n",
    "    batch = make_data_regular(batch,half_model)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15624478",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "i = 1\n",
    "start = batch[0:model.history_length]\n",
    "end= batch[i]\n",
    "dataset = data_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3265a2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    time_stamp = None\n",
    "    pos = None\n",
    "    assert len(start)==1\n",
    "    if isinstance(start[-1],list):\n",
    "        assert len(start[-1])<=3 # only allow tensor + time_stamp + pos\n",
    "        tensor, time_stamp, pos = start[-1]\n",
    "        start = [tensor]\n",
    "\n",
    "    Field  = start[-1]\n",
    "    normlized_Field_list = dataset.do_normlize_data([start])[0]  #always use normlized input\n",
    "    normlized_Field    = normlized_Field_list[0] if len(normlized_Field_list)==1 else torch.stack(normlized_Field_list,2)\n",
    "\n",
    "\n",
    "    if time_stamp is not None or pos is not None:\n",
    "        target = dataset.do_normlize_data([end[0]])[0] #always use normlized target\n",
    "    else:\n",
    "        target = dataset.do_normlize_data([end])[0] #always use normlized target\n",
    "\n",
    "    if model.training and model.input_noise_std and i==1:\n",
    "        normlized_Field += torch.randn_like(normlized_Field)*model.input_noise_std\n",
    "\n",
    "    if time_stamp is not None or pos is not None :\n",
    "        out   = model(normlized_Field,time_stamp,pos)\n",
    "    else:\n",
    "        out   = model(normlized_Field)\n",
    "\n",
    "    extra_loss = 0\n",
    "    extra_info_from_model_list = []\n",
    "    if isinstance(out,(list,tuple)):\n",
    "        extra_loss                 = out[1]\n",
    "        extra_info_from_model_list = out[2:]\n",
    "        out = out[0]\n",
    "\n",
    "    ltmv_pred = dataset.inv_normlize_data([out])[0]\n",
    "\n",
    "    if isinstance(start[0],(list,tuple)):\n",
    "        start = start[1:]+[[ltmv_pred, 0 , end[-1]]]\n",
    "    else:\n",
    "        start     = start[1:] + [ltmv_pred]\n",
    "    #print(ltmv_pred.shape,torch.std_mean(ltmv_pred))\n",
    "    #print(target.shape,torch.std_mean(target))\n",
    "    get_center_index_depend_on = model.module.get_center_index_depend_on if hasattr(model,'module') else model.get_center_index_depend_on\n",
    "    if len(ltmv_pred.shape)>2: #(B,P,Z,H,W)\n",
    "        if ltmv_pred.shape!=target.shape: # (B,P,W,H) -> (B,P,W-4,H) mode\n",
    "            if len(target.shape) == 5:\n",
    "                img_shape = target.shape[-3:]\n",
    "                sld_shape = ltmv_pred.shape[-3:]\n",
    "                z_idx, h_idx, l_idx = get_center_index_depend_on(sld_shape, img_shape)[0]\n",
    "                target = target[...,z_idx, h_idx, l_idx]\n",
    "            elif len(target.shape) == 4:\n",
    "                img_shape = target.shape[-2:]\n",
    "                sld_shape = ltmv_pred.shape[-2:]\n",
    "                h_idx, l_idx = get_center_index_depend_on(sld_shape, img_shape)[0]\n",
    "                target = target[...,h_idx, l_idx]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            # (B,P,W,H) -> (B,P,W,H) mode\n",
    "            pass\n",
    "    else: #(B, P)\n",
    "        if len(target.shape) == 4:\n",
    "            B,P,W,H=target.shape\n",
    "            target = target[...,W//2,H//2]\n",
    "        elif len(target.shape) == 5:\n",
    "            B,P,Z,W,H=target.shape\n",
    "            target = target[...,Z//2,W//2,H//2]  \n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d08a77d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def run_one_iter(model, batch, criterion, status, gpu, dataset):\n",
    "    iter_info_pool={}\n",
    "    loss = 0\n",
    "    diff = 0\n",
    "    random_run_step = np.random.randint(1,len(batch)) if len(batch)>1 else 0\n",
    "    time_step_1_mode=False\n",
    "    if len(batch) == 1 and isinstance(batch[0],(list,tuple)) and len(batch[0])>1:\n",
    "        batch = batch[0] # (Field, FieldDt)\n",
    "        time_step_1_mode=True\n",
    "    if model.history_length > len(batch):\n",
    "        print(f\"you want to use history={model.history_length}\")\n",
    "        print(f\"but your input batch(timesteps) only has len(batch)={len(batch)}\")\n",
    "        raise\n",
    "    pred_step = 0\n",
    "    start = batch[0:model.history_length] # start must be a list\n",
    "    ltmv_pred_record = []\n",
    "    target_record = []\n",
    "    for i in range(model.history_length,len(batch)):# i now is the target index\n",
    "        ltmv_pred, target, extra_loss, extra_info_from_model_list, start = once_forward(model,i,start,batch[i],dataset,time_step_1_mode)\n",
    "        if extra_loss !=0:\n",
    "            iter_info_pool[f'{status}_extra_loss_gpu{gpu}_timestep{i}'] = extra_loss.item()\n",
    "        for extra_info_from_model in extra_info_from_model_list:\n",
    "            for name, value in extra_info_from_model.items():\n",
    "                iter_info_pool[f'valid_on_{status}_{name}_timestep{i}'] = value\n",
    "        \n",
    "        ltmv_pred = dataset.do_normlize_data([ltmv_pred])[0]\n",
    "        print(torch.mean((ltmv_pred - target)**2))\n",
    "        abs_loss = criterion(ltmv_pred,target)\n",
    "        print(abs_loss)\n",
    "        ltmv_pred_record.append(ltmv_pred)\n",
    "        target_record.append(target)\n",
    "        iter_info_pool[f'{status}_abs_loss_gpu{gpu}_timestep{i}'] =  abs_loss.item()\n",
    "        pred_step+=1\n",
    "        \n",
    "        loss += abs_loss + extra_loss\n",
    "        diff += abs_loss\n",
    "        if model.random_time_step_train and i >= random_run_step:\n",
    "            break\n",
    "    # loss = loss/(len(batch) - 1)\n",
    "    # diff = diff/(len(batch) - 1)\n",
    "    loss = loss/pred_step\n",
    "    diff = diff/pred_step\n",
    "    return loss, diff, iter_info_pool,torch.cat(ltmv_pred_record),torch.cat(target_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.cross_sample  =1\n",
    "train_dataloader  = torch.utils.data.DataLoader(train_dataset,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47fe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.cross_sample  =1\n",
    "val_dataloader  = torch.utils.data.DataLoader(val_dataset,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a04216",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4389189d",
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     20,
     43,
     82,
     98
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([256, 70, 5, 5]), torch.Size([256, 70, 5, 5])]\n",
      "tensor(0.3339, device='cuda:0')\n",
      "tensor(0.3339, device='cuda:0')\n",
      "[torch.Size([256, 70, 5, 5]), torch.Size([256, 70, 5, 5])].33 loss:0.33 cost:[Date]:7.0e-01 [Train]:5.7e-02 \n",
      "tensor(0.3240, device='cuda:0')\n",
      "tensor(0.3240, device='cuda:0')\n",
      "[torch.Size([256, 70, 5, 5]), torch.Size([256, 70, 5, 5])].32 loss:0.32 cost:[Date]:6.6e-01 [Train]:5.7e-02 \n",
      "tensor(0.3351, device='cuda:0')\n",
      "tensor(0.3351, device='cuda:0')\n",
      "[torch.Size([256, 70, 5, 5]), torch.Size([256, 70, 5, 5])].34 loss:0.34 cost:[Date]:6.0e-01 [Train]:5.7e-02 \n",
      "tensor(0.3363, device='cuda:0')\n",
      "tensor(0.3363, device='cuda:0')\n",
      "[torch.Size([256, 70, 5, 5]), torch.Size([256, 70, 5, 5])].34 loss:0.34 cost:[Date]:5.9e-01 [Train]:5.7e-02 \n",
      "tensor(0.3417, device='cuda:0')\n",
      "tensor(0.3417, device='cuda:0')\n",
      "[torch.Size([179, 70, 5, 5]), torch.Size([179, 70, 5, 5])].34 loss:0.34 cost:[Date]:5.9e-01 [Train]:5.7e-02 \n",
      "tensor(0.3168, device='cuda:0')\n",
      "tensor(0.3168, device='cuda:0')\n",
      "epoch:000 iter:[    6]/[6] [TimeLeng]:2 GPU:[0] abs_loss:0.32 loss:0.32 cost:[Date]:4.9e-01 [Train]:4.1e-02 \r"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "start_step = 0\n",
    "data_loader = val_dataloader\n",
    "status = 'valid'\n",
    "if status == 'train':\n",
    "    model.train()\n",
    "    logsys.train()\n",
    "elif status == 'valid':\n",
    "    model.eval()\n",
    "    logsys.eval()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "accumulation_steps = model.accumulation_steps # should be 16 for finetune. but I think its ok.\n",
    "half_model = next(model.parameters()).dtype == torch.float16\n",
    "\n",
    "data_cost  = []\n",
    "train_cost = []\n",
    "rest_cost  = []\n",
    "now = time.time()\n",
    "\n",
    "Fethcher   = RandomSelectPatchFetcher if( status =='train' and \\\n",
    "                                          data_loader.dataset.use_offline_data and \\\n",
    "                                          data_loader.dataset.split=='train' and \\\n",
    "                                          'Patch' in data_loader.dataset.__class__.__name__) else Datafetcher\n",
    "device     = next(model.parameters()).device\n",
    "prefetcher = Fethcher(data_loader,device)\n",
    "#raise\n",
    "batches    = len(data_loader)\n",
    "\n",
    "inter_b    = logsys.create_progress_bar(batches,unit=' img',unit_scale=data_loader.batch_size)\n",
    "gpu        = dist.get_rank() if hasattr(model,'module') else 0\n",
    "\n",
    "if start_step == 0:optimizer.zero_grad()\n",
    "intervel = batches//100 + 1\n",
    "\n",
    "\n",
    "total_diff,total_num  = torch.Tensor([0]).to(device), torch.Tensor([0]).to(device)\n",
    "nan_count = 0\n",
    "Nodeloss1 = Nodeloss2 = Nodeloss12 = -1\n",
    "\n",
    "inter_b.lwrite(f\"load everything, start_{status}ing......\", end=\"\\r\")\n",
    "preds = []\n",
    "reals = []\n",
    "while inter_b.update_step():\n",
    "    #if inter_b.now>10:break\n",
    "    step = inter_b.now\n",
    "    batch = prefetcher.next()\n",
    "    #print(batch[0].shape)\n",
    "    #raise\n",
    "    if step < start_step:continue\n",
    "    #batch = data_loader.dataset.do_normlize_data(batch)\n",
    "\n",
    "    batch = make_data_regular(batch,half_model)\n",
    "    \n",
    "    \n",
    "    #if len(batch)==1:batch = batch[0] # for Field -> Field_Dt dataset\n",
    "    data_cost.append(time.time() - now);now = time.time()\n",
    "    if status == 'train':\n",
    "        if hasattr(model,'set_step'):model.set_step(step=step,epoch=epoch)\n",
    "        if hasattr(model,'module') and hasattr(model.module,'set_step'):model.module.set_step(step=step,epoch=epoch)\n",
    "        if model.train_mode =='pretrain':\n",
    "            time_truncate = max(min(epoch//3,data_loader.dataset.time_step),2)\n",
    "            batch=batch[:model.history_length -1 + time_truncate]\n",
    "\n",
    "        # the normal initial method will cause numerial explore by using timestep > 4 senenrio.\n",
    "        if model.use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss, abs_loss, iter_info_pool =run_one_iter(model, batch, criterion, 'train', gpu, data_loader.dataset)\n",
    "        else:\n",
    "            loss, abs_loss, iter_info_pool =run_one_iter(model, batch, criterion, 'train', gpu, data_loader.dataset)\n",
    "        loss, nan_count, skip = nan_diagnose_weight(model,loss,nan_count,logsys)\n",
    "        if skip:continue\n",
    "        loss /= accumulation_steps\n",
    "\n",
    "\n",
    "\n",
    "        if model.use_amp:\n",
    "            loss_scaler.scale(loss).backward()\n",
    "\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        if optimizer.grad_modifier is not None:\n",
    "            #assert not model.use_amp\n",
    "            assert accumulation_steps == 1\n",
    "            if model.use_amp:\n",
    "                loss_scaler.unscale_(optimizer) # do unscaler here for right gradient modify like clip or norm\n",
    "            assert len(batch)==2 # we now only allow one \n",
    "            assert isinstance(batch[0],torch.Tensor)\n",
    "            optimizer.grad_modifier.backward(model, batch[0], batch[1], strict=False)\n",
    "            Nodeloss1, Nodeloss12, Nodeloss2 = optimizer.grad_modifier.inference(model, batch[0], batch[1], strict=False)\n",
    "\n",
    "        #GradientModifier().backward(model,x,y)\n",
    "        #nan_count, skip = nan_diagnose_grad(model,nan_count,logsys)\n",
    "        # if skip:\n",
    "        #     optimizer.zero_grad()\n",
    "        #     continue\n",
    "\n",
    "        if model.clip_grad:\n",
    "            if model.use_amp:\n",
    "                assert accumulation_steps == 1\n",
    "                loss_scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), model.clip_grad)\n",
    "\n",
    "        if (step+1) % accumulation_steps == 0:\n",
    "            if model.use_amp:\n",
    "\n",
    "                loss_scaler.step(optimizer)\n",
    "                loss_scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            print([t.shape for t in batch])\n",
    "            loss, abs_loss, iter_info_pool,pred,real =run_one_iter(model, batch, criterion, status, gpu, data_loader.dataset)\n",
    "            if optimizer.grad_modifier is not None:\n",
    "                Nodeloss1, Nodeloss12, Nodeloss2 = optimizer.grad_modifier.inference(model, batch[0], batch[1], strict=False)\n",
    "    iter_info_pool={}\n",
    "    iter_info_pool[f'{status}_loss_gpu{gpu}']     =  loss.item()\n",
    "    iter_info_pool[f'{status}_Nodeloss1_gpu{gpu}'] = Nodeloss1\n",
    "    iter_info_pool[f'{status}_Nodeloss12_gpu{gpu}'] = Nodeloss12\n",
    "    iter_info_pool[f'{status}_Nodeloss2_gpu{gpu}'] = Nodeloss2\n",
    "    total_diff  += abs_loss.item()\n",
    "    #total_num   += len(batch) - 1 #batch \n",
    "    total_num   += 1 \n",
    "\n",
    "    train_cost.append(time.time() - now);now = time.time()\n",
    "    time_step_now = len(batch)\n",
    "    outstring=(f\"epoch:{epoch:03d} iter:[{step:5d}]/[{len(data_loader)}] [TimeLeng]:{time_step_now:} GPU:[{gpu}] abs_loss:{abs_loss.item():.2f} loss:{loss.item():.2f} cost:[Date]:{np.mean(data_cost):.1e} [Train]:{np.mean(train_cost):.1e} \")\n",
    "    #print(data_loader.dataset.record_load_tensor.mean().item())\n",
    "    data_cost  = []\n",
    "    train_cost = []\n",
    "    rest_cost = []\n",
    "    inter_b.lwrite(outstring, end=\"\\r\")\n",
    "    preds.append(pred.detach().cpu())\n",
    "    reals.append(real.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b6952",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'master_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmaster_bar\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m start_epoch:\u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m):model\u001b[38;5;241m.\u001b[39mset_epoch(epoch\u001b[38;5;241m=\u001b[39mepoch,epoch_total\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'master_bar' is not defined"
     ]
    }
   ],
   "source": [
    "# for epoch in mastera_bar:\n",
    "#     if epoch < start_epoch:continue\n",
    "#     if hasattr(model,'set_epoch'):model.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "#     if hasattr(model,'module') and hasattr(model.module,'set_epoch'):model.module.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "#     logsys.record('learning rate',optimizer.param_groups[0]['lr'],epoch, epoch_flag='epoch')\n",
    "#     train_loss = run_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,logsys,'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tensor=None;\n",
    "train_record_load=None;\n",
    "valid_dataset_tensor=None;\n",
    "valid_record_load=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b458ea",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================> start training <==========================\n",
    "print(f\"entering {args.mode} training in {next(model.parameters()).device}\")\n",
    "now_best_path = SAVE_PATH / 'backbone.best.pt'\n",
    "latest_ckpt_p = SAVE_PATH / 'pretrain_latest.pt'\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, train_dataloader,val_dataloader = get_train_and_valid_dataset(args,\n",
    "               train_dataset_tensor=train_dataset_tensor,train_record_load=train_record_load,\n",
    "               valid_dataset_tensor=valid_dataset_tensor,valid_record_load=valid_record_load)\n",
    "logsys.info(f\"use dataset ==> {train_dataset.__class__.__name__}\")\n",
    "logsys.info(f\"Start training for {args.epochs} epochs\")\n",
    "metric_list = ['loss']\n",
    "master_bar        = logsys.create_master_bar(args.epochs)\n",
    "master_bar.set_multiply_graph(figsize=(9,3),engine=[['plot','plot']],labels=[metric_list])\n",
    "for epoch in master_bar:\n",
    "    if epoch < start_epoch:continue\n",
    "    if hasattr(model,'set_epoch'):model.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "    if hasattr(model,'module') and hasattr(model.module,'set_epoch'):model.module.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "    logsys.record('learning rate',optimizer.param_groups[0]['lr'],epoch)\n",
    "    train_loss = run_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,logsys,'train')\n",
    "    if (not args.more_epoch_train) and (lr_scheduler is not None):lr_scheduler.step(epoch)\n",
    "    #torch.cuda.empty_cache()\n",
    "    #train_loss = single_step_evaluate(train_dataloader, model, criterion,epoch,logsys,status='train') if 'small' in args.train_set else -1\n",
    "    val_loss   = run_one_epoch(epoch, start_step, model, criterion, val_dataloader, optimizer, loss_scaler,logsys,'valid')\n",
    "\n",
    "    if (not args.distributed) or (args.rank == 0 and local_rank == 0) :\n",
    "        logsys.info(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "        logsys.record('train', train_loss, epoch)\n",
    "        logsys.record('valid', val_loss, epoch)\n",
    "        if use_wandb:wandb.log({\"epoch\":epoch,'train':train_loss,'valid':val_loss})\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            if epoch > args.epochs//10:\n",
    "                logsys.info(f\"saving best model ....\")\n",
    "                save_model(model, path=now_best_path, only_model=True)\n",
    "                logsys.info(f\"done;\")\n",
    "            #if last_best_path is not None:os.system(f\"rm {last_best_path}\")\n",
    "            #last_best_path= now_best_path\n",
    "            logsys.info(f\"The best accu is {val_loss}\")\n",
    "        logsys.record('best_loss', min_loss, epoch)\n",
    "        update_experiment_info(experiment_hub_path,epoch,args)\n",
    "        if epoch>args.save_warm_up:\n",
    "            logsys.info(f\"saving latest model ....\")\n",
    "            save_model(model, epoch+1, 0, optimizer, lr_scheduler, loss_scaler, min_loss, latest_ckpt_p)\n",
    "            logsys.info(f\"done ....\")\n",
    "\n",
    "if os.path.exists(now_best_path) and args.do_final_fourcast:\n",
    "    logsys.info(f\"we finish training, then start test on the best checkpoint {now_best_path}\")\n",
    "    start_epoch, start_step, min_loss = load_model(model.module if args.distributed else model, path=now_best_path, only_model=True)\n",
    "    run_fourcast(args, model,logsys)\n",
    "if use_wandb:wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fbe32",
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "if local_rank == 0:\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "master_bar        = logsys.create_master_bar(args.epochs)\n",
    "last_best_path = None\n",
    "for epoch in master_bar:\n",
    "    if epoch < start_epoch:continue\n",
    "    train_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,lr_scheduler, min_loss,logsys)\n",
    "    lr_scheduler.step(epoch)\n",
    "    #torch.cuda.empty_cache()\n",
    "    train_loss = single_step_evaluate(train_dataloader, model, criterion,epoch,logsys)\n",
    "    #train_loss = -1\n",
    "    val_loss   = single_step_evaluate(val_dataloader, model, criterion,epoch,logsys)\n",
    "\n",
    "    if rank == 0 and local_rank == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "        logsys.record('train', train_loss, epoch)\n",
    "        logsys.record('valid', val_loss, epoch)\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            print(f\"saving best model ....\")\n",
    "            now_best_path = SAVE_PATH / f'backbone.best.pt'\n",
    "            if epoch>args.save_warm_up:save_model(model, path=now_best_path, only_model=True)\n",
    "            #if last_best_path is not None:os.system(f\"rm {last_best_path}\")\n",
    "            #last_best_path= now_best_path\n",
    "            print(f\"done; the best accu is {val_loss}\")\n",
    "        logsys.record('best_loss', min_loss, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c43ef",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    ngpus = ngpus_per_node = torch.cuda.device_count()\n",
    "    args.world_size = -1\n",
    "    args.dist_file  = None\n",
    "    args.rank       = 0\n",
    "    args.dist_backend = \"nccl\"\n",
    "    args.multiprocessing_distributed = ngpus>1\n",
    "    if not hasattr(args,'train_set'):args.train_set='large'\n",
    "    ip = os.environ.get(\"MASTER_ADDR\", \"127.0.0.1\")\n",
    "    port = os.environ.get(\"MASTER_PORT\", \"54247\")\n",
    "    hosts = int(os.environ.get(\"WORLD_SIZE\", \"1\"))  # number of nodes\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))  # node id\n",
    "    gpus = torch.cuda.device_count()  # gpus per node\n",
    "    args.dist_url = f\"tcp://{ip}:{port}\"\n",
    "    if args.world_size == -1 and \"SLURM_NPROCS\" in os.environ:\n",
    "        args.world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "        args.rank       = int(os.environ[\"SLURM_PROCID\"])\n",
    "        jobid           = os.environ[\"SLURM_JOBID\"]\n",
    "\n",
    "        hostfile        = \"dist_url.\" + jobid  + \".txt\"\n",
    "        if args.dist_file is not None:\n",
    "            args.dist_url = \"file://{}.{}\".format(os.path.realpath(args.dist_file), jobid)\n",
    "        elif args.rank == 0:\n",
    "            import socket\n",
    "            ip = socket.gethostbyname(socket.gethostname())\n",
    "            port = find_free_port()\n",
    "            args.dist_url = \"tcp://{}:{}\".format(ip, port)\n",
    "            #with open(hostfile, \"w\") as f:f.write(args.dist_url)\n",
    "        else:\n",
    "            import os\n",
    "            import time\n",
    "            while not os.path.exists(hostfile):\n",
    "                time.sleep(1)\n",
    "            with open(hostfile, \"r\") as f:\n",
    "                args.dist_url = f.read()\n",
    "        print(\"dist-url:{} at PROCID {} / {}\".format(args.dist_url, args.rank, args.world_size))\n",
    "    else:\n",
    "        args.world_size = 1\n",
    "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "    train_dataset_tensor=valid_dataset_tensor=None\n",
    "\n",
    "    print(\"======== loading data ==========\")\n",
    "    if 'small' in args.train_set:\n",
    "        if not args.fourcast:\n",
    "            train_dataset_tensor = load_small_dataset_in_memory('train').share_memory_()\n",
    "            valid_dataset_tensor = load_small_dataset_in_memory('valid').share_memory_()\n",
    "        else:\n",
    "            train_dataset_tensor = load_small_dataset_in_memory('test').share_memory_()\n",
    "            valid_dataset_tensor = None\n",
    "    else:\n",
    "        if args.fourcast:\n",
    "            train_dataset_tensor = load_test_dataset_in_memory(years=[2018],root=\"/nvme/zhangtianning/datasets/ERA5\").share_memory_()\n",
    "            valid_dataset_tensor = None\n",
    "    print(\"=======done==========\")\n",
    "    print(train_dataset_tensor.shape)\n",
    "    if args.multiprocessing_distributed:\n",
    "        args.world_size = ngpus_per_node * args.world_size\n",
    "        torch.multiprocessing.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args,train_dataset_tensor,valid_dataset_tensor))\n",
    "    else:\n",
    "        main_worker(0, ngpus_per_node, args,train_dataset_tensor,valid_dataset_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b4e50d8356ba75a9636787f9051ee458aaf13e87df491415cc800c7b2b8a21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
