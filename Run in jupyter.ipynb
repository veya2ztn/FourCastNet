{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab07f72",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from train.pretrain import *\n",
    "from train.pretrain import get_args\n",
    "from mltool.universal_model_util import get_model_para_detail\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d8affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87cfecb",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# #args.debug = True\n",
    "# local_rank = 0\n",
    "# args.train_set='2D7066N' \n",
    "# args.use_time_stamp=0\n",
    "# args.history_length=1\n",
    "# args.time_step=2\n",
    "# #args.model_type='NaiveConvModel2D'\n",
    "# #args.wrapper_model='Time_Projection_Model'\n",
    "# args.embed_dim=256*3\n",
    "# args.use_amp=0\n",
    "# args.model_depth=12\n",
    "# args.valid_batch_size=12\n",
    "# #args.n_heads=8\n",
    "# args.share_memory=False\n",
    "# args.input_channel= 70\n",
    "# args.output_channel= 70\n",
    "# args.unique_up_sample_channel = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0220b65f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "json_args= {\n",
    "    \"mode\": \"pretrain\",\n",
    "    \"train_set\": \"2D70N\",\n",
    "    \"batch_size\": 1024,\n",
    "    \"valid_batch_size\": 2,\n",
    "    \"epochs\": 1000,\n",
    "    \"save_warm_up\": 5,\n",
    "    \"more_epoch_train\": 0,\n",
    "    \"skip_first_valid\": 1,\n",
    "    \"valid_every_epoch\": 250,\n",
    "    \"save_every_epoch\": 100,\n",
    "    \"seed\": 2022,\n",
    "    \"input_noise_std\": 0.0,\n",
    "    \"do_final_fourcast\": 0,\n",
    "    \"debug\": 0,\n",
    "    \"distributed\": False,\n",
    "    \"rank\": 0,\n",
    "    \"share_memory\": 1,\n",
    "    \"continue_train\": 0,\n",
    "    \"accumulation_steps\": 4,\n",
    "    \"use_wandb\": \"wandb_runtime\",\n",
    "    \"GDMod_type\": \"off\",\n",
    "    \"GDMod_lambda1\": 1,\n",
    "    \"GDMod_lambda2\": 0,\n",
    "    \"model_type\": \"AFNONet\",\n",
    "    \"patch_size\": 1,\n",
    "    \"img_size\": [\n",
    "        5,\n",
    "        5\n",
    "    ],\n",
    "    \"modes\": \"17,33,6\",\n",
    "    \"mode_select\": \"normal\",\n",
    "    \"physics_num\": 4,\n",
    "    \"input_channel\": 70,\n",
    "    \"output_channel\": 70,\n",
    "    \"embed_dim\": 768,\n",
    "    \"model_depth\": 12,\n",
    "    \"history_length\": 1,\n",
    "    \"block_target_timestamp\": 0,\n",
    "    \"canonical_fft\": 1,\n",
    "    \"unique_up_sample_channel\": 70,\n",
    "    \"n_heads\": 8,\n",
    "    \"pred_len\": 1,\n",
    "    \"label_len\": 3,\n",
    "    \"use_amp\": True,\n",
    "    \"random_time_step\": False,\n",
    "    \"use_scalar_advection\": False,\n",
    "    \"fno_bias\": False,\n",
    "    \"fno_blocks\": 4,\n",
    "    \"fno_softshrink\": 0.0,\n",
    "    \"double_skip\": False,\n",
    "    \"tensorboard_dir\": None,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 12,\n",
    "    \"checkpoint_activations\": False,\n",
    "    \"autoresume\": False,\n",
    "    \"wrapper_model\": \"PatchWrapper\",\n",
    "    \"dataset_type\": \"WeathBench7066PatchDataset\",\n",
    "    \"dataset_flag\": \"\",\n",
    "    \"time_reverse_flag\": \"only_forward\",\n",
    "    \"time_intervel\": 1,\n",
    "    \"time_step\": 2,\n",
    "    \"data_root\": \"\",\n",
    "    \"use_time_stamp\": 0,\n",
    "    \"cross_sample\": 1,\n",
    "    \"use_inmemory_dataset\": 0,\n",
    "    \"random_dataset\": 1,\n",
    "    \"num_workers\": 2,\n",
    "    \"use_offline_data\": 1,\n",
    "    \"pretrain_weight\": \"\",\n",
    "    \"fourcast_randn_initial\": 0,\n",
    "    \"force_fourcast\": 0,\n",
    "    \"opt\": \"adamw\",\n",
    "    \"opt_eps\": 1e-08,\n",
    "    \"opt_betas\": None,\n",
    "    \"clip_grad\": 0,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"lr\": 0.0011841555126572746,\n",
    "    \"sched\": \"\",\n",
    "    \"lr_noise\": None,\n",
    "    \"lr_noise_pct\": 0.67,\n",
    "    \"lr_noise_std\": 1.0,\n",
    "    \"warmup_lr\": 1e-06,\n",
    "    \"min_lr\": 1e-05,\n",
    "    \"decay_epochs\": 30,\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"cooldown_epochs\": 10,\n",
    "    \"patience_epochs\": 10,\n",
    "    \"decay_rate\": 0.1,\n",
    "    \"config_file\": \"checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_14_22_22_24-seed_2022/config.json\",\n",
    "    \"hparam_dict\": {\n",
    "        \"lr\": 0.0011841555126572746\n",
    "    },\n",
    "    \"world_size\": 1,\n",
    "    \"dist_file\": None,\n",
    "    \"dist_backend\": \"nccl\",\n",
    "    \"multiprocessing_distributed\": False,\n",
    "    \"ngpus_per_node\": 1,\n",
    "    \"dist_url\": \"tcp://127.0.0.1:54247\",\n",
    "    \"gpu\": 0,\n",
    "    \"local_rank\": 0,\n",
    "    \"half_model\": False,\n",
    "    \"accumulation_steps_global\": 4,\n",
    "    \"dataset_kargs\": {\n",
    "        \"dataset_flag\": \"2D70N\",\n",
    "        \"root\": None,\n",
    "        \"mode\": \"pretrain\",\n",
    "        \"time_step\": 2,\n",
    "        \"check_data\": True,\n",
    "        \"time_reverse_flag\": \"only_forward\",\n",
    "        \"use_offline_data\": 1,\n",
    "        \"time_intervel\": 1,\n",
    "        \"cross_sample\": 1,\n",
    "        \"random_dataset\": 1,\n",
    "        \"img_size\": [\n",
    "            5,\n",
    "            5\n",
    "        ]\n",
    "    },\n",
    "    \"model_kargs\": {\n",
    "        \"img_size\": [\n",
    "            5,\n",
    "            5\n",
    "        ],\n",
    "        \"patch_size\": 1,\n",
    "        \"in_chans\": 70,\n",
    "        \"out_chans\": 70,\n",
    "        \"fno_blocks\": 4,\n",
    "        \"embed_dim\": 768,\n",
    "        \"depth\": 12,\n",
    "        \"debug_mode\": 0,\n",
    "        \"double_skip\": False,\n",
    "        \"fno_bias\": False,\n",
    "        \"fno_softshrink\": 0.0,\n",
    "        \"history_length\": 1,\n",
    "        \"reduce_Field_coef\": False,\n",
    "        \"modes\": [\n",
    "            17,\n",
    "            33,\n",
    "            6\n",
    "        ],\n",
    "        \"mode_select\": \"normal\",\n",
    "        \"physics_num\": 4,\n",
    "        \"pred_len\": 1,\n",
    "        \"n_heads\": 8,\n",
    "        \"label_len\": 3,\n",
    "        \"canonical_fft\": 1,\n",
    "        \"unique_up_sample_channel\": 70,\n",
    "        \"share_memory\": 1\n",
    "    },\n",
    "    \"trail_name\": \"11_17_17_43_20-seed_2022\",\n",
    "    \"SAVE_PATH\": \"checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_17_17_43_20-seed_2022\",\n",
    "    \"logsys\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555311a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in json_args.items():\n",
    "    if hasattr(args,key):\n",
    "        setattr(args,key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287c52be",
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 16:10:16,153 model args: img_size= (5, 5)\n",
      "2022-11-18 16:10:16,154 model args: patch_size= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log at checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_17_17_43_20-seed_2022\n",
      "mode                           ---> pretrain\n",
      "train_set                      ---> 2D70N\n",
      "batch_size                     ---> 1024\n",
      "valid_batch_size               ---> 2\n",
      "epochs                         ---> 1000\n",
      "save_warm_up                   ---> 5\n",
      "more_epoch_train               ---> 0\n",
      "skip_first_valid               ---> 1\n",
      "valid_every_epoch              ---> 250\n",
      "save_every_epoch               ---> 100\n",
      "seed                           ---> 2022\n",
      "input_noise_std                ---> 0.0\n",
      "do_final_fourcast              ---> 0\n",
      "debug                          ---> 0\n",
      "distributed                    ---> False\n",
      "rank                           ---> 0\n",
      "share_memory                   ---> 1\n",
      "continue_train                 ---> 0\n",
      "accumulation_steps             ---> 4\n",
      "use_wandb                      ---> 0\n",
      "GDMod_type                     ---> off\n",
      "GDMod_lambda1                  ---> 1\n",
      "GDMod_lambda2                  ---> 0\n",
      "model_type                     ---> AFNONet\n",
      "patch_size                     ---> 1\n",
      "img_size                       ---> (5, 5)\n",
      "modes                          ---> 17,33,6\n",
      "mode_select                    ---> normal\n",
      "physics_num                    ---> 4\n",
      "input_channel                  ---> 70\n",
      "output_channel                 ---> 70\n",
      "embed_dim                      ---> 768\n",
      "model_depth                    ---> 12\n",
      "history_length                 ---> 1\n",
      "block_target_timestamp         ---> 0\n",
      "canonical_fft                  ---> 1\n",
      "unique_up_sample_channel       ---> 70\n",
      "n_heads                        ---> 8\n",
      "pred_len                       ---> 1\n",
      "label_len                      ---> 3\n",
      "use_amp                        ---> True\n",
      "random_time_step               ---> False\n",
      "use_scalar_advection           ---> False\n",
      "fno_bias                       ---> False\n",
      "fno_blocks                     ---> 4\n",
      "fno_softshrink                 ---> 0.0\n",
      "double_skip                    ---> False\n",
      "tensorboard_dir                ---> None\n",
      "hidden_size                    ---> 256\n",
      "num_layers                     ---> 12\n",
      "checkpoint_activations         ---> False\n",
      "autoresume                     ---> False\n",
      "wrapper_model                  ---> PatchWrapper\n",
      "dataset_type                   ---> WeathBench7066PatchDataset\n",
      "dataset_flag                   ---> \n",
      "time_reverse_flag              ---> only_forward\n",
      "time_intervel                  ---> 1\n",
      "time_step                      ---> 2\n",
      "data_root                      ---> \n",
      "use_time_stamp                 ---> 0\n",
      "cross_sample                   ---> 1\n",
      "use_inmemory_dataset           ---> 0\n",
      "random_dataset                 ---> 1\n",
      "num_workers                    ---> 2\n",
      "use_offline_data               ---> 1\n",
      "pretrain_weight                ---> checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_17_17_43_20-seed_2022/pretrain_latest.pt\n",
      "fourcast_randn_initial         ---> 0\n",
      "force_fourcast                 ---> 0\n",
      "opt                            ---> adamw\n",
      "opt_eps                        ---> 1e-08\n",
      "opt_betas                      ---> None\n",
      "clip_grad                      ---> 0\n",
      "momentum                       ---> 0.9\n",
      "weight_decay                   ---> 0.05\n",
      "lr                             ---> 0.0011841555126572746\n",
      "sched                          ---> \n",
      "lr_noise                       ---> None\n",
      "lr_noise_pct                   ---> 0.67\n",
      "lr_noise_std                   ---> 1.0\n",
      "warmup_lr                      ---> 1e-06\n",
      "min_lr                         ---> 1e-05\n",
      "decay_epochs                   ---> 30\n",
      "warmup_epochs                  ---> 5\n",
      "cooldown_epochs                ---> 10\n",
      "patience_epochs                ---> 10\n",
      "decay_rate                     ---> 0.1\n",
      "config_file                    ---> checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_14_22_22_24-seed_2022/config.json\n",
      "gpu                            ---> 0\n",
      "local_rank                     ---> 0\n",
      "half_model                     ---> False\n",
      "dataset_kargs                  ---> {'dataset_flag': '2D70N', 'root': None, 'mode': 'pretrain', 'time_step': 2, 'check_data': True, 'time_reverse_flag': 'only_forward', 'use_offline_data': 1, 'time_intervel': 1, 'cross_sample': 1, 'random_dataset': 1, 'img_size': (5, 5)}\n",
      "model_kargs                    ---> {'img_size': (5, 5), 'patch_size': 1, 'in_chans': 70, 'out_chans': 70, 'fno_blocks': 4, 'embed_dim': 768, 'depth': 12, 'debug_mode': 0, 'double_skip': False, 'fno_bias': False, 'fno_softshrink': 0.0, 'history_length': 1, 'reduce_Field_coef': False, 'modes': (17, 33, 6), 'mode_select': 'normal', 'physics_num': 4, 'pred_len': 1, 'n_heads': 8, 'label_len': 3, 'canonical_fft': 1, 'unique_up_sample_channel': 70, 'share_memory': 1}\n",
      "trail_name                     ---> 11_18_16_10_16-seed_2022\n",
      "SAVE_PATH                      ---> checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_17_17_43_20-seed_2022\n",
      "logsys                         ---> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 16:10:16,570 use model ==> PatchWrapper\n",
      "2022-11-18 16:10:16,571 Rank: 0, Local_rank: 0 | Number of Parameters: 46439577, Number of Buffers: 0, Size of Model: 177.1529 MB\n",
      "\n",
      "2022-11-18 16:10:18,026 use lr_scheduler:None\n",
      "2022-11-18 16:10:18,027 loading weight from checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_17_17_43_20-seed_2022/pretrain_latest.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_17_17_43_20-seed_2022/pretrain_latest.pt...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 16:10:18,389 done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model weight success...........\n",
      "loading optimizer weight success...........\n",
      "loading lr_scheduler weight success...........\n",
      "loading loss_scaler weight success...........\n",
      "loading model success...........\n"
     ]
    }
   ],
   "source": [
    "args.use_wandb=0\n",
    "args.gpu = args.local_rank = gpu  = local_rank = 0\n",
    "##### parse args: dataset_kargs / model_kargs / train_kargs  ###########\n",
    "args= parse_default_args(args)\n",
    "SAVE_PATH = get_ckpt_path(args)\n",
    "SAVE_PATH = \"checkpoints/WeathBench7066PatchDataset/PatchWrapper-AFNONet/time_step_2_pretrain-2D70N_every_1_step_random_dataset/11_17_17_43_20-seed_2022\"\n",
    "args.SAVE_PATH = str(SAVE_PATH)\n",
    "args.pretrain_weight = os.path.join(args.SAVE_PATH,'pretrain_latest.pt')\n",
    "########## inital log ###################\n",
    "logsys = create_logsys(args)\n",
    "\n",
    "\n",
    "if args.distributed:\n",
    "    if args.dist_url == \"env://\" and args.rank == -1:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "    if args.multiprocessing_distributed:\n",
    "        # For multiprocessing distributed training, rank needs to be the\n",
    "        # global rank among all the processes\n",
    "        args.rank = args.rank * ngpus_per_node + local_rank\n",
    "    logsys.info(f\"start init_process_group,backend={args.dist_backend}, init_method={args.dist_url},world_size={args.world_size}, rank={args.rank}\")\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "model           = build_model(args)\n",
    "#param_groups    = timm.optim.optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "optimizer,lr_scheduler,criterion = build_optimizer(args,model)\n",
    "loss_scaler     = torch.cuda.amp.GradScaler(enabled=True)\n",
    "logsys.info(f'use lr_scheduler:{lr_scheduler}')\n",
    "\n",
    "args.pretrain_weight = args.pretrain_weight.strip()\n",
    "logsys.info(f\"loading weight from {args.pretrain_weight}\")\n",
    "start_epoch, start_step, min_loss = load_model(model.module if args.distributed else model, optimizer, lr_scheduler, loss_scaler, path=args.pretrain_weight, \n",
    "                    only_model= (args.mode=='fourcast') or (args.mode=='finetune' and not args.continue_train) ,loc = 'cuda:{}'.format(args.gpu))\n",
    "if args.more_epoch_train:\n",
    "    assert args.pretrain_weight\n",
    "    print(f\"detect more epoch training, we will do a copy processing for {args.pretrain_weight}\")\n",
    "    os.system(f'cp {args.pretrain_weight} {args.pretrain_weight}-epoch{start_epoch}')\n",
    "logsys.info(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86055a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_fourcast(args, model,logsys,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8890b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.mode="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b857c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.mode='fourcast'\n",
    "args.time_step = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbc6edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_flag': '2D70N', 'root': None, 'mode': 'pretrain', 'time_step': 6, 'check_data': True, 'time_reverse_flag': 'only_forward', 'use_offline_data': 1, 'time_intervel': 1, 'cross_sample': 1, 'random_dataset': 1, 'img_size': (5, 5)}\n",
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test.npy\n"
     ]
    }
   ],
   "source": [
    "test_dataset,  test_dataloader = get_test_dataset(args,test_dataset_tensor=None,\n",
    "                                      test_record_load=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e93a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41871d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 16:11:00,119 use dataset ==> WeathBench7066PatchDataset\n",
      "2022-11-18 16:11:00,120 starting fourcast~!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logsys.info_log_path = os.path.join(logsys.ckpt_root, 'fourcast.info')\n",
    "if test_dataloader is None:\n",
    "    test_dataset,  test_dataloader = get_test_dataset(args)\n",
    "test_dataset = test_dataloader.dataset\n",
    "\n",
    "\n",
    "#args.force_fourcast=True\n",
    "gpu       = dist.get_rank() if hasattr(model,'module') else 0\n",
    "save_path = os.path.join(logsys.ckpt_root,f\"fourcastresult.gpu_{gpu}\")\n",
    "\n",
    "logsys.info(f\"use dataset ==> {test_dataset.__class__.__name__}\")\n",
    "logsys.info(\"starting fourcast~!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e5e6271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load everything, start_validating......\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='728', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/728 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='728', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.14% [1/728 00:02<25:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='728', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.27% [2/728 00:04<25:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='728', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.41% [3/728 00:06<25:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='728', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.55% [4/728 00:08<25:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='728', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.69% [5/728 00:10<25:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='728', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.82% [6/728 00:12<24:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourcastresult = fourcast_step(test_dataloader, model,logsys,random_repeat = args.fourcast_randn_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourcastresult = fourcast_step(test_dataloader, model,logsys,random_repeat = args.fourcast_randn_initial)\n",
    "# torch.save(fourcastresult,save_path)\n",
    "# logsys.info(f\"save fourcastresult at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959b737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = test_dataloader\n",
    "dataset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee88bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load everything, start_validating......\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='724', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/724 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m batch       \u001b[38;5;241m=\u001b[39m make_data_regular(batch,half_model)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# first sum should be (B, P, W, H )\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "logsys.eval()\n",
    "status     = 'test'\n",
    "gpu        = dist.get_rank() if hasattr(model,'module') else 0\n",
    "Fethcher   = Datafetcher\n",
    "prefetcher = Fethcher(data_loader,next(model.parameters()).device)\n",
    "batches = len(data_loader)\n",
    "inter_b    = logsys.create_progress_bar(batches,unit=' img',unit_scale=data_loader.batch_size)\n",
    "device = next(model.parameters()).device\n",
    "data_cost = train_cost = rest_cost = 0\n",
    "now = time.time()\n",
    "model.clim = torch.Tensor(data_loader.dataset.clim_tensor).to(device)\n",
    "fourcastresult={}\n",
    "save_prediction_first_step = None#torch.zeros_like(data_loader.dataset.data)\n",
    "save_prediction_final_step = None#torch.zeros_like(data_loader.dataset.data)\n",
    "intervel = batches//100 + 1\n",
    "with torch.no_grad():\n",
    "    inter_b.lwrite(\"load everything, start_validating......\", end=\"\\r\")\n",
    "\n",
    "    while inter_b.update_step():\n",
    "        #if inter_b.now>10:break\n",
    "        data_cost += time.time() - now;now = time.time()\n",
    "        step        = inter_b.now\n",
    "        idxes,batch = prefetcher.next()\n",
    "        batch       = make_data_regular(batch,half_model)\n",
    "        # first sum should be (B, P, W, H )\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "add5ce55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 28, 64])\n",
      "torch.Size([2, 70, 28, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 28, 64])\n",
      "torch.Size([2, 70, 24, 64])\n",
      "torch.Size([2, 70, 24, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 24, 64])\n",
      "torch.Size([2, 70, 20, 64])\n",
      "torch.Size([2, 70, 20, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 20, 64])\n",
      "torch.Size([2, 70, 16, 64])\n",
      "torch.Size([2, 70, 16, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 16, 64])\n",
      "torch.Size([2, 70, 12, 64])\n",
      "torch.Size([2, 70, 12, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 12, 64])\n",
      "torch.Size([2, 70, 8, 64])\n",
      "torch.Size([2, 70, 8, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 8, 64])\n",
      "torch.Size([2, 70, 4, 64])\n",
      "torch.Size([2, 70, 4, 64])\n",
      "torch.Size([2, 70, 32, 64])\n",
      "torch.Size([2, 70, 4, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [0, -1, 5, 5] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[i]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(start[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m ltmv_pred, target, extra_loss, extra_info_from_model_list, start \u001b[38;5;241m=\u001b[39m \u001b[43monce_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime_step_1_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(ltmv_pred\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/projects_local/FourCastNet/train/pretrain.py:268\u001b[0m, in \u001b[0;36monce_forward\u001b[0;34m(model, i, start, end, dataset, time_step_1_mode)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m once_forward_with_timestamp(model,i,start,end,dataset,time_step_1_mode)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatch\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43monce_forward_patch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime_step_1_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m  once_forward_normal(model,i,start,end,dataset,time_step_1_mode)\n",
      "File \u001b[0;32m~/projects_local/FourCastNet/train/pretrain.py:222\u001b[0m, in \u001b[0;36monce_forward_patch\u001b[0;34m(model, i, start, end, dataset, time_step_1_mode)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39minput_noise_std \u001b[38;5;129;01mand\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    220\u001b[0m     normlized_Field \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(normlized_Field)\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput_noise_std\n\u001b[0;32m--> 222\u001b[0m out   \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormlized_Field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m extra_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    225\u001b[0m extra_info_from_model_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects_local/FourCastNet/model/patch_model.py:337\u001b[0m, in \u001b[0;36mPatchWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mThe input either (B,P,patch_range,patch_range) or (B,P,w,h)\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mThe output then is  (B,P) or (B,P,w-patch_range//2,h-patch_range//2)\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    336\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_to_patches(x)\n\u001b[0;32m--> 337\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    339\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatches_to_image(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects_local/FourCastNet/model/afnonet.py:437\u001b[0m, in \u001b[0;36mAFNONet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    435\u001b[0m B \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m ot_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m--> 437\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# (B, p, z, h, w) or (B, p, h, w)\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m#timer.restart(level=0)\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m#print(torch.std_mean(x))\u001b[39;00m\n\u001b[1;32m    440\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_features(x);\u001b[38;5;66;03m#print(torch.std_mean(x))\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [0, -1, 5, 5] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    accu_series=[]\n",
    "    rmse_series=[]\n",
    "    extra_info = {}\n",
    "    time_step_1_mode=False\n",
    "    clim = model.clim.detach().cpu()\n",
    "    start = batch[0:model.history_length] # start must be a list\n",
    "    for i in range(model.history_length,len(batch)):# i now is the target index\n",
    "        print(batch[i].shape)\n",
    "        print(start[0].shape)\n",
    "        ltmv_pred, target, extra_loss, extra_info_from_model_list, start = once_forward(model,i,start,batch[i],dataset,time_step_1_mode)\n",
    "        torch.cuda.empty_cache()\n",
    "        print(ltmv_pred.shape)\n",
    "        print(target.shape)\n",
    "\n",
    "        for extra_info_from_model in extra_info_from_model_list:\n",
    "            for key, val in extra_info_from_model.items():\n",
    "                if i not in extra_info:extra_info[i] = {}\n",
    "                if key not in extra_info[i]:extra_info[i][key] = []\n",
    "                extra_info[i][key].append(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        fourcastresult,extra_info = run_one_fourcast_iter(model, batch, idxes, fourcastresult,data_loader.dataset,\n",
    "                                     save_prediction_first_step=save_prediction_first_step,save_prediction_final_step=save_prediction_final_step)\n",
    "        train_cost += time.time() - now;now = time.time()\n",
    "        start = batch[0]\n",
    "        for _ in range(random_repeat):\n",
    "            batch[0] = start*(1 + torch.randn_like(start)*0.05)\n",
    "            fourcastresult,extra_info = run_one_fourcast_iter(model, batch, idxes, fourcastresult,data_loader.dataset)\n",
    "        rest_cost += time.time() - now;now = time.time()\n",
    "        if (step+1) % intervel==0 or step==0:\n",
    "            for idx, val_pool in extra_info.items():\n",
    "                info_pool={}\n",
    "                for key, val in val_pool.items():\n",
    "                    logsys.record(f'test_{key}_each_fourcast_step', np.mean(val), idx, epoch_flag = 'time_step')\n",
    "                    info_pool[f'test_{key}_each_fourcast_step'] = np.mean(val)\n",
    "            outstring=(f\"epoch:fourcast iter:[{step:5d}]/[{len(data_loader)}] GPU:[{gpu}] cost:[Date]:{data_cost/intervel:.1e} [Train]:{train_cost/intervel:.1e} [Rest]:{rest_cost/intervel:.1e}\")\n",
    "            inter_b.lwrite(outstring, end=\"\\r\")\n",
    "if save_prediction_first_step is not None:torch.save(save_prediction_first_step,os.path.join(logsys.ckpt_root,'save_prediction_first_step')) \n",
    "if save_prediction_final_step is not None:torch.save(save_prediction_final_step,os.path.join(logsys.ckpt_root,'save_prediction_final_step')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355656ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a817e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f34ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0878ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_tensor=None;\n",
    "train_record_load=None;\n",
    "valid_dataset_tensor=None;\n",
    "valid_record_load=None\n",
    "args.debug = True\n",
    "args.valid_batch_size=3\n",
    "train_dataset, val_dataset, train_dataloader,val_dataloader = get_train_and_valid_dataset(args,\n",
    "               train_dataset_tensor=train_dataset_tensor,train_record_load=train_record_load,\n",
    "               valid_dataset_tensor=valid_dataset_tensor,valid_record_load=valid_record_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.load_otensor(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d30bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tensor=None;\n",
    "train_record_load=None;\n",
    "valid_dataset_tensor=None;\n",
    "valid_record_load=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b458ea",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =======================> start training <==========================\n",
    "print(f\"entering {args.mode} training in {next(model.parameters()).device}\")\n",
    "now_best_path = SAVE_PATH / 'backbone.best.pt'\n",
    "latest_ckpt_p = SAVE_PATH / 'pretrain_latest.pt'\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, train_dataloader,val_dataloader = get_train_and_valid_dataset(args,\n",
    "               train_dataset_tensor=train_dataset_tensor,train_record_load=train_record_load,\n",
    "               valid_dataset_tensor=valid_dataset_tensor,valid_record_load=valid_record_load)\n",
    "logsys.info(f\"use dataset ==> {train_dataset.__class__.__name__}\")\n",
    "logsys.info(f\"Start training for {args.epochs} epochs\")\n",
    "metric_list = ['loss']\n",
    "master_bar        = logsys.create_master_bar(args.epochs)\n",
    "master_bar.set_multiply_graph(figsize=(9,3),engine=[['plot','plot']],labels=[metric_list])\n",
    "for epoch in master_bar:\n",
    "    if epoch < start_epoch:continue\n",
    "    if hasattr(model,'set_epoch'):model.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "    if hasattr(model,'module') and hasattr(model.module,'set_epoch'):model.module.set_epoch(epoch=epoch,epoch_total=args.epochs)\n",
    "    logsys.record('learning rate',optimizer.param_groups[0]['lr'],epoch)\n",
    "    train_loss = run_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,logsys,'train')\n",
    "    if (not args.more_epoch_train) and (lr_scheduler is not None):lr_scheduler.step(epoch)\n",
    "    #torch.cuda.empty_cache()\n",
    "    #train_loss = single_step_evaluate(train_dataloader, model, criterion,epoch,logsys,status='train') if 'small' in args.train_set else -1\n",
    "    val_loss   = run_one_epoch(epoch, start_step, model, criterion, val_dataloader, optimizer, loss_scaler,logsys,'valid')\n",
    "\n",
    "    if (not args.distributed) or (args.rank == 0 and local_rank == 0) :\n",
    "        logsys.info(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "        logsys.record('train', train_loss, epoch)\n",
    "        logsys.record('valid', val_loss, epoch)\n",
    "        if use_wandb:wandb.log({\"epoch\":epoch,'train':train_loss,'valid':val_loss})\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            if epoch > args.epochs//10:\n",
    "                logsys.info(f\"saving best model ....\")\n",
    "                save_model(model, path=now_best_path, only_model=True)\n",
    "                logsys.info(f\"done;\")\n",
    "            #if last_best_path is not None:os.system(f\"rm {last_best_path}\")\n",
    "            #last_best_path= now_best_path\n",
    "            logsys.info(f\"The best accu is {val_loss}\")\n",
    "        logsys.record('best_loss', min_loss, epoch)\n",
    "        update_experiment_info(experiment_hub_path,epoch,args)\n",
    "        if epoch>args.save_warm_up:\n",
    "            logsys.info(f\"saving latest model ....\")\n",
    "            save_model(model, epoch+1, 0, optimizer, lr_scheduler, loss_scaler, min_loss, latest_ckpt_p)\n",
    "            logsys.info(f\"done ....\")\n",
    "\n",
    "if os.path.exists(now_best_path) and args.do_final_fourcast:\n",
    "    logsys.info(f\"we finish training, then start test on the best checkpoint {now_best_path}\")\n",
    "    start_epoch, start_step, min_loss = load_model(model.module if args.distributed else model, path=now_best_path, only_model=True)\n",
    "    run_fourcast(args, model,logsys)\n",
    "if use_wandb:wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fbe32",
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "if local_rank == 0:\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "master_bar        = logsys.create_master_bar(args.epochs)\n",
    "last_best_path = None\n",
    "for epoch in master_bar:\n",
    "    if epoch < start_epoch:continue\n",
    "    train_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,lr_scheduler, min_loss,logsys)\n",
    "    lr_scheduler.step(epoch)\n",
    "    #torch.cuda.empty_cache()\n",
    "    train_loss = single_step_evaluate(train_dataloader, model, criterion,epoch,logsys)\n",
    "    #train_loss = -1\n",
    "    val_loss   = single_step_evaluate(val_dataloader, model, criterion,epoch,logsys)\n",
    "\n",
    "    if rank == 0 and local_rank == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "        logsys.record('train', train_loss, epoch)\n",
    "        logsys.record('valid', val_loss, epoch)\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            print(f\"saving best model ....\")\n",
    "            now_best_path = SAVE_PATH / f'backbone.best.pt'\n",
    "            if epoch>args.save_warm_up:save_model(model, path=now_best_path, only_model=True)\n",
    "            #if last_best_path is not None:os.system(f\"rm {last_best_path}\")\n",
    "            #last_best_path= now_best_path\n",
    "            print(f\"done; the best accu is {val_loss}\")\n",
    "        logsys.record('best_loss', min_loss, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c43ef",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    ngpus = ngpus_per_node = torch.cuda.device_count()\n",
    "    args.world_size = -1\n",
    "    args.dist_file  = None\n",
    "    args.rank       = 0\n",
    "    args.dist_backend = \"nccl\"\n",
    "    args.multiprocessing_distributed = ngpus>1\n",
    "    if not hasattr(args,'train_set'):args.train_set='large'\n",
    "    ip = os.environ.get(\"MASTER_ADDR\", \"127.0.0.1\")\n",
    "    port = os.environ.get(\"MASTER_PORT\", \"54247\")\n",
    "    hosts = int(os.environ.get(\"WORLD_SIZE\", \"1\"))  # number of nodes\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))  # node id\n",
    "    gpus = torch.cuda.device_count()  # gpus per node\n",
    "    args.dist_url = f\"tcp://{ip}:{port}\"\n",
    "    if args.world_size == -1 and \"SLURM_NPROCS\" in os.environ:\n",
    "        args.world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "        args.rank       = int(os.environ[\"SLURM_PROCID\"])\n",
    "        jobid           = os.environ[\"SLURM_JOBID\"]\n",
    "\n",
    "        hostfile        = \"dist_url.\" + jobid  + \".txt\"\n",
    "        if args.dist_file is not None:\n",
    "            args.dist_url = \"file://{}.{}\".format(os.path.realpath(args.dist_file), jobid)\n",
    "        elif args.rank == 0:\n",
    "            import socket\n",
    "            ip = socket.gethostbyname(socket.gethostname())\n",
    "            port = find_free_port()\n",
    "            args.dist_url = \"tcp://{}:{}\".format(ip, port)\n",
    "            #with open(hostfile, \"w\") as f:f.write(args.dist_url)\n",
    "        else:\n",
    "            import os\n",
    "            import time\n",
    "            while not os.path.exists(hostfile):\n",
    "                time.sleep(1)\n",
    "            with open(hostfile, \"r\") as f:\n",
    "                args.dist_url = f.read()\n",
    "        print(\"dist-url:{} at PROCID {} / {}\".format(args.dist_url, args.rank, args.world_size))\n",
    "    else:\n",
    "        args.world_size = 1\n",
    "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "    train_dataset_tensor=valid_dataset_tensor=None\n",
    "\n",
    "    print(\"======== loading data ==========\")\n",
    "    if 'small' in args.train_set:\n",
    "        if not args.fourcast:\n",
    "            train_dataset_tensor = load_small_dataset_in_memory('train').share_memory_()\n",
    "            valid_dataset_tensor = load_small_dataset_in_memory('valid').share_memory_()\n",
    "        else:\n",
    "            train_dataset_tensor = load_small_dataset_in_memory('test').share_memory_()\n",
    "            valid_dataset_tensor = None\n",
    "    else:\n",
    "        if args.fourcast:\n",
    "            train_dataset_tensor = load_test_dataset_in_memory(years=[2018],root=\"/nvme/zhangtianning/datasets/ERA5\").share_memory_()\n",
    "            valid_dataset_tensor = None\n",
    "    print(\"=======done==========\")\n",
    "    print(train_dataset_tensor.shape)\n",
    "    if args.multiprocessing_distributed:\n",
    "        args.world_size = ngpus_per_node * args.world_size\n",
    "        torch.multiprocessing.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args,train_dataset_tensor,valid_dataset_tensor))\n",
    "    else:\n",
    "        main_worker(0, ngpus_per_node, args,train_dataset_tensor,valid_dataset_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
